{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import SNDataset,SNDatasetClimate, myNormalize, myToTensor, Augmentations\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import numpy as np\n",
    "from dataset.utils.utils import TextColors as tc\n",
    "from plot_utils.plot import plot_train_test_losses\n",
    "from datetime import date, datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder called 'results' in the current directory if it doesn't exist\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date and Time: 2023-05-08 12:45:16\n"
     ]
    }
   ],
   "source": [
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "start_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Current Date and Time:\", start_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nafiseh'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "NAFISEH = \"Nafiseh\"\n",
    "MOIEN = \"Moien\"\n",
    "\n",
    "if \"d:\" in os.getcwd():\n",
    "    USER = MOIEN\n",
    "elif \"c:\" in os.getcwd():\n",
    "    USER = NAFISEH\n",
    "else:\n",
    "    raise Exception(\"Unknown user\")\n",
    "\n",
    "USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "NUM_WORKERS = 6 if USER == NAFISEH else 2\n",
    "TRAIN_BATCH_SIZE = 32 if USER == NAFISEH else 4\n",
    "TEST_BATCH_SIZE = 32 if USER == NAFISEH else 4\n",
    "LEARNING_RATE = 1e-4 if USER == NAFISEH else 1e-4\n",
    "NUM_EPOCHS = 20 if USER == NAFISEH else 5\n",
    "LR_SCHEDULER = \"step\" # step, plateau or None\n",
    "\n",
    "OC_MAX = 87\n",
    "USE_SRTM = True\n",
    "\n",
    "USE_SPATIAL_ATTENTION = True if USER == NAFISEH else False\n",
    "USE_LSTM_BRANCH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USE_SRTM:\n",
    "    mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)], [(12), (-4,2963)], [(13), (0, 90)]], oc_min = 0, oc_max = OC_MAX)\n",
    "else:\n",
    "    mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)]], oc_min = 0, oc_max = OC_MAX)\n",
    "    \n",
    "my_to_tensor = myToTensor()\n",
    "my_augmentation = Augmentations()\n",
    "train_transform = transforms.Compose([mynorm, my_to_tensor,my_augmentation])\n",
    "test_transform = transforms.Compose([mynorm, my_to_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USER == MOIEN:\n",
    "    train_l8_folder_path = 'D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\'\n",
    "    test_l8_folder_path = 'D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\'\n",
    "    lucas_csv_path = 'D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv'\n",
    "    climate_csv_folder_path = \"D:\\\\python\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled\\\\\"\n",
    "    \n",
    "elif USER == NAFISEH:\n",
    "    train_l8_folder_path = 'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\train'\n",
    "    test_l8_folder_path = 'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\test'\n",
    "    lucas_csv_path = 'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv'\n",
    "    climate_csv_folder_path = 'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [0,1,2,3,4,5,6,7,8,9,10,11] if not USE_SRTM else [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "\n",
    "################################# IF Not USE_LSTM_BRANCH ###############################\n",
    "if not USE_LSTM_BRANCH: # NOT USING THE CLIMATE DATA\n",
    "    train_ds = SNDataset(train_l8_folder_path, lucas_csv_path,l8_bands=bands, transform=train_transform)\n",
    "    test_ds =  SNDataset(test_l8_folder_path, lucas_csv_path,l8_bands=bands, transform=test_transform)\n",
    "    \n",
    "    \n",
    "################################### IF USE_LSTM_BRANCH #################################\n",
    "else: # USING THE CLIMATE DATA\n",
    "    train_ds = SNDatasetClimate(train_l8_folder_path,\n",
    "                                    lucas_csv_path,\n",
    "                                    climate_csv_folder_path,\n",
    "                                    l8_bands=bands, transform=train_transform)\n",
    "\n",
    "    test_ds = SNDatasetClimate(test_l8_folder_path,\n",
    "                                lucas_csv_path,\n",
    "                                climate_csv_folder_path,\n",
    "                                l8_bands=bands, transform=test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COUNTING the csv files in the csv folder\n",
    "CSV_FILES = [f for f in os.listdir(climate_csv_folder_path) if f.endswith('.csv')]\n",
    "NUM_CLIMATE_FEATURES = len(CSV_FILES)\n",
    "NUM_CLIMATE_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soilnet.soil_net import SoilNetFC, ResNet, ResNetLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "# # Get image and label from custom DataLoader\n",
    "# img_custom, climate,label_custom = next(iter(train_dl))\n",
    "# print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")\n",
    "# print(f\"Climate shape: {climate.shape} -> [batch_size, climate_channels]\")\n",
    "# print(f\"Label shape: {label_custom.shape}\")\n",
    "# img_gpu = img_custom.to(device)\n",
    "# print(img_gpu.device, img_gpu.shape)\n",
    "# print(img_gpu.shape)\n",
    "# y = model(img_gpu)\n",
    "# y.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = {\"train_loss\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"MAE\": [],\n",
    "               \"R2\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date and Time: D_2023_05_08_T_12_45\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "run_name = now.strftime(\"D_%Y_%m_%d_T_%H_%M\")\n",
    "print(\"Current Date and Time:\", run_name)\n",
    "# create a folder called 'results' in the current directory if it doesn't exist\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEDS = [1] if USER == MOIEN else [1, 4, 69, 75, 79, 128, 474, 786, 2048, 3333]\n",
    "SEEDS = [1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;45m CROSS VAL 1 \u001b[0m\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[92m Epoch 1\n",
      "------------------------------- \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/478 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\nkakhani\\_Multimodal\\SoilNet-3\\SoilNet\\dataset\\dataset_loader.py\", line 115, in __getitem__\n    clim_arr = np.stack([df.values.squeeze() for df in self.clim_dfs_row], axis=1)\n  File \"<__array_function__ internals>\", line 180, in stack\n  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\numpy\\core\\shape_base.py\", line 426, in stack\n    raise ValueError('all input arrays must have the same shape')\nValueError: all input arrays must have the same shape\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m# Saving the model on the last epoch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m save_model_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults/RUN_\u001b[39m\u001b[39m{\u001b[39;00mrun_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mUSER\u001b[39m}\u001b[39;00m\u001b[39m.pth.tar\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m idx \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(SEEDS)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m results \u001b[39m=\u001b[39m train(model, train_dl, test_dl,\n\u001b[0;32m     25\u001b[0m                 torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49mLEARNING_RATE),\n\u001b[0;32m     26\u001b[0m                 RMSELoss(), epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS, lr_scheduler\u001b[39m=\u001b[39;49mLR_SCHEDULER,\n\u001b[0;32m     27\u001b[0m                 save_model_path\u001b[39m=\u001b[39;49m save_model_path)\n\u001b[0;32m     30\u001b[0m cv_results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     31\u001b[0m cv_results[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(results[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\_Multimodal\\SoilNet-3\\SoilNet\\train_utils.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, lr_scheduler, save_model_path)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    181\u001b[0m     \u001b[39mprint\u001b[39m(tc\u001b[39m.\u001b[39mOKGREEN,\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m,tc\u001b[39m.\u001b[39mENDC)\n\u001b[1;32m--> 182\u001b[0m     train_loss \u001b[39m=\u001b[39m train_step(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m    183\u001b[0m                                        data_loader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m    184\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m    185\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[0;32m    186\u001b[0m     test_loss \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    187\u001b[0m         data_loader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[0;32m    188\u001b[0m         loss_fn\u001b[39m=\u001b[39mloss_fn)\n\u001b[0;32m    190\u001b[0m     \u001b[39m# 4. Print out what's happening\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\_Multimodal\\SoilNet-3\\SoilNet\\train_utils.py:71\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data_loader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     69\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     70\u001b[0m loop \u001b[39m=\u001b[39m tqdm(data_loader, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 71\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loop):\n\u001b[0;32m     72\u001b[0m     \u001b[39m# Send data to target device\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(X, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(X, \u001b[39mlist\u001b[39m): \u001b[39m# if its a tuple it has the climate data in it\u001b[39;00m\n\u001b[0;32m     74\u001b[0m         X \u001b[39m=\u001b[39m [tensor\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(X)]\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\nkakhani\\_Multimodal\\SoilNet-3\\SoilNet\\dataset\\dataset_loader.py\", line 115, in __getitem__\n    clim_arr = np.stack([df.values.squeeze() for df in self.clim_dfs_row], axis=1)\n  File \"<__array_function__ internals>\", line 180, in stack\n  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\numpy\\core\\shape_base.py\", line 426, in stack\n    raise ValueError('all input arrays must have the same shape')\nValueError: all input arrays must have the same shape\n"
     ]
    }
   ],
   "source": [
    "for idx, seed in enumerate(SEEDS):\n",
    "    print(tc.BOLD_BAKGROUNDs.PURPLE, f\"CROSS VAL {idx+1}\", tc.ENDC)\n",
    "    \n",
    "    \n",
    "    train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    test_dl = DataLoader(test_ds, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    #model = SoilNetFC(cnn_in_channels=12, regresor_input_from_cnn=1024, hidden_size=128).to(device)\n",
    "    architecture = \"101+GLAM\" if USE_SPATIAL_ATTENTION else \"101\"\n",
    "    if USE_LSTM_BRANCH:\n",
    "        model = ResNetLSTM(resnet_architecture=architecture, cnn_in_channels=len(bands), regresor_input_from_cnn=1024,\n",
    "                       lstm_n_features= NUM_CLIMATE_FEATURES, lstm_n_layers= 2, lstm_out= 128,\n",
    "                       hidden_size=128).to(device)\n",
    "    else:\n",
    "        model = ResNet(resnet_architecture=architecture, cnn_in_channels=len(bands), regresor_input_from_cnn=1024, hidden_size=128).to(device)\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Saving the model on the last epoch\n",
    "    save_model_path = f\"results/RUN_{run_name}_{USER}.pth.tar\" if idx == len(SEEDS)-1 else None\n",
    "    \n",
    "    results = train(model, train_dl, test_dl,\n",
    "                    torch.optim.Adam(model.parameters(), lr=LEARNING_RATE),\n",
    "                    RMSELoss(), epochs=NUM_EPOCHS, lr_scheduler=LR_SCHEDULER,\n",
    "                    save_model_path= save_model_path)\n",
    "\n",
    "    \n",
    "    cv_results['train_loss'].append(results['train_loss'])\n",
    "    cv_results['test_loss'].append(results['test_loss'])\n",
    "    cv_results['MAE'].append(results['MAE'][0])\n",
    "    cv_results['R2'].append(results['R2'][0])\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "somehow the test_loss is smaller than train_loss -> could be due to small smaple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = np.asarray(cv_results['train_loss'])\n",
    "test_arr = np.asarray(cv_results['test_loss'])\n",
    "train_arr.shape, test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_losses(train_arr,test_arr, title=\"Train/Test Losses\", x_label=\"Epochs\", y_label=\"RMSE\", min_max_bounds= True, tight_x_lim= True, save_path=f\"results/RUN_{run_name}_{USER}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "finish_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Current Date and Time:\", finish_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"MAE\"],cv_results[\"R2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_full = {}\n",
    "cv_results_full['MAE_MEAN'] = np.mean(cv_results['MAE'])\n",
    "cv_results_full['R2_MEAN'] = np.mean(cv_results['R2'])\n",
    "cv_results_full['MAE_MEAN'],cv_results_full['R2_MEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_full['USE_LSTM_BRANCH'] = USE_LSTM_BRANCH\n",
    "cv_results_full['NUM_CLIMATE_FEATURES'] = NUM_CLIMATE_FEATURES if USE_LSTM_BRANCH else None\n",
    "cv_results_full['CSV_FILES'] = CSV_FILES if USE_LSTM_BRANCH else None\n",
    "cv_results_full['NUM_WORKERS'] = NUM_WORKERS\n",
    "cv_results_full['TRAIN_BATCH_SIZE'] = TRAIN_BATCH_SIZE\n",
    "cv_results_full['TEST_BATCH_SIZE'] = TEST_BATCH_SIZE\n",
    "cv_results_full['LEARNING_RATE'] = LEARNING_RATE\n",
    "cv_results_full['NUM_EPOCHS'] = NUM_EPOCHS\n",
    "cv_results_full['LR_SCHEDULER'] = LR_SCHEDULER\n",
    "cv_results_full['USE_SPATIAL_ATTENTION'] = USE_SPATIAL_ATTENTION\n",
    "cv_results_full['SEEDS'] = SEEDS\n",
    "cv_results_full['OC_MAX'] = OC_MAX\n",
    "cv_results_full['USE_SRTM'] = USE_SRTM\n",
    "cv_results_full['TIME'] = {\"start\": start_string, \"finish\": finish_string}\n",
    "cv_results_full['cv_results'] = cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/RUN_{run_name}_{USER}.json\", \"w\") as fp:\n",
    "    json.dump(cv_results_full, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ JSON FILE\n",
    "# with open(\"results/my_dict.json\", \"r\") as fp:\n",
    "#     my_dict = json.load(fp)\n",
    "\n",
    "# print(my_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c74e58d50e1361e9a22e65e4f4b85ac48544f43f48a101259d9c47c27371742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
