{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import SNDataset, SNDatasetClimate, myNormalize, myToTensor, Augmentations, RFTransform, TensorCenterPixels\n",
    "from torchvision import transforms\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import date, datetime\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder called 'results' in the current directory if it doesn't exist\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "start_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Current Date and Time:\", start_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "NAFISEH = \"Nafiseh\"\n",
    "MOIEN = \"Moien\"\n",
    "\n",
    "if \"d:\" in os.getcwd():\n",
    "    USER = MOIEN\n",
    "elif \"c:\" in os.getcwd():\n",
    "    USER = NAFISEH\n",
    "else:\n",
    "    raise Exception(\"Unknown user\")\n",
    "\n",
    "USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OC_MAX = 87\n",
    "USE_CLIMATE = False\n",
    "USE_SRTM = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Normalization\n",
    "Random Forest doens't need normalization. So I addedthe RF transfom, it only reshapes the image into channels first format.\n",
    "then used myTransfomr to resize to 64x64.\n",
    "\n",
    "You can test my Normalize transform by uncommenting the line in the cell below.\n",
    "\n",
    "### Cut Center\n",
    "Cuts a 2x2 square from the center of the image.\n",
    "If `interplate_center_pixel` is set to True, then the center pixel is interpolated from the 4 surrounding pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)], [(12), (-4,2963)], [(13), (0, 90)]], oc_min = 0, oc_max = 200)\n",
    "rf_transform = RFTransform(oc_min = 0, oc_min = OC_MAX)\n",
    "my_to_tensor = myToTensor()\n",
    "cut_center = TensorCenterPixels(pixel_radius=1 ,interpolate_center_pixel = False)\n",
    "transform = transforms.Compose([rf_transform,my_to_tensor,cut_center])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bands to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [0,1,2,3,4,5,6,7,8,9,10,11] if not USE_SRTM else [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "########################################################################################\n",
    "################################# IF Not USE_CLIMATE ###############################\n",
    "########################################################################################\n",
    "\n",
    "if not USE_CLIMATE: # NOT USING THE CLIMATE DATA\n",
    "    if USER == MOIEN:\n",
    "        train_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "    elif USER == NAFISEH:\n",
    "        train_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\train',\\\n",
    "                             'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh \n",
    "    if USER == MOIEN:\n",
    "        test_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "    elif USER == NAFISEH:\n",
    "        test_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\test',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh \n",
    "        \n",
    "########################################################################################\n",
    "################################### IF USE_CLIMATE #################################\n",
    "########################################################################################\n",
    "else: # USING THE CLIMATE DATA\n",
    "    if USER == MOIEN:\n",
    "        train_ds = SNDatasetClimate('D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\',\n",
    "                                    'D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',\n",
    "                                    \"D:\\\\python\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled\\\\\",\n",
    "                                    l8_bands=bands, transform=transform, normalize_climate = False)\n",
    "    elif USER == NAFISEH:\n",
    "        train_ds = SNDatasetClimate('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\train',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled',\n",
    "                            l8_bands=bands, transform=transform, normalize_climate = False) #Nafiseh \n",
    "    if USER == MOIEN:\n",
    "        test_ds = SNDatasetClimate('D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\',\n",
    "                                'D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',\n",
    "                                \"D:\\\\python\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled\\\\\",\n",
    "                                l8_bands=bands, transform=transform, normalize_climate = False)\n",
    "    elif USER == NAFISEH:\n",
    "        test_ds = SNDatasetClimate('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\test',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled',\n",
    "                            l8_bands=bands, transform=transform, normalize_climate = False) #Nafiseh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0][0][0].shape, train_ds[0][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "NUM_WORKERS = 6 if USER == NAFISEH else 2\n",
    "TRAIN_BATCH_SIZE = 32 if USER == NAFISEH else 4\n",
    "TEST_BATCH_SIZE = 32 if USER == NAFISEH else 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dl = DataLoader(test_ds, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(train_dl):\n",
    "        images_np = features[0].numpy()\n",
    "        climate_np = features[1].numpy()\n",
    "        # Preprocess the features as needed\n",
    "        images_processed = images_np.reshape(images_np.shape[0], -1) # Flatten the images with shape (batch_size, num_pixels * num_bands) -> e.g: (32, 4 * 12) if 4 pixel is being used or (32, 1 * 12) if 1 pixel is being used \n",
    "        climate_processed = climate_np.reshape(climate_np.shape[0], -1) # Flatten the climate data with shape (batch_size, num_climate_features * sequence_length) -> e.g: (32, 14*61) if 14 climate feature is being used and the each feature is a sequence of 61 months\n",
    "        features_processed = np.concatenate([images_processed, climate_processed], axis=1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)  # (DataLoader Length, num_pixels * num_bands + num_climate_features * sequence_length)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)  # (DataLoader Length,)\n",
    "else:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(train_dl):\n",
    "        features_np = features.numpy()\n",
    "        # Preprocess the features as needed\n",
    "        features_processed = features_np.reshape(features_np.shape[0], -1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_processed.shape, X_processed.dtype,\"|\",y_processed.shape, y_processed.dtype)\n",
    "print(f\"Memory size of the Train array is {X_processed.nbytes/(1024**2)} MB or {X_processed.nbytes/(1024**3)} GB\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search.\n",
    "I don't know what are the best parameters for the random forest. <span style=\"color: green;\">Please change them and let me know what works best</span>. Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of hyperparameters to search over\n",
    "param_dist = {\n",
    "    # 'n_estimators': randint(30, 1000),\n",
    "    'n_estimators': [1, 2, 5, 10, 20, 30, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 10],\n",
    "    # 'max_features': [1.0, 'sqrt'],\n",
    "    'min_samples_split': [1, 2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15, 20],\n",
    "    'max_leaf_nodes': [1, 2, 5, 10, 15, 20],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEP SEARCH\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 20, 30],\n",
    "#     'max_depth': [None, 5, 10, 20, 30],\n",
    "#     'max_features': ['sqrt', 'log2'],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'criterion': ['mse', 'mae'],\n",
    "#     'bootstrap': [True, False],\n",
    "#     'oob_score': [True, False],\n",
    "#     'max_samples': [0.5, 0.75, None],\n",
    "#     'max_leaf_nodes': [None, 10, 20],\n",
    "#     'min_impurity_decrease': [0.0, 0.1],\n",
    "#     'ccp_alpha': [0.0, 0.1],\n",
    "#     'warm_start': [True, False]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITTERS = 50 if USER == NAFISEH else 1\n",
    "CV = 10 if USER == NAFISEH else 2\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RandomForestRegressor\n",
    "rfr_ = RandomForestRegressor()\n",
    "# Define the randomized search object\n",
    "rfr = RandomizedSearchCV(\n",
    "    estimator=rfr_,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=NUM_ITTERS, # Number of Combinations from the grid to try\n",
    "    cv=CV, # Corss Validation Folds\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.fit(X_processed, y_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfr.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a Random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Use the trained model to predict on a new image\n",
    "    n_rand = np.random.randint(0, len(test_ds))\n",
    "    new_image = test_ds[n_rand][0][0].numpy()\n",
    "    new_climate = test_ds[n_rand][0][1].numpy()\n",
    "    new_image_processed = new_image.reshape(1, -1)\n",
    "    new_climate_processed = new_climate.reshape(1, -1)\n",
    "    new_features_processed = np.concatenate([new_image_processed, new_climate_processed], axis=1)\n",
    "    y_pred = rfr.predict(new_features_processed)\n",
    "    print(\"y_pred: \", y_pred[0], \"|\" ,\"y_true: \", test_ds[n_rand][1].numpy())\n",
    "else:\n",
    "    # Use the trained model to predict on a new image\n",
    "    n_rand = np.random.randint(0, len(test_ds))\n",
    "    new_image = test_ds[n_rand][0].numpy()\n",
    "    new_image_processed = new_image.reshape(1, -1)\n",
    "    y_pred = rfr.predict(new_image_processed)\n",
    "    print(\"y_pred: \", y_pred[0], \"|\" ,\"y_true: \", test_ds[n_rand][1].numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(test_dl):\n",
    "        images_np = features[0].numpy()\n",
    "        climate_np = features[1].numpy()\n",
    "        # Preprocess the features as needed\n",
    "        images_processed = images_np.reshape(images_np.shape[0], -1) # Flatten the images with shape (batch_size, num_pixels * num_bands) -> e.g: (32, 4 * 12) if 4 pixel is being used or (32, 1 * 12) if 1 pixel is being used \n",
    "        climate_processed = climate_np.reshape(climate_np.shape[0], -1) # Flatten the climate data with shape (batch_size, num_climate_features * sequence_length) -> e.g: (32, 14*61) if 14 climate feature is being used and the each feature is a sequence of 61 months\n",
    "        features_processed = np.concatenate([images_processed, climate_processed], axis=1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)  # (DataLoader Length, num_pixels * num_bands + num_climate_features * sequence_length)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)  # (DataLoader Length,)\n",
    "else:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(test_dl):\n",
    "        features_np = features.numpy()\n",
    "        # Preprocess the features as needed\n",
    "        features_processed = features_np.reshape(features_np.shape[0], -1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfr.predict(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_processed, y_pred))\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "finish_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "file_name = now.strftime(\"D_%Y_%m_%d_T_%H_%M\")\n",
    "print(\"Current Date and Time:\", finish_string)\n",
    "print(\"File Name:\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_json = {}\n",
    "log_json['RMSE'] = rmse\n",
    "log_json['USE_CLIMATE'] = USE_CLIMATE\n",
    "log_json['USE_SRTM'] = USE_SRTM\n",
    "\n",
    "log_json['NUM_ITTERS'] = NUM_ITTERS\n",
    "log_json['CV'] = CV\n",
    "log_json['SEED'] = SEED\n",
    "\n",
    "log_json['param_dist'] = str(param_dist)\n",
    "log_json['BEST_PARAMS'] = rfr.best_params_\n",
    "\n",
    "log_json['TIME'] = {'start': start_string, 'finish': finish_string}\n",
    "\n",
    "log_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/RF_{file_name}_{USER}.json\", \"w\") as fp:\n",
    "    json.dump(log_json, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
