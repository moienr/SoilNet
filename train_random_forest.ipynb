{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import SNDataset, SNDatasetClimate, myNormalize, myToTensor, Augmentations, RFTransform, TensorCenterPixels\n",
    "from torchvision import transforms\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import date, datetime\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder called 'results' in the current directory if it doesn't exist\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date and Time: 2023-05-17 16:03:34\n"
     ]
    }
   ],
   "source": [
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "start_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Current Date and Time:\", start_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nafiseh'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "NAFISEH = \"Nafiseh\"\n",
    "MOIEN = \"Moien\"\n",
    "\n",
    "if \"d:\" in os.getcwd():\n",
    "    USER = MOIEN\n",
    "elif \"c:\" in os.getcwd():\n",
    "    USER = NAFISEH\n",
    "else:\n",
    "    raise Exception(\"Unknown user\")\n",
    "\n",
    "USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OC_MAX = 87\n",
    "USE_CLIMATE = False\n",
    "USE_SRTM = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Normalization\n",
    "Random Forest doens't need normalization. So I addedthe RF transfom, it only reshapes the image into channels first format.\n",
    "then used myTransfomr to resize to 64x64.\n",
    "\n",
    "You can test my Normalize transform by uncommenting the line in the cell below.\n",
    "\n",
    "### Cut Center\n",
    "Cuts a 2x2 square from the center of the image.\n",
    "If `interplate_center_pixel` is set to True, then the center pixel is interpolated from the 4 surrounding pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)], [(12), (-4,2963)], [(13), (0, 90)]], oc_min = 0, oc_max = 200)\n",
    "rf_transform = RFTransform(oc_min = 0, oc_max = OC_MAX)\n",
    "my_to_tensor = myToTensor()\n",
    "cut_center = TensorCenterPixels(pixel_radius=1 ,interpolate_center_pixel = False)\n",
    "transform = transforms.Compose([rf_transform,my_to_tensor,cut_center])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bands to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [0,1,2,3,4,5,6,7,8,9,10,11] if not USE_SRTM else [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "########################################################################################\n",
    "################################# IF Not USE_CLIMATE ###############################\n",
    "########################################################################################\n",
    "\n",
    "if not USE_CLIMATE: # NOT USING THE CLIMATE DATA\n",
    "    if USER == MOIEN:\n",
    "        train_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "    elif USER == NAFISEH:\n",
    "        train_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\train',\\\n",
    "                             'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh \n",
    "    if USER == MOIEN:\n",
    "        test_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "    elif USER == NAFISEH:\n",
    "        test_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\test',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh \n",
    "        \n",
    "########################################################################################\n",
    "################################### IF USE_CLIMATE #################################\n",
    "########################################################################################\n",
    "else: # USING THE CLIMATE DATA\n",
    "    if USER == MOIEN:\n",
    "        train_ds = SNDatasetClimate('D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\',\n",
    "                                    'D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',\n",
    "                                    \"D:\\\\python\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled\\\\\",\n",
    "                                    l8_bands=bands, transform=transform, normalize_climate = False)\n",
    "    elif USER == NAFISEH:\n",
    "        train_ds = SNDatasetClimate('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\train',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled',\n",
    "                            l8_bands=bands, transform=transform, normalize_climate = False) #Nafiseh \n",
    "    if USER == MOIEN:\n",
    "        test_ds = SNDatasetClimate('D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\',\n",
    "                                'D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',\n",
    "                                \"D:\\\\python\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled\\\\\",\n",
    "                                l8_bands=bands, transform=transform, normalize_climate = False)\n",
    "    elif USER == NAFISEH:\n",
    "        test_ds = SNDatasetClimate('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\test',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled',\n",
    "                            l8_bands=bands, transform=transform, normalize_climate = False) #Nafiseh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), torch.Size([2, 2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0][0].shape, train_ds[0][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "NUM_WORKERS = 6 if USER == NAFISEH else 2\n",
    "TRAIN_BATCH_SIZE = 32 if USER == NAFISEH else 4\n",
    "TEST_BATCH_SIZE = 32 if USER == NAFISEH else 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dl = DataLoader(test_ds, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(train_dl):\n",
    "        images_np = features[0].numpy()\n",
    "        climate_np = features[1].numpy()\n",
    "        # Preprocess the features as needed\n",
    "        images_processed = images_np.reshape(images_np.shape[0], -1) # Flatten the images with shape (batch_size, num_pixels * num_bands) -> e.g: (32, 4 * 12) if 4 pixel is being used or (32, 1 * 12) if 1 pixel is being used \n",
    "        climate_processed = climate_np.reshape(climate_np.shape[0], -1) # Flatten the climate data with shape (batch_size, num_climate_features * sequence_length) -> e.g: (32, 14*61) if 14 climate feature is being used and the each feature is a sequence of 61 months\n",
    "        features_processed = np.concatenate([images_processed, climate_processed], axis=1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)  # (DataLoader Length, num_pixels * num_bands + num_climate_features * sequence_length)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)  # (DataLoader Length,)\n",
    "else:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(train_dl):\n",
    "        features_np = features.numpy()\n",
    "        # Preprocess the features as needed\n",
    "        features_processed = features_np.reshape(features_np.shape[0], -1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15296, 56) float32 | (15296,) float32\n",
      "Memory size of the Train array is 3.267578125 MB or 0.0031909942626953125 GB\n"
     ]
    }
   ],
   "source": [
    "print(X_processed.shape, X_processed.dtype,\"|\",y_processed.shape, y_processed.dtype)\n",
    "print(f\"Memory size of the Train array is {X_processed.nbytes/(1024**2)} MB or {X_processed.nbytes/(1024**3)} GB\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search.\n",
    "I don't know what are the best parameters for the random forest. <span style=\"color: green;\">Please change them and let me know what works best</span>. Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of hyperparameters to search over\n",
    "param_dist = {\n",
    "    # 'n_estimators': randint(30, 1000),\n",
    "    'n_estimators': [1, 2, 5, 10, 20, 30, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 10],\n",
    "    # 'max_features': [1.0, 'sqrt'],\n",
    "    'min_samples_split': [1, 2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15, 20],\n",
    "    'max_leaf_nodes': [1, 2, 5, 10, 15, 20],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEP SEARCH\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 20, 30],\n",
    "#     'max_depth': [None, 5, 10, 20, 30],\n",
    "#     'max_features': ['sqrt', 'log2'],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'criterion': ['mse', 'mae'],\n",
    "#     'bootstrap': [True, False],\n",
    "#     'oob_score': [True, False],\n",
    "#     'max_samples': [0.5, 0.75, None],\n",
    "#     'max_leaf_nodes': [None, 10, 20],\n",
    "#     'min_impurity_decrease': [0.0, 0.1],\n",
    "#     'ccp_alpha': [0.0, 0.1],\n",
    "#     'warm_start': [True, False]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITTERS = 50 if USER == NAFISEH else 1\n",
    "CV = 10 if USER == NAFISEH else 2\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RandomForestRegressor\n",
    "rfr_ = RandomForestRegressor()\n",
    "# Define the randomized search object\n",
    "rfr = RandomizedSearchCV(\n",
    "    estimator=rfr_,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=NUM_ITTERS, # Number of Combinations from the grid to try\n",
    "    cv=CV, # Corss Validation Folds\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "180 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_leaf_nodes' parameter of RandomForestRegressor must be an int in the range [2, inf) or None. Got 1 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of RandomForestRegressor must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.1218364         nan 0.23432408        nan 0.12184693\n",
      " 0.23659841 0.18745674 0.20656202 0.11947234 0.23972309        nan\n",
      "        nan        nan 0.23312399 0.23318029        nan 0.12190367\n",
      "        nan 0.22167492 0.24185592        nan        nan 0.12187327\n",
      " 0.12165581 0.20483359 0.11164249        nan 0.17243292 0.1873139\n",
      " 0.20655987 0.22213306 0.23297993 0.22808877        nan        nan\n",
      " 0.18726562 0.12209864 0.12169133        nan        nan        nan\n",
      " 0.12193322 0.12174185        nan 0.1221702  0.18675619 0.12113218\n",
      " 0.12167154        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=10, estimator=RandomForestRegressor(), n_iter=50,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [1, 2, 3, 4, 5, 10],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [1, 2, 5, 10, 15, 20],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 5, 10, 15,\n",
       "                                                             20],\n",
       "                                        &#x27;min_samples_split&#x27;: [1, 2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [1, 2, 5, 10, 20, 30,\n",
       "                                                         50, 100, 150, 200, 250,\n",
       "                                                         300, 350, 400, 450,\n",
       "                                                         500]},\n",
       "                   random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=10, estimator=RandomForestRegressor(), n_iter=50,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [1, 2, 3, 4, 5, 10],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [1, 2, 5, 10, 15, 20],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 5, 10, 15,\n",
       "                                                             20],\n",
       "                                        &#x27;min_samples_split&#x27;: [1, 2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [1, 2, 5, 10, 20, 30,\n",
       "                                                         50, 100, 150, 200, 250,\n",
       "                                                         300, 350, 400, 450,\n",
       "                                                         500]},\n",
       "                   random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestRegressor(), n_iter=50,\n",
       "                   param_distributions={'max_depth': [1, 2, 3, 4, 5, 10],\n",
       "                                        'max_leaf_nodes': [1, 2, 5, 10, 15, 20],\n",
       "                                        'min_samples_leaf': [1, 2, 5, 10, 15,\n",
       "                                                             20],\n",
       "                                        'min_samples_split': [1, 2, 5, 10],\n",
       "                                        'n_estimators': [1, 2, 5, 10, 20, 30,\n",
       "                                                         50, 100, 150, 200, 250,\n",
       "                                                         300, 350, 400, 450,\n",
       "                                                         500]},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.fit(X_processed, y_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 20, 'max_leaf_nodes': 20, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "print(rfr.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a Random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  15.875988239077675 | y_true:  12.7\n"
     ]
    }
   ],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Use the trained model to predict on a new image\n",
    "    n_rand = np.random.randint(0, len(test_ds))\n",
    "    new_image = test_ds[n_rand][0][0].numpy()\n",
    "    new_climate = test_ds[n_rand][0][1].numpy()\n",
    "    new_image_processed = new_image.reshape(1, -1)\n",
    "    new_climate_processed = new_climate.reshape(1, -1)\n",
    "    new_features_processed = np.concatenate([new_image_processed, new_climate_processed], axis=1)\n",
    "    y_pred = rfr.predict(new_features_processed)\n",
    "    print(\"y_pred: \", y_pred[0], \"|\" ,\"y_true: \", test_ds[n_rand][1].numpy())\n",
    "else:\n",
    "    # Use the trained model to predict on a new image\n",
    "    n_rand = np.random.randint(0, len(test_ds))\n",
    "    new_image = test_ds[n_rand][0].numpy()\n",
    "    new_image_processed = new_image.reshape(1, -1)\n",
    "    y_pred = rfr.predict(new_image_processed)\n",
    "    print(\"y_pred: \", y_pred[0], \"|\" ,\"y_true: \", test_ds[n_rand][1].numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(test_dl):\n",
    "        images_np = features[0].numpy()\n",
    "        climate_np = features[1].numpy()\n",
    "        # Preprocess the features as needed\n",
    "        images_processed = images_np.reshape(images_np.shape[0], -1) # Flatten the images with shape (batch_size, num_pixels * num_bands) -> e.g: (32, 4 * 12) if 4 pixel is being used or (32, 1 * 12) if 1 pixel is being used \n",
    "        climate_processed = climate_np.reshape(climate_np.shape[0], -1) # Flatten the climate data with shape (batch_size, num_climate_features * sequence_length) -> e.g: (32, 14*61) if 14 climate feature is being used and the each feature is a sequence of 61 months\n",
    "        features_processed = np.concatenate([images_processed, climate_processed], axis=1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)  # (DataLoader Length, num_pixels * num_bands + num_climate_features * sequence_length)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)  # (DataLoader Length,)\n",
    "else:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(test_dl):\n",
    "        features_np = features.numpy()\n",
    "        # Preprocess the features as needed\n",
    "        features_processed = features_np.reshape(features_np.shape[0], -1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfr.predict(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 21.26010056233226\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_processed, y_pred))\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date and Time: 2023-05-17 18:14:36\n",
      "File Name: D_2023_05_17_T_18_14\n"
     ]
    }
   ],
   "source": [
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "finish_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "file_name = now.strftime(\"D_%Y_%m_%d_T_%H_%M\")\n",
    "print(\"Current Date and Time:\", finish_string)\n",
    "print(\"File Name:\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RMSE': 21.26010056233226,\n",
       " 'USE_CLIMATE': False,\n",
       " 'USE_SRTM': True,\n",
       " 'NUM_ITTERS': 50,\n",
       " 'CV': 10,\n",
       " 'SEED': 42,\n",
       " 'param_dist': \"{'n_estimators': [1, 2, 5, 10, 20, 30, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500], 'max_depth': [1, 2, 3, 4, 5, 10], 'min_samples_split': [1, 2, 5, 10], 'min_samples_leaf': [1, 2, 5, 10, 15, 20], 'max_leaf_nodes': [1, 2, 5, 10, 15, 20]}\",\n",
       " 'BEST_PARAMS': {'n_estimators': 300,\n",
       "  'min_samples_split': 5,\n",
       "  'min_samples_leaf': 20,\n",
       "  'max_leaf_nodes': 20,\n",
       "  'max_depth': 10},\n",
       " 'TIME': {'start': '2023-05-17 16:03:34', 'finish': '2023-05-17 18:14:36'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_json = {}\n",
    "log_json['RMSE'] = rmse\n",
    "log_json['USE_CLIMATE'] = USE_CLIMATE\n",
    "log_json['USE_SRTM'] = USE_SRTM\n",
    "\n",
    "log_json['NUM_ITTERS'] = NUM_ITTERS\n",
    "log_json['CV'] = CV\n",
    "log_json['SEED'] = SEED\n",
    "\n",
    "log_json['param_dist'] = str(param_dist)\n",
    "log_json['BEST_PARAMS'] = rfr.best_params_\n",
    "\n",
    "log_json['TIME'] = {'start': start_string, 'finish': finish_string}\n",
    "\n",
    "log_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/RF_{file_name}_{USER}.json\", \"w\") as fp:\n",
    "    json.dump(log_json, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
