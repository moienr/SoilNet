{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import SNDataset, SNDatasetClimate, myNormalize, myToTensor, Augmentations, RFTransform, TensorCenterPixels\n",
    "from torchvision import transforms\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import date, datetime\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder called 'results' in the current directory if it doesn't exist\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "start_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "file_name = now.strftime(\"D_%Y_%m_%d_T_%H_%M\")\n",
    "print(\"File Name:\", file_name)\n",
    "print(\"Current Date and Time:\", start_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "NAFISEH = \"Nafiseh\"\n",
    "MOIEN = \"Moien\"\n",
    "\n",
    "if \"d:\" in os.getcwd():\n",
    "    USER = MOIEN\n",
    "elif \"c:\" in os.getcwd():\n",
    "    USER = NAFISEH\n",
    "else:\n",
    "    raise Exception(\"Unknown user\")\n",
    "\n",
    "USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model TO USE**\n",
    "\n",
    "| Model Name | Description |\n",
    "|------------|-------------|\n",
    "| RF         | Random Forest |\n",
    "| ETR        | Extremely Randomized Trees |\n",
    "| XGB        | XGBoost      |\n",
    "| LGBM       | LightGBM     |\n",
    "| GBDT       | Gradient Boosting Decision Tree |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"LGBM\"\n",
    "allowed_models = [\"RF\", \"ETR\", \"XGB\", \"LGBM\", \"GBDT\"]\n",
    "assert MODEL_NAME in allowed_models, f\"MODEL_NAME must be one of {allowed_models}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OC_MAX = 87\n",
    "USE_CLIMATE = False\n",
    "USE_SRTM = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Normalization\n",
    "Random Forest doens't need normalization. So I addedthe RF transfom, it only reshapes the image into channels first format.\n",
    "then used myTransfomr to resize to 64x64.\n",
    "\n",
    "You can test my Normalize transform by uncommenting the line in the cell below.\n",
    "\n",
    "### Cut Center\n",
    "Cuts a 2x2 square from the center of the image.\n",
    "If `interplate_center_pixel` is set to True, then the center pixel is interpolated from the 4 surrounding pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)], [(12), (-4,2963)], [(13), (0, 90)]], oc_min = 0, oc_max = 200)\n",
    "rf_transform = RFTransform(oc_min = 0, oc_max = OC_MAX)\n",
    "my_to_tensor = myToTensor()\n",
    "INTERPOLATE_CENTER_PIXEL = True\n",
    "cut_center = TensorCenterPixels(pixel_radius=1 ,interpolate_center_pixel = INTERPOLATE_CENTER_PIXEL)\n",
    "transform = transforms.Compose([rf_transform,my_to_tensor,cut_center])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bands to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [0,1,2,3,4,5,6,7,8,9,10,11] if not USE_SRTM else [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "########################################################################################\n",
    "################################# IF Not USE_CLIMATE ###############################\n",
    "########################################################################################\n",
    "\n",
    "if not USE_CLIMATE: # NOT USING THE CLIMATE DATA\n",
    "    if USER == MOIEN:\n",
    "        train_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "    elif USER == NAFISEH:\n",
    "        train_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\train',\\\n",
    "                             'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh \n",
    "    if USER == MOIEN:\n",
    "        test_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',\n",
    "                            l8_bands=bands, transform=transform, return_point_id=True)\n",
    "    elif USER == NAFISEH:\n",
    "        test_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\test',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',\n",
    "                            l8_bands=bands, transform=transform,return_point_id=True) #Nafiseh \n",
    "        \n",
    "########################################################################################\n",
    "################################### IF USE_CLIMATE #################################\n",
    "########################################################################################\n",
    "else: # USING THE CLIMATE DATA\n",
    "    if USER == MOIEN:\n",
    "        train_ds = SNDatasetClimate('D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\',\n",
    "                                    'D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',\n",
    "                                    \"D:\\\\python\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled\\\\\",\n",
    "                                    l8_bands=bands, transform=transform, normalize_climate = False)\n",
    "    elif USER == NAFISEH:\n",
    "        train_ds = SNDatasetClimate('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\train',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled',\n",
    "                            l8_bands=bands, transform=transform, normalize_climate = False) #Nafiseh \n",
    "    if USER == MOIEN:\n",
    "        test_ds = SNDatasetClimate('D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\',\n",
    "                                'D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',\n",
    "                                \"D:\\\\python\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled\\\\\",\n",
    "                                l8_bands=bands, transform=transform, normalize_climate = False, return_point_id=True)\n",
    "    elif USER == NAFISEH:\n",
    "        test_ds = SNDatasetClimate('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\l8_images\\\\test',\\\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\LUCAS_2015_all.csv',\n",
    "                            'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-3\\\\SoilNet\\\\dataset\\\\Climate\\\\All\\\\filled',\n",
    "                            l8_bands=bands, transform=transform, normalize_climate = False, return_point_id=True) #Nafiseh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0][0][0].shape, train_ds[0][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "NUM_WORKERS = 6 if USER == NAFISEH else 2\n",
    "TRAIN_BATCH_SIZE = 32 if USER == NAFISEH else 4\n",
    "TEST_BATCH_SIZE = 32 if USER == NAFISEH else 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dl = DataLoader(test_ds, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(train_dl):\n",
    "        images_np = features[0].numpy()\n",
    "        climate_np = features[1].numpy()\n",
    "        # Preprocess the features as needed\n",
    "        images_processed = images_np.reshape(images_np.shape[0], -1) # Flatten the images with shape (batch_size, num_pixels * num_bands) -> e.g: (32, 4 * 12) if 4 pixel is being used or (32, 1 * 12) if 1 pixel is being used \n",
    "        climate_processed = climate_np.reshape(climate_np.shape[0], -1) # Flatten the climate data with shape (batch_size, num_climate_features * sequence_length) -> e.g: (32, 14*61) if 14 climate feature is being used and the each feature is a sequence of 61 months\n",
    "        features_processed = np.concatenate([images_processed, climate_processed], axis=1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)  # (DataLoader Length, num_pixels * num_bands + num_climate_features * sequence_length)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)  # (DataLoader Length,)\n",
    "else:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for batch_idx, (features, target) in enumerate(train_dl):\n",
    "        features_np = features.numpy()\n",
    "        # Preprocess the features as needed\n",
    "        features_processed = features_np.reshape(features_np.shape[0], -1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_processed.shape, X_processed.dtype,\"|\",y_processed.shape, y_processed.dtype)\n",
    "print(f\"Memory size of the Train array is {X_processed.nbytes/(1024**2)} MB or {X_processed.nbytes/(1024**3)} GB\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search.\n",
    "I don't know what are the best parameters for the random forest. <span style=\"color: green;\">Please change them and let me know what works best</span>. Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of hyperparameters to search over\n",
    "# param_dist = {\n",
    "#     # 'n_estimators': randint(30, 1000),\n",
    "#     'n_estimators': [1, 2, 5, 10, 20, 30, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "#     'max_depth': [1, 2, 3, 4, 5, 10],\n",
    "#     # 'max_features': [1.0, 'sqrt'],\n",
    "#     'min_samples_split': [1, 2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 5, 10, 15, 20],\n",
    "#     'max_leaf_nodes': [2, 5, 10, 15, 20],\n",
    "# }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEP SEARCH\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 20, 30],\n",
    "#     'max_depth': [None, 5, 10, 20, 30],\n",
    "#     'max_features': ['sqrt', 'log2'],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'criterion': ['mse', 'mae'],\n",
    "#     'bootstrap': [True, False],\n",
    "#     'oob_score': [True, False],\n",
    "#     'max_samples': [0.5, 0.75, None],\n",
    "#     'max_leaf_nodes': [None, 10, 20],\n",
    "#     'min_impurity_decrease': [0.0, 0.1],\n",
    "#     'ccp_alpha': [0.0, 0.1],\n",
    "#     'warm_start': [True, False]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_ITTERS = 1 if USER == NAFISEH else 1\n",
    "# CV = 2 if USER == NAFISEH else 2\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define RandomForestRegressor\n",
    "# rfr_ = RandomForestRegressor()\n",
    "# # Define the randomized search object\n",
    "# rfr = RandomizedSearchCV(\n",
    "#     estimator=rfr_,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=NUM_ITTERS, # Number of Combinations from the grid to try\n",
    "#     cv=CV, # Corss Validation Folds\n",
    "#     random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"RF\":\n",
    "    # Define RandomForestRegressor with the desired parameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators = 300,\n",
    "        max_depth = 10,\n",
    "        min_samples_split = 5,\n",
    "        min_samples_leaf = 20,\n",
    "        max_leaf_nodes = 20,\n",
    "    )\n",
    "elif MODEL_NAME == \"ETR\":\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    # Extremely Randomized Trees\n",
    "    # Define ExtraTreesRegressor with the desired parameters\n",
    "    model = ExtraTreesRegressor(\n",
    "        n_estimators = 300,\n",
    "        max_depth = 10,\n",
    "        min_samples_split = 5,\n",
    "        min_samples_leaf = 20,\n",
    "        max_leaf_nodes = 20,\n",
    "    )\n",
    "elif MODEL_NAME == \"XGB\":\n",
    "    from xgboost import XGBRegressor\n",
    "    # Define XGBRegressor with the desired parameters\n",
    "    model = XGBRegressor(\n",
    "        n_estimators = 300,\n",
    "        max_depth = 10,\n",
    "        min_child_weight = 5,\n",
    "        gamma = 0,\n",
    "        subsample = 0.8,\n",
    "        colsample_bytree = 0.8,\n",
    "        objective= 'reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=SEED,\n",
    "        # learning_rate = 0.1, # smaller values require more trees but can improve the performance.\n",
    "    ) \n",
    "elif MODEL_NAME == \"LGBM\":\n",
    "    from lightgbm import LGBMRegressor\n",
    "    # Define LGBMRegressor with the desired parameters\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators = 300,\n",
    "        max_depth = 10,\n",
    "        min_child_samples = 20,\n",
    "        num_leaves = 20,\n",
    "    )\n",
    "elif MODEL_NAME == \"GBDT\":\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    # Define GradientBoostingRegressor with the desired parameters\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators = 300,\n",
    "        max_depth = 10,\n",
    "        min_samples_split = 5,\n",
    "        min_samples_leaf = 20,\n",
    "        max_leaf_nodes = 20,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(f\"MODEL_NAME must be one of {allowed_models}\")\n",
    "\n",
    "# Fit the model\n",
    "print(f\"Fitting the {MODEL_NAME} model...\")\n",
    "model.fit(X_processed, y_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rfr.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a Random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Use the trained model to predict on a new image\n",
    "    n_rand = np.random.randint(0, len(test_ds))\n",
    "    new_image = test_ds[n_rand][0][0].numpy()\n",
    "    new_climate = test_ds[n_rand][0][1].numpy()\n",
    "    point_id = test_ds[n_rand][2]\n",
    "    print(\"Point ID: \", point_id, \"type: \", type(point_id))\n",
    "    new_image_processed = new_image.reshape(1, -1)\n",
    "    new_climate_processed = new_climate.reshape(1, -1)\n",
    "    new_features_processed = np.concatenate([new_image_processed, new_climate_processed], axis=1)\n",
    "    y_pred = model.predict(new_features_processed)\n",
    "    print(\"y_pred: \", y_pred[0], \"|\" ,\"y_true: \", test_ds[n_rand][1].numpy())\n",
    "else:\n",
    "    # Use the trained model to predict on a new image\n",
    "    n_rand = np.random.randint(0, len(test_ds))\n",
    "    new_image = test_ds[n_rand][0].numpy()\n",
    "    new_image_processed = new_image.reshape(1, -1)\n",
    "    y_pred = model.predict(new_image_processed)\n",
    "    point_id = test_ds[n_rand][2]\n",
    "    print(\"Point ID: \", point_id, \"type: \", type(point_id))\n",
    "    print(\"y_pred: \", y_pred[0], \"|\" ,\"y_true: \", test_ds[n_rand][1].numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLIMATE:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    point_id_list = []\n",
    "    for batch_idx, (features, target,point_id) in enumerate(test_dl):\n",
    "        images_np = features[0].numpy()\n",
    "        climate_np = features[1].numpy()\n",
    "        \n",
    "        # Preprocess the features as needed\n",
    "        images_processed = images_np.reshape(images_np.shape[0], -1) # Flatten the images with shape (batch_size, num_pixels * num_bands) -> e.g: (32, 4 * 12) if 4 pixel is being used or (32, 1 * 12) if 1 pixel is being used \n",
    "        climate_processed = climate_np.reshape(climate_np.shape[0], -1) # Flatten the climate data with shape (batch_size, num_climate_features * sequence_length) -> e.g: (32, 14*61) if 14 climate feature is being used and the each feature is a sequence of 61 months\n",
    "        features_processed = np.concatenate([images_processed, climate_processed], axis=1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "        point_id_list = point_id_list + list(point_id)\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)  # (DataLoader Length, num_pixels * num_bands + num_climate_features * sequence_length)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)  # (DataLoader Length,)\n",
    "else:\n",
    "    # Preprocess the data using the DataLoader\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    point_id_list = []\n",
    "    for batch_idx, (features, target,point_id) in enumerate(test_dl):\n",
    "        features_np = features.numpy()\n",
    "        # Preprocess the features as needed\n",
    "        features_processed = features_np.reshape(features_np.shape[0], -1)\n",
    "        X_processed.append(features_processed)\n",
    "        y_processed.append(target.numpy())\n",
    "        point_id_list = point_id_list + list(point_id)\n",
    "\n",
    "    X_processed = np.concatenate(X_processed, axis=0)\n",
    "    y_processed = np.concatenate(y_processed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_id_list = [int(i) for i in point_id_list]\n",
    "point_id_arr = np.array(point_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(point_id_arr), len(y_processed), len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSME\n",
    "rmse = np.sqrt(mean_squared_error(y_processed, y_pred))\n",
    "print('RMSE:', rmse)\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_processed, y_pred)\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save y_pred and y_true, point_id to a csv file\n",
    "df = pd.DataFrame({'point_id': point_id_list, 'y_true': y_processed, 'y_pred': y_pred})\n",
    "df.to_csv(f\"results/{MODEL_NAME}_{file_name}_{USER}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date and time\n",
    "now = datetime.now()\n",
    "finish_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Finish Date and Time:\", finish_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_json = {}\n",
    "log_json['MODEL'] = MODEL_NAME\n",
    "log_json['RMSE'] = rmse\n",
    "log_json['MAE'] = mae\n",
    "log_json['USE_CLIMATE'] = USE_CLIMATE\n",
    "log_json['USE_SRTM'] = USE_SRTM\n",
    "log_json['INTERPOLATE_CENTER_PIXEL'] = INTERPOLATE_CENTER_PIXEL\n",
    "# log_json['NUM_ITTERS'] = NUM_ITTERS\n",
    "# log_json['CV'] = CV\n",
    "log_json['SEED'] = SEED\n",
    "\n",
    "# log_json['param_dist'] = str(param_dist)\n",
    "# log_json['BEST_PARAMS'] = rfr.best_params_\n",
    "\n",
    "log_json['TIME'] = {'start': start_string, 'finish': finish_string}\n",
    "\n",
    "log_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "with open(f\"results/{MODEL_NAME}_{file_name}_{USER}.json\", \"w\") as fp:\n",
    "    json.dump(log_json, fp, cls=NumpyEncoder, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
