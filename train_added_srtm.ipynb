{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moi3n\\miniconda3\\envs\\pytorchGPU\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import SNDataset, myNormalize, myToTensor\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)], [(12), (-4,2963)], [(13), (0, 90)]], oc_min = 0, oc_max = 200)\n",
    "mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)]], oc_min = 0, oc_max = 200)\n",
    "my_to_tensor = myToTensor()\n",
    "transform = transforms.Compose([mynorm, my_to_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "# train_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "train_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-1\\\\dataset\\\\l8_images\\\\train',\\\n",
    "                     'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-1\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh \n",
    "\n",
    "# test_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "test_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-1\\\\dataset\\\\l8_images\\\\test',\\\n",
    "                     'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-1\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CONFIG\n",
    "# NUM_WORKERS = 2\n",
    "# TRAIN_BATCH_SIZE = 8\n",
    "# TEST_BATCH_SIZE = 2\n",
    "\n",
    "# CONFIG\n",
    "NUM_WORKERS = 6\n",
    "TRAIN_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dl = DataLoader(test_ds, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([40, 12, 64, 64]) -> [batch_size, color_channels, height, width]\n",
      "Label shape: torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "# Get image and label from custom DataLoader\n",
    "img_custom, label_custom = next(iter(train_dl))\n",
    "\n",
    "print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label_custom.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soilnet.soil_net import SoilNetFC, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 torch.Size([40, 12, 64, 64])\n",
      "torch.Size([40, 12, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0581],\n",
       "        [0.0487],\n",
       "        [0.0625],\n",
       "        [0.0740],\n",
       "        [0.0630],\n",
       "        [0.0331],\n",
       "        [0.0390],\n",
       "        [0.0668],\n",
       "        [0.0355],\n",
       "        [0.0803],\n",
       "        [0.0762],\n",
       "        [0.0633],\n",
       "        [0.0645],\n",
       "        [0.0693],\n",
       "        [0.0673],\n",
       "        [0.0681],\n",
       "        [0.0690],\n",
       "        [0.0159],\n",
       "        [0.0462],\n",
       "        [0.0558],\n",
       "        [0.0891],\n",
       "        [0.0701],\n",
       "        [0.0668],\n",
       "        [0.0595],\n",
       "        [0.0517],\n",
       "        [0.0548],\n",
       "        [0.0688],\n",
       "        [0.0756],\n",
       "        [0.0596],\n",
       "        [0.0599],\n",
       "        [0.0640],\n",
       "        [0.0835],\n",
       "        [0.0903],\n",
       "        [0.0892],\n",
       "        [0.0509],\n",
       "        [0.0803],\n",
       "        [0.0767],\n",
       "        [0.1005],\n",
       "        [0.1114],\n",
       "        [0.0712]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = SoilNetFC(cnn_in_channels=12, regresor_input_from_cnn=1024, hidden_size=128).to(device)\n",
    "model = ResNet(resnet_architecture=\"101\",cnn_in_channels=12, regresor_input_from_cnn=1024, hidden_size=128).to(device)\n",
    "img_gpu = img_custom.to(device)\n",
    "print(img_gpu.device, img_gpu.shape)\n",
    "print(img_gpu.shape)\n",
    "y = model(img_gpu)\n",
    "y.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0039, device='cuda:0'), tensor(0.0081, device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(img_gpu[0]) , torch.min(img_gpu[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model:nn.Module, data_loader:DataLoader, loss_fn:nn.Module, optimizer:torch.optim.Optimizer):\n",
    "    size = len(data_loader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y.unsqueeze(1))\n",
    "        train_loss += loss.item() \n",
    "\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 2 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    train_loss = train_loss / len(data_loader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= train_step(model, train_dl, RMSELoss(), torch.optim.Adam(model.parameters(), lr=1e-4))\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test step function\n",
    "def test_step(model:nn.Module, data_loader:DataLoader, loss_fn:nn.Module):\n",
    "    size = len(data_loader.dataset)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(data_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y.unsqueeze(1))\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # if batch % 2 == 0:\n",
    "            #     loss, current = loss.item(), batch * len(X)\n",
    "            #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    test_loss /= len(data_loader)\n",
    "    print(f\"Test Loss: {test_loss:>8f}%\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= test_step(model, test_dl, RMSELoss())\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = RMSELoss(),\n",
    "          epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model=model,\n",
    "                                           data_loader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss = test_step(model=model,\n",
    "            data_loader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \",\n",
    "            f\"train_loss: {train_loss} | \",\n",
    "            f\"test_loss: {test_loss} | \"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Model is to Small it can't even overfit! | try RESNET~~\n",
    "## A lower learning rate (1e-5) solved out problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36683dd442ec47b1bb70932c63e759b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.262164  [    0/15296]\n",
      "loss: 0.128228  [   80/15296]\n",
      "loss: 0.272501  [  160/15296]\n",
      "loss: 0.110108  [  240/15296]\n",
      "loss: 0.170466  [  320/15296]\n",
      "loss: 0.313222  [  400/15296]\n",
      "loss: 0.247358  [  480/15296]\n",
      "loss: 0.311356  [  560/15296]\n",
      "loss: 0.311026  [  640/15296]\n",
      "loss: 0.175252  [  720/15296]\n",
      "loss: 0.170851  [  800/15296]\n",
      "loss: 0.306202  [  880/15296]\n",
      "loss: 0.277061  [  960/15296]\n",
      "loss: 0.281729  [ 1040/15296]\n",
      "loss: 0.180702  [ 1120/15296]\n",
      "loss: 0.182989  [ 1200/15296]\n",
      "loss: 0.239269  [ 1280/15296]\n",
      "loss: 0.211538  [ 1360/15296]\n",
      "loss: 0.196537  [ 1440/15296]\n",
      "loss: 0.244849  [ 1520/15296]\n",
      "loss: 0.288492  [ 1600/15296]\n",
      "loss: 0.204621  [ 1680/15296]\n",
      "loss: 0.314439  [ 1760/15296]\n",
      "loss: 0.106161  [ 1840/15296]\n",
      "loss: 0.328589  [ 1920/15296]\n",
      "loss: 0.281972  [ 2000/15296]\n",
      "loss: 0.231625  [ 2080/15296]\n",
      "loss: 0.253049  [ 2160/15296]\n",
      "loss: 0.249226  [ 2240/15296]\n",
      "loss: 0.273661  [ 2320/15296]\n",
      "loss: 0.253644  [ 2400/15296]\n",
      "loss: 0.188450  [ 2480/15296]\n",
      "loss: 0.179007  [ 2560/15296]\n",
      "loss: 0.181600  [ 2640/15296]\n",
      "loss: 0.313872  [ 2720/15296]\n",
      "loss: 0.214657  [ 2800/15296]\n",
      "loss: 0.182826  [ 2880/15296]\n",
      "loss: 0.190576  [ 2960/15296]\n",
      "loss: 0.182832  [ 3040/15296]\n",
      "loss: 0.161739  [ 3120/15296]\n",
      "loss: 0.208688  [ 3200/15296]\n",
      "loss: 0.165963  [ 3280/15296]\n",
      "loss: 0.097701  [ 3360/15296]\n",
      "loss: 0.179053  [ 3440/15296]\n",
      "loss: 0.172198  [ 3520/15296]\n",
      "loss: 0.351171  [ 3600/15296]\n",
      "loss: 0.211258  [ 3680/15296]\n",
      "loss: 0.206975  [ 3760/15296]\n",
      "loss: 0.179704  [ 3840/15296]\n",
      "loss: 0.121257  [ 3920/15296]\n",
      "loss: 0.114801  [ 4000/15296]\n",
      "loss: 0.069888  [ 4080/15296]\n",
      "loss: 0.336288  [ 4160/15296]\n",
      "loss: 0.256213  [ 4240/15296]\n",
      "loss: 0.157945  [ 4320/15296]\n",
      "loss: 0.122172  [ 4400/15296]\n",
      "loss: 0.250109  [ 4480/15296]\n",
      "loss: 0.185512  [ 4560/15296]\n",
      "loss: 0.212763  [ 4640/15296]\n",
      "loss: 0.249665  [ 4720/15296]\n",
      "loss: 0.185236  [ 4800/15296]\n",
      "loss: 0.167218  [ 4880/15296]\n",
      "loss: 0.230238  [ 4960/15296]\n",
      "loss: 0.168451  [ 5040/15296]\n",
      "loss: 0.197457  [ 5120/15296]\n",
      "loss: 0.238135  [ 5200/15296]\n",
      "loss: 0.219596  [ 5280/15296]\n",
      "loss: 0.179174  [ 5360/15296]\n",
      "loss: 0.179095  [ 5440/15296]\n",
      "loss: 0.107539  [ 5520/15296]\n",
      "loss: 0.322285  [ 5600/15296]\n",
      "loss: 0.176962  [ 5680/15296]\n",
      "loss: 0.281298  [ 5760/15296]\n",
      "loss: 0.167694  [ 5840/15296]\n",
      "loss: 0.256602  [ 5920/15296]\n",
      "loss: 0.216489  [ 6000/15296]\n",
      "loss: 0.206578  [ 6080/15296]\n",
      "loss: 0.145692  [ 6160/15296]\n",
      "loss: 0.189805  [ 6240/15296]\n",
      "loss: 0.280290  [ 6320/15296]\n",
      "loss: 0.168250  [ 6400/15296]\n",
      "loss: 0.162384  [ 6480/15296]\n",
      "loss: 0.203953  [ 6560/15296]\n",
      "loss: 0.212652  [ 6640/15296]\n",
      "loss: 0.213944  [ 6720/15296]\n",
      "loss: 0.189263  [ 6800/15296]\n",
      "loss: 0.249140  [ 6880/15296]\n",
      "loss: 0.167404  [ 6960/15296]\n",
      "loss: 0.205042  [ 7040/15296]\n",
      "loss: 0.192991  [ 7120/15296]\n",
      "loss: 0.183380  [ 7200/15296]\n",
      "loss: 0.180306  [ 7280/15296]\n",
      "loss: 0.249418  [ 7360/15296]\n",
      "loss: 0.265958  [ 7440/15296]\n",
      "loss: 0.159433  [ 7520/15296]\n",
      "loss: 0.273993  [ 7600/15296]\n",
      "loss: 0.186029  [ 7680/15296]\n",
      "loss: 0.177480  [ 7760/15296]\n",
      "loss: 0.209929  [ 7840/15296]\n",
      "loss: 0.135835  [ 7920/15296]\n",
      "loss: 0.267570  [ 8000/15296]\n",
      "loss: 0.229446  [ 8080/15296]\n",
      "loss: 0.306757  [ 8160/15296]\n",
      "loss: 0.217714  [ 8240/15296]\n",
      "loss: 0.166052  [ 8320/15296]\n",
      "loss: 0.198077  [ 8400/15296]\n",
      "loss: 0.120188  [ 8480/15296]\n",
      "loss: 0.345591  [ 8560/15296]\n",
      "loss: 0.208405  [ 8640/15296]\n",
      "loss: 0.290654  [ 8720/15296]\n",
      "loss: 0.190628  [ 8800/15296]\n",
      "loss: 0.249349  [ 8880/15296]\n",
      "loss: 0.307407  [ 8960/15296]\n",
      "loss: 0.187563  [ 9040/15296]\n",
      "loss: 0.211661  [ 9120/15296]\n",
      "loss: 0.256986  [ 9200/15296]\n",
      "loss: 0.260735  [ 9280/15296]\n",
      "loss: 0.169265  [ 9360/15296]\n",
      "loss: 0.264471  [ 9440/15296]\n",
      "loss: 0.190028  [ 9520/15296]\n",
      "loss: 0.174253  [ 9600/15296]\n",
      "loss: 0.163606  [ 9680/15296]\n",
      "loss: 0.203226  [ 9760/15296]\n",
      "loss: 0.199484  [ 9840/15296]\n",
      "loss: 0.166542  [ 9920/15296]\n",
      "loss: 0.279854  [10000/15296]\n",
      "loss: 0.250739  [10080/15296]\n",
      "loss: 0.201648  [10160/15296]\n",
      "loss: 0.188887  [10240/15296]\n",
      "loss: 0.206422  [10320/15296]\n",
      "loss: 0.206526  [10400/15296]\n",
      "loss: 0.215748  [10480/15296]\n",
      "loss: 0.234821  [10560/15296]\n",
      "loss: 0.165969  [10640/15296]\n",
      "loss: 0.213292  [10720/15296]\n",
      "loss: 0.332065  [10800/15296]\n",
      "loss: 0.222651  [10880/15296]\n",
      "loss: 0.225607  [10960/15296]\n",
      "loss: 0.272418  [11040/15296]\n",
      "loss: 0.167525  [11120/15296]\n",
      "loss: 0.243936  [11200/15296]\n",
      "loss: 0.268728  [11280/15296]\n",
      "loss: 0.211371  [11360/15296]\n",
      "loss: 0.183482  [11440/15296]\n",
      "loss: 0.216760  [11520/15296]\n",
      "loss: 0.246884  [11600/15296]\n",
      "loss: 0.229046  [11680/15296]\n",
      "loss: 0.254308  [11760/15296]\n",
      "loss: 0.290158  [11840/15296]\n",
      "loss: 0.202367  [11920/15296]\n",
      "loss: 0.135501  [12000/15296]\n",
      "loss: 0.183858  [12080/15296]\n",
      "loss: 0.224317  [12160/15296]\n",
      "loss: 0.204115  [12240/15296]\n",
      "loss: 0.202825  [12320/15296]\n",
      "loss: 0.225301  [12400/15296]\n",
      "loss: 0.129374  [12480/15296]\n",
      "loss: 0.231753  [12560/15296]\n",
      "loss: 0.183435  [12640/15296]\n",
      "loss: 0.221535  [12720/15296]\n",
      "loss: 0.100579  [12800/15296]\n",
      "loss: 0.284095  [12880/15296]\n",
      "loss: 0.199776  [12960/15296]\n",
      "loss: 0.277003  [13040/15296]\n",
      "loss: 0.195095  [13120/15296]\n",
      "loss: 0.165940  [13200/15296]\n",
      "loss: 0.216116  [13280/15296]\n",
      "loss: 0.225587  [13360/15296]\n",
      "loss: 0.212438  [13440/15296]\n",
      "loss: 0.245706  [13520/15296]\n",
      "loss: 0.128258  [13600/15296]\n",
      "loss: 0.224238  [13680/15296]\n",
      "loss: 0.162267  [13760/15296]\n",
      "loss: 0.197496  [13840/15296]\n",
      "loss: 0.262917  [13920/15296]\n",
      "loss: 0.249768  [14000/15296]\n",
      "loss: 0.284100  [14080/15296]\n",
      "loss: 0.220052  [14160/15296]\n",
      "loss: 0.243231  [14240/15296]\n",
      "loss: 0.290897  [14320/15296]\n",
      "loss: 0.138181  [14400/15296]\n",
      "loss: 0.181521  [14480/15296]\n",
      "loss: 0.186249  [14560/15296]\n",
      "loss: 0.176667  [14640/15296]\n",
      "loss: 0.237527  [14720/15296]\n",
      "loss: 0.143781  [14800/15296]\n",
      "loss: 0.221678  [14880/15296]\n",
      "loss: 0.270434  [14960/15296]\n",
      "loss: 0.265161  [15040/15296]\n",
      "loss: 0.200206  [15120/15296]\n",
      "loss: 0.323451  [15200/15296]\n",
      "loss: 0.271908  [ 6112/15296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.190745%\n",
      "Epoch: 1 |  train_loss: 0.2124485336924969 |  test_loss: 0.19074469124345475 | \n",
      "loss: 0.226012  [    0/15296]\n",
      "loss: 0.103859  [   80/15296]\n",
      "loss: 0.188940  [  160/15296]\n",
      "loss: 0.234730  [  240/15296]\n",
      "loss: 0.160507  [  320/15296]\n",
      "loss: 0.097801  [  400/15296]\n",
      "loss: 0.181355  [  480/15296]\n",
      "loss: 0.229649  [  560/15296]\n",
      "loss: 0.214906  [  640/15296]\n",
      "loss: 0.259706  [  720/15296]\n",
      "loss: 0.195027  [  800/15296]\n",
      "loss: 0.142075  [  880/15296]\n",
      "loss: 0.216346  [  960/15296]\n",
      "loss: 0.177041  [ 1040/15296]\n",
      "loss: 0.220427  [ 1120/15296]\n",
      "loss: 0.222111  [ 1200/15296]\n",
      "loss: 0.184099  [ 1280/15296]\n",
      "loss: 0.243941  [ 1360/15296]\n",
      "loss: 0.186323  [ 1440/15296]\n",
      "loss: 0.164113  [ 1520/15296]\n",
      "loss: 0.180122  [ 1600/15296]\n",
      "loss: 0.177560  [ 1680/15296]\n",
      "loss: 0.086970  [ 1760/15296]\n",
      "loss: 0.299391  [ 1840/15296]\n",
      "loss: 0.129401  [ 1920/15296]\n",
      "loss: 0.286828  [ 2000/15296]\n",
      "loss: 0.125230  [ 2080/15296]\n",
      "loss: 0.163177  [ 2160/15296]\n",
      "loss: 0.174043  [ 2240/15296]\n",
      "loss: 0.218924  [ 2320/15296]\n",
      "loss: 0.136674  [ 2400/15296]\n",
      "loss: 0.170222  [ 2480/15296]\n",
      "loss: 0.240653  [ 2560/15296]\n",
      "loss: 0.194000  [ 2640/15296]\n",
      "loss: 0.269172  [ 2720/15296]\n",
      "loss: 0.195844  [ 2800/15296]\n",
      "loss: 0.255618  [ 2880/15296]\n",
      "loss: 0.173531  [ 2960/15296]\n",
      "loss: 0.249717  [ 3040/15296]\n",
      "loss: 0.223629  [ 3120/15296]\n",
      "loss: 0.222963  [ 3200/15296]\n",
      "loss: 0.132365  [ 3280/15296]\n",
      "loss: 0.208029  [ 3360/15296]\n",
      "loss: 0.234218  [ 3440/15296]\n",
      "loss: 0.246735  [ 3520/15296]\n",
      "loss: 0.198044  [ 3600/15296]\n",
      "loss: 0.108911  [ 3680/15296]\n",
      "loss: 0.222008  [ 3760/15296]\n",
      "loss: 0.286726  [ 3840/15296]\n",
      "loss: 0.177236  [ 3920/15296]\n",
      "loss: 0.229416  [ 4000/15296]\n",
      "loss: 0.205819  [ 4080/15296]\n",
      "loss: 0.113970  [ 4160/15296]\n",
      "loss: 0.209089  [ 4240/15296]\n",
      "loss: 0.181283  [ 4320/15296]\n",
      "loss: 0.210884  [ 4400/15296]\n",
      "loss: 0.276965  [ 4480/15296]\n",
      "loss: 0.166602  [ 4560/15296]\n",
      "loss: 0.222522  [ 4640/15296]\n",
      "loss: 0.251135  [ 4720/15296]\n",
      "loss: 0.213816  [ 4800/15296]\n",
      "loss: 0.197675  [ 4880/15296]\n",
      "loss: 0.100244  [ 4960/15296]\n",
      "loss: 0.173149  [ 5040/15296]\n",
      "loss: 0.195469  [ 5120/15296]\n",
      "loss: 0.155698  [ 5200/15296]\n",
      "loss: 0.222414  [ 5280/15296]\n",
      "loss: 0.282789  [ 5360/15296]\n",
      "loss: 0.199448  [ 5440/15296]\n",
      "loss: 0.219808  [ 5520/15296]\n",
      "loss: 0.213704  [ 5600/15296]\n",
      "loss: 0.302410  [ 5680/15296]\n",
      "loss: 0.163412  [ 5760/15296]\n",
      "loss: 0.197555  [ 5840/15296]\n",
      "loss: 0.229369  [ 5920/15296]\n",
      "loss: 0.214964  [ 6000/15296]\n",
      "loss: 0.158722  [ 6080/15296]\n",
      "loss: 0.226241  [ 6160/15296]\n",
      "loss: 0.174368  [ 6240/15296]\n",
      "loss: 0.111404  [ 6320/15296]\n",
      "loss: 0.225015  [ 6400/15296]\n",
      "loss: 0.116167  [ 6480/15296]\n",
      "loss: 0.331901  [ 6560/15296]\n",
      "loss: 0.234344  [ 6640/15296]\n",
      "loss: 0.093045  [ 6720/15296]\n",
      "loss: 0.132918  [ 6800/15296]\n",
      "loss: 0.167485  [ 6880/15296]\n",
      "loss: 0.173207  [ 6960/15296]\n",
      "loss: 0.279728  [ 7040/15296]\n",
      "loss: 0.295750  [ 7120/15296]\n",
      "loss: 0.288035  [ 7200/15296]\n",
      "loss: 0.238591  [ 7280/15296]\n",
      "loss: 0.295545  [ 7360/15296]\n",
      "loss: 0.177197  [ 7440/15296]\n",
      "loss: 0.289274  [ 7520/15296]\n",
      "loss: 0.176826  [ 7600/15296]\n",
      "loss: 0.142349  [ 7680/15296]\n",
      "loss: 0.164591  [ 7760/15296]\n",
      "loss: 0.216336  [ 7840/15296]\n",
      "loss: 0.240939  [ 7920/15296]\n",
      "loss: 0.218137  [ 8000/15296]\n",
      "loss: 0.210099  [ 8080/15296]\n",
      "loss: 0.171517  [ 8160/15296]\n",
      "loss: 0.187712  [ 8240/15296]\n",
      "loss: 0.229724  [ 8320/15296]\n",
      "loss: 0.221134  [ 8400/15296]\n",
      "loss: 0.189658  [ 8480/15296]\n",
      "loss: 0.228572  [ 8560/15296]\n",
      "loss: 0.141444  [ 8640/15296]\n",
      "loss: 0.134653  [ 8720/15296]\n",
      "loss: 0.203863  [ 8800/15296]\n",
      "loss: 0.254696  [ 8880/15296]\n",
      "loss: 0.225423  [ 8960/15296]\n",
      "loss: 0.245008  [ 9040/15296]\n",
      "loss: 0.218017  [ 9120/15296]\n",
      "loss: 0.174899  [ 9200/15296]\n",
      "loss: 0.223542  [ 9280/15296]\n",
      "loss: 0.157751  [ 9360/15296]\n",
      "loss: 0.196083  [ 9440/15296]\n",
      "loss: 0.147228  [ 9520/15296]\n",
      "loss: 0.247768  [ 9600/15296]\n",
      "loss: 0.203252  [ 9680/15296]\n",
      "loss: 0.135542  [ 9760/15296]\n",
      "loss: 0.142407  [ 9840/15296]\n",
      "loss: 0.135506  [ 9920/15296]\n",
      "loss: 0.091620  [10000/15296]\n",
      "loss: 0.097639  [10080/15296]\n",
      "loss: 0.268791  [10160/15296]\n",
      "loss: 0.185459  [10240/15296]\n",
      "loss: 0.186781  [10320/15296]\n",
      "loss: 0.213490  [10400/15296]\n",
      "loss: 0.194329  [10480/15296]\n",
      "loss: 0.217246  [10560/15296]\n",
      "loss: 0.180637  [10640/15296]\n",
      "loss: 0.109596  [10720/15296]\n",
      "loss: 0.178536  [10800/15296]\n",
      "loss: 0.239464  [10880/15296]\n",
      "loss: 0.206598  [10960/15296]\n",
      "loss: 0.300778  [11040/15296]\n",
      "loss: 0.201895  [11120/15296]\n",
      "loss: 0.161640  [11200/15296]\n",
      "loss: 0.206026  [11280/15296]\n",
      "loss: 0.217982  [11360/15296]\n",
      "loss: 0.106665  [11440/15296]\n",
      "loss: 0.177234  [11520/15296]\n",
      "loss: 0.198352  [11600/15296]\n",
      "loss: 0.206114  [11680/15296]\n",
      "loss: 0.222559  [11760/15296]\n",
      "loss: 0.188378  [11840/15296]\n",
      "loss: 0.243761  [11920/15296]\n",
      "loss: 0.193501  [12000/15296]\n",
      "loss: 0.115548  [12080/15296]\n",
      "loss: 0.242752  [12160/15296]\n",
      "loss: 0.229513  [12240/15296]\n",
      "loss: 0.123514  [12320/15296]\n",
      "loss: 0.197928  [12400/15296]\n",
      "loss: 0.180597  [12480/15296]\n",
      "loss: 0.291541  [12560/15296]\n",
      "loss: 0.203989  [12640/15296]\n",
      "loss: 0.266526  [12720/15296]\n",
      "loss: 0.102050  [12800/15296]\n",
      "loss: 0.225652  [12880/15296]\n",
      "loss: 0.191531  [12960/15296]\n",
      "loss: 0.240084  [13040/15296]\n",
      "loss: 0.203934  [13120/15296]\n",
      "loss: 0.277469  [13200/15296]\n",
      "loss: 0.247974  [13280/15296]\n",
      "loss: 0.237699  [13360/15296]\n",
      "loss: 0.196573  [13440/15296]\n",
      "loss: 0.215844  [13520/15296]\n",
      "loss: 0.165579  [13600/15296]\n",
      "loss: 0.214998  [13680/15296]\n",
      "loss: 0.228026  [13760/15296]\n",
      "loss: 0.110816  [13840/15296]\n",
      "loss: 0.249324  [13920/15296]\n",
      "loss: 0.275846  [14000/15296]\n",
      "loss: 0.223878  [14080/15296]\n",
      "loss: 0.244786  [14160/15296]\n",
      "loss: 0.341507  [14240/15296]\n",
      "loss: 0.229983  [14320/15296]\n",
      "loss: 0.192122  [14400/15296]\n",
      "loss: 0.340604  [14480/15296]\n",
      "loss: 0.307295  [14560/15296]\n",
      "loss: 0.229913  [14640/15296]\n",
      "loss: 0.159254  [14720/15296]\n",
      "loss: 0.205720  [14800/15296]\n",
      "loss: 0.159459  [14880/15296]\n",
      "loss: 0.188443  [14960/15296]\n",
      "loss: 0.182199  [15040/15296]\n",
      "loss: 0.248659  [15120/15296]\n",
      "loss: 0.185000  [15200/15296]\n",
      "loss: 0.255545  [ 6112/15296]\n",
      "Test Loss: 0.192084%\n",
      "Epoch: 2 |  train_loss: 0.2107699076710731 |  test_loss: 0.1920838513364803 | \n",
      "loss: 0.159162  [    0/15296]\n",
      "loss: 0.243155  [   80/15296]\n",
      "loss: 0.290580  [  160/15296]\n",
      "loss: 0.274873  [  240/15296]\n",
      "loss: 0.182075  [  320/15296]\n",
      "loss: 0.261015  [  400/15296]\n",
      "loss: 0.229726  [  480/15296]\n",
      "loss: 0.219122  [  560/15296]\n",
      "loss: 0.241935  [  640/15296]\n",
      "loss: 0.275060  [  720/15296]\n",
      "loss: 0.275366  [  800/15296]\n",
      "loss: 0.238750  [  880/15296]\n",
      "loss: 0.176890  [  960/15296]\n",
      "loss: 0.235808  [ 1040/15296]\n",
      "loss: 0.075363  [ 1120/15296]\n",
      "loss: 0.206816  [ 1200/15296]\n",
      "loss: 0.228678  [ 1280/15296]\n",
      "loss: 0.216188  [ 1360/15296]\n",
      "loss: 0.172356  [ 1440/15296]\n",
      "loss: 0.162776  [ 1520/15296]\n",
      "loss: 0.207116  [ 1600/15296]\n",
      "loss: 0.210818  [ 1680/15296]\n",
      "loss: 0.208452  [ 1760/15296]\n",
      "loss: 0.267276  [ 1840/15296]\n",
      "loss: 0.250285  [ 1920/15296]\n",
      "loss: 0.219151  [ 2000/15296]\n",
      "loss: 0.146698  [ 2080/15296]\n",
      "loss: 0.296110  [ 2160/15296]\n",
      "loss: 0.195549  [ 2240/15296]\n",
      "loss: 0.191706  [ 2320/15296]\n",
      "loss: 0.213892  [ 2400/15296]\n",
      "loss: 0.089966  [ 2480/15296]\n",
      "loss: 0.191504  [ 2560/15296]\n",
      "loss: 0.267876  [ 2640/15296]\n",
      "loss: 0.177731  [ 2720/15296]\n",
      "loss: 0.182483  [ 2800/15296]\n",
      "loss: 0.267518  [ 2880/15296]\n",
      "loss: 0.303008  [ 2960/15296]\n",
      "loss: 0.153564  [ 3040/15296]\n",
      "loss: 0.122538  [ 3120/15296]\n",
      "loss: 0.225927  [ 3200/15296]\n",
      "loss: 0.240256  [ 3280/15296]\n",
      "loss: 0.253646  [ 3360/15296]\n",
      "loss: 0.205475  [ 3440/15296]\n",
      "loss: 0.218065  [ 3520/15296]\n",
      "loss: 0.258673  [ 3600/15296]\n",
      "loss: 0.110269  [ 3680/15296]\n",
      "loss: 0.247690  [ 3760/15296]\n",
      "loss: 0.168999  [ 3840/15296]\n",
      "loss: 0.276975  [ 3920/15296]\n",
      "loss: 0.214381  [ 4000/15296]\n",
      "loss: 0.205669  [ 4080/15296]\n",
      "loss: 0.171472  [ 4160/15296]\n",
      "loss: 0.275576  [ 4240/15296]\n",
      "loss: 0.210540  [ 4320/15296]\n",
      "loss: 0.207976  [ 4400/15296]\n",
      "loss: 0.221108  [ 4480/15296]\n",
      "loss: 0.193881  [ 4560/15296]\n",
      "loss: 0.187932  [ 4640/15296]\n",
      "loss: 0.307596  [ 4720/15296]\n",
      "loss: 0.256646  [ 4800/15296]\n",
      "loss: 0.220716  [ 4880/15296]\n",
      "loss: 0.270110  [ 4960/15296]\n",
      "loss: 0.227151  [ 5040/15296]\n",
      "loss: 0.263941  [ 5120/15296]\n",
      "loss: 0.198251  [ 5200/15296]\n",
      "loss: 0.235041  [ 5280/15296]\n",
      "loss: 0.129768  [ 5360/15296]\n",
      "loss: 0.237758  [ 5440/15296]\n",
      "loss: 0.189726  [ 5520/15296]\n",
      "loss: 0.127387  [ 5600/15296]\n",
      "loss: 0.141028  [ 5680/15296]\n",
      "loss: 0.213997  [ 5760/15296]\n",
      "loss: 0.186137  [ 5840/15296]\n",
      "loss: 0.243270  [ 5920/15296]\n",
      "loss: 0.241428  [ 6000/15296]\n",
      "loss: 0.224103  [ 6080/15296]\n",
      "loss: 0.151611  [ 6160/15296]\n",
      "loss: 0.194557  [ 6240/15296]\n",
      "loss: 0.176094  [ 6320/15296]\n",
      "loss: 0.258875  [ 6400/15296]\n",
      "loss: 0.155452  [ 6480/15296]\n",
      "loss: 0.114434  [ 6560/15296]\n",
      "loss: 0.280574  [ 6640/15296]\n",
      "loss: 0.255981  [ 6720/15296]\n",
      "loss: 0.196738  [ 6800/15296]\n",
      "loss: 0.202149  [ 6880/15296]\n",
      "loss: 0.159312  [ 6960/15296]\n",
      "loss: 0.165070  [ 7040/15296]\n",
      "loss: 0.281509  [ 7120/15296]\n",
      "loss: 0.217274  [ 7200/15296]\n",
      "loss: 0.188404  [ 7280/15296]\n",
      "loss: 0.164413  [ 7360/15296]\n",
      "loss: 0.231131  [ 7440/15296]\n",
      "loss: 0.269928  [ 7520/15296]\n",
      "loss: 0.262419  [ 7600/15296]\n",
      "loss: 0.267088  [ 7680/15296]\n",
      "loss: 0.184955  [ 7760/15296]\n",
      "loss: 0.163967  [ 7840/15296]\n",
      "loss: 0.186126  [ 7920/15296]\n",
      "loss: 0.225269  [ 8000/15296]\n",
      "loss: 0.222445  [ 8080/15296]\n",
      "loss: 0.165155  [ 8160/15296]\n",
      "loss: 0.282905  [ 8240/15296]\n",
      "loss: 0.228091  [ 8320/15296]\n",
      "loss: 0.278234  [ 8400/15296]\n",
      "loss: 0.289785  [ 8480/15296]\n",
      "loss: 0.187373  [ 8560/15296]\n",
      "loss: 0.244286  [ 8640/15296]\n",
      "loss: 0.255004  [ 8720/15296]\n",
      "loss: 0.180105  [ 8800/15296]\n",
      "loss: 0.099926  [ 8880/15296]\n",
      "loss: 0.292280  [ 8960/15296]\n",
      "loss: 0.229900  [ 9040/15296]\n",
      "loss: 0.218836  [ 9120/15296]\n",
      "loss: 0.263379  [ 9200/15296]\n",
      "loss: 0.158430  [ 9280/15296]\n",
      "loss: 0.171397  [ 9360/15296]\n",
      "loss: 0.254197  [ 9440/15296]\n",
      "loss: 0.260023  [ 9520/15296]\n",
      "loss: 0.218590  [ 9600/15296]\n",
      "loss: 0.255881  [ 9680/15296]\n",
      "loss: 0.232283  [ 9760/15296]\n",
      "loss: 0.238162  [ 9840/15296]\n",
      "loss: 0.183531  [ 9920/15296]\n",
      "loss: 0.178794  [10000/15296]\n",
      "loss: 0.263450  [10080/15296]\n",
      "loss: 0.242559  [10160/15296]\n",
      "loss: 0.254310  [10240/15296]\n",
      "loss: 0.232646  [10320/15296]\n",
      "loss: 0.214552  [10400/15296]\n",
      "loss: 0.251271  [10480/15296]\n",
      "loss: 0.247534  [10560/15296]\n",
      "loss: 0.226538  [10640/15296]\n",
      "loss: 0.169779  [10720/15296]\n",
      "loss: 0.217982  [10800/15296]\n",
      "loss: 0.234188  [10880/15296]\n",
      "loss: 0.249044  [10960/15296]\n",
      "loss: 0.231523  [11040/15296]\n",
      "loss: 0.194588  [11120/15296]\n",
      "loss: 0.224116  [11200/15296]\n",
      "loss: 0.160413  [11280/15296]\n",
      "loss: 0.236675  [11360/15296]\n",
      "loss: 0.225824  [11440/15296]\n",
      "loss: 0.134049  [11520/15296]\n",
      "loss: 0.185413  [11600/15296]\n",
      "loss: 0.120488  [11680/15296]\n",
      "loss: 0.207042  [11760/15296]\n",
      "loss: 0.199661  [11840/15296]\n",
      "loss: 0.239197  [11920/15296]\n",
      "loss: 0.213521  [12000/15296]\n",
      "loss: 0.232364  [12080/15296]\n",
      "loss: 0.114623  [12160/15296]\n",
      "loss: 0.197446  [12240/15296]\n",
      "loss: 0.157909  [12320/15296]\n",
      "loss: 0.353729  [12400/15296]\n",
      "loss: 0.145317  [12480/15296]\n",
      "loss: 0.191955  [12560/15296]\n",
      "loss: 0.188797  [12640/15296]\n",
      "loss: 0.299880  [12720/15296]\n",
      "loss: 0.192694  [12800/15296]\n",
      "loss: 0.246612  [12880/15296]\n",
      "loss: 0.197989  [12960/15296]\n",
      "loss: 0.248999  [13040/15296]\n",
      "loss: 0.272720  [13120/15296]\n",
      "loss: 0.112318  [13200/15296]\n",
      "loss: 0.186137  [13280/15296]\n",
      "loss: 0.241989  [13360/15296]\n",
      "loss: 0.206728  [13440/15296]\n",
      "loss: 0.260297  [13520/15296]\n",
      "loss: 0.295928  [13600/15296]\n",
      "loss: 0.170170  [13680/15296]\n",
      "loss: 0.111805  [13760/15296]\n",
      "loss: 0.171406  [13840/15296]\n",
      "loss: 0.177399  [13920/15296]\n",
      "loss: 0.174522  [14000/15296]\n",
      "loss: 0.141690  [14080/15296]\n",
      "loss: 0.190420  [14160/15296]\n",
      "loss: 0.178683  [14240/15296]\n",
      "loss: 0.213600  [14320/15296]\n",
      "loss: 0.245240  [14400/15296]\n",
      "loss: 0.262511  [14480/15296]\n",
      "loss: 0.166175  [14560/15296]\n",
      "loss: 0.297856  [14640/15296]\n",
      "loss: 0.202685  [14720/15296]\n",
      "loss: 0.240531  [14800/15296]\n",
      "loss: 0.211547  [14880/15296]\n",
      "loss: 0.170021  [14960/15296]\n",
      "loss: 0.254890  [15040/15296]\n",
      "loss: 0.182331  [15120/15296]\n",
      "loss: 0.221575  [15200/15296]\n",
      "loss: 0.231270  [ 6112/15296]\n",
      "Test Loss: 0.197990%\n",
      "Epoch: 3 |  train_loss: 0.21193355847120907 |  test_loss: 0.1979904960007428 | \n",
      "loss: 0.301900  [    0/15296]\n",
      "loss: 0.334516  [   80/15296]\n",
      "loss: 0.166658  [  160/15296]\n",
      "loss: 0.200531  [  240/15296]\n",
      "loss: 0.271365  [  320/15296]\n",
      "loss: 0.111041  [  400/15296]\n",
      "loss: 0.295309  [  480/15296]\n",
      "loss: 0.154976  [  560/15296]\n",
      "loss: 0.270353  [  640/15296]\n",
      "loss: 0.248524  [  720/15296]\n",
      "loss: 0.348234  [  800/15296]\n",
      "loss: 0.156666  [  880/15296]\n",
      "loss: 0.206722  [  960/15296]\n",
      "loss: 0.126460  [ 1040/15296]\n",
      "loss: 0.225220  [ 1120/15296]\n",
      "loss: 0.239247  [ 1200/15296]\n",
      "loss: 0.393810  [ 1280/15296]\n",
      "loss: 0.264483  [ 1360/15296]\n",
      "loss: 0.173352  [ 1440/15296]\n",
      "loss: 0.208982  [ 1520/15296]\n",
      "loss: 0.358195  [ 1600/15296]\n",
      "loss: 0.227935  [ 1680/15296]\n",
      "loss: 0.262066  [ 1760/15296]\n",
      "loss: 0.166052  [ 1840/15296]\n",
      "loss: 0.291029  [ 1920/15296]\n",
      "loss: 0.270294  [ 2000/15296]\n",
      "loss: 0.181001  [ 2080/15296]\n",
      "loss: 0.215781  [ 2160/15296]\n",
      "loss: 0.231896  [ 2240/15296]\n",
      "loss: 0.122607  [ 2320/15296]\n",
      "loss: 0.261578  [ 2400/15296]\n",
      "loss: 0.249315  [ 2480/15296]\n",
      "loss: 0.209813  [ 2560/15296]\n",
      "loss: 0.259864  [ 2640/15296]\n",
      "loss: 0.173044  [ 2720/15296]\n",
      "loss: 0.230992  [ 2800/15296]\n",
      "loss: 0.279591  [ 2880/15296]\n",
      "loss: 0.250680  [ 2960/15296]\n",
      "loss: 0.224173  [ 3040/15296]\n",
      "loss: 0.165808  [ 3120/15296]\n",
      "loss: 0.163806  [ 3200/15296]\n",
      "loss: 0.109342  [ 3280/15296]\n",
      "loss: 0.189283  [ 3360/15296]\n",
      "loss: 0.290467  [ 3440/15296]\n",
      "loss: 0.300523  [ 3520/15296]\n",
      "loss: 0.212208  [ 3600/15296]\n",
      "loss: 0.252963  [ 3680/15296]\n",
      "loss: 0.151380  [ 3760/15296]\n",
      "loss: 0.215228  [ 3840/15296]\n",
      "loss: 0.243648  [ 3920/15296]\n",
      "loss: 0.212988  [ 4000/15296]\n",
      "loss: 0.169502  [ 4080/15296]\n",
      "loss: 0.215617  [ 4160/15296]\n",
      "loss: 0.205199  [ 4240/15296]\n",
      "loss: 0.201057  [ 4320/15296]\n",
      "loss: 0.237691  [ 4400/15296]\n",
      "loss: 0.294007  [ 4480/15296]\n",
      "loss: 0.145964  [ 4560/15296]\n",
      "loss: 0.230033  [ 4640/15296]\n",
      "loss: 0.175880  [ 4720/15296]\n",
      "loss: 0.264240  [ 4800/15296]\n",
      "loss: 0.204811  [ 4880/15296]\n",
      "loss: 0.271922  [ 4960/15296]\n",
      "loss: 0.190857  [ 5040/15296]\n",
      "loss: 0.111405  [ 5120/15296]\n",
      "loss: 0.187093  [ 5200/15296]\n",
      "loss: 0.270636  [ 5280/15296]\n",
      "loss: 0.171525  [ 5360/15296]\n",
      "loss: 0.187772  [ 5440/15296]\n",
      "loss: 0.181587  [ 5520/15296]\n",
      "loss: 0.171015  [ 5600/15296]\n",
      "loss: 0.262841  [ 5680/15296]\n",
      "loss: 0.229603  [ 5760/15296]\n",
      "loss: 0.154942  [ 5840/15296]\n",
      "loss: 0.236433  [ 5920/15296]\n",
      "loss: 0.094617  [ 6000/15296]\n",
      "loss: 0.172511  [ 6080/15296]\n",
      "loss: 0.209764  [ 6160/15296]\n",
      "loss: 0.199588  [ 6240/15296]\n",
      "loss: 0.279829  [ 6320/15296]\n",
      "loss: 0.168739  [ 6400/15296]\n",
      "loss: 0.221833  [ 6480/15296]\n",
      "loss: 0.216122  [ 6560/15296]\n",
      "loss: 0.309550  [ 6640/15296]\n",
      "loss: 0.252343  [ 6720/15296]\n",
      "loss: 0.277051  [ 6800/15296]\n",
      "loss: 0.224096  [ 6880/15296]\n",
      "loss: 0.187785  [ 6960/15296]\n",
      "loss: 0.216215  [ 7040/15296]\n",
      "loss: 0.257500  [ 7120/15296]\n",
      "loss: 0.178716  [ 7200/15296]\n",
      "loss: 0.193542  [ 7280/15296]\n",
      "loss: 0.282226  [ 7360/15296]\n",
      "loss: 0.195741  [ 7440/15296]\n",
      "loss: 0.162426  [ 7520/15296]\n",
      "loss: 0.254649  [ 7600/15296]\n",
      "loss: 0.207125  [ 7680/15296]\n",
      "loss: 0.166580  [ 7760/15296]\n",
      "loss: 0.251559  [ 7840/15296]\n",
      "loss: 0.193587  [ 7920/15296]\n",
      "loss: 0.247351  [ 8000/15296]\n",
      "loss: 0.202625  [ 8080/15296]\n",
      "loss: 0.224307  [ 8160/15296]\n",
      "loss: 0.250771  [ 8240/15296]\n",
      "loss: 0.198402  [ 8320/15296]\n",
      "loss: 0.206489  [ 8400/15296]\n",
      "loss: 0.247797  [ 8480/15296]\n",
      "loss: 0.246187  [ 8560/15296]\n",
      "loss: 0.294733  [ 8640/15296]\n",
      "loss: 0.199491  [ 8720/15296]\n",
      "loss: 0.218844  [ 8800/15296]\n",
      "loss: 0.153195  [ 8880/15296]\n",
      "loss: 0.217703  [ 8960/15296]\n",
      "loss: 0.117074  [ 9040/15296]\n",
      "loss: 0.228484  [ 9120/15296]\n",
      "loss: 0.236658  [ 9200/15296]\n",
      "loss: 0.117517  [ 9280/15296]\n",
      "loss: 0.148865  [ 9360/15296]\n",
      "loss: 0.253404  [ 9440/15296]\n",
      "loss: 0.230700  [ 9520/15296]\n",
      "loss: 0.219256  [ 9600/15296]\n",
      "loss: 0.237099  [ 9680/15296]\n",
      "loss: 0.224610  [ 9760/15296]\n",
      "loss: 0.141415  [ 9840/15296]\n",
      "loss: 0.214588  [ 9920/15296]\n",
      "loss: 0.159867  [10000/15296]\n",
      "loss: 0.322309  [10080/15296]\n",
      "loss: 0.225930  [10160/15296]\n",
      "loss: 0.103215  [10240/15296]\n",
      "loss: 0.299654  [10320/15296]\n",
      "loss: 0.224785  [10400/15296]\n",
      "loss: 0.259751  [10480/15296]\n",
      "loss: 0.326238  [10560/15296]\n",
      "loss: 0.167898  [10640/15296]\n",
      "loss: 0.242444  [10720/15296]\n",
      "loss: 0.242291  [10800/15296]\n",
      "loss: 0.302725  [10880/15296]\n",
      "loss: 0.282809  [10960/15296]\n",
      "loss: 0.138323  [11040/15296]\n",
      "loss: 0.255308  [11120/15296]\n",
      "loss: 0.188571  [11200/15296]\n",
      "loss: 0.194405  [11280/15296]\n",
      "loss: 0.098584  [11360/15296]\n",
      "loss: 0.222121  [11440/15296]\n",
      "loss: 0.100302  [11520/15296]\n",
      "loss: 0.101384  [11600/15296]\n",
      "loss: 0.299931  [11680/15296]\n",
      "loss: 0.195493  [11760/15296]\n",
      "loss: 0.250914  [11840/15296]\n",
      "loss: 0.248712  [11920/15296]\n",
      "loss: 0.221269  [12000/15296]\n",
      "loss: 0.091966  [12080/15296]\n",
      "loss: 0.346094  [12160/15296]\n",
      "loss: 0.223479  [12240/15296]\n",
      "loss: 0.295541  [12320/15296]\n",
      "loss: 0.191668  [12400/15296]\n",
      "loss: 0.256599  [12480/15296]\n",
      "loss: 0.165076  [12560/15296]\n",
      "loss: 0.194129  [12640/15296]\n",
      "loss: 0.195097  [12720/15296]\n",
      "loss: 0.157410  [12800/15296]\n",
      "loss: 0.215961  [12880/15296]\n",
      "loss: 0.175150  [12960/15296]\n",
      "loss: 0.128161  [13040/15296]\n",
      "loss: 0.162125  [13120/15296]\n",
      "loss: 0.121488  [13200/15296]\n",
      "loss: 0.283597  [13280/15296]\n",
      "loss: 0.273404  [13360/15296]\n",
      "loss: 0.202030  [13440/15296]\n",
      "loss: 0.161181  [13520/15296]\n",
      "loss: 0.142125  [13600/15296]\n",
      "loss: 0.194398  [13680/15296]\n",
      "loss: 0.182643  [13760/15296]\n",
      "loss: 0.205183  [13840/15296]\n",
      "loss: 0.157118  [13920/15296]\n",
      "loss: 0.211031  [14000/15296]\n",
      "loss: 0.234980  [14080/15296]\n",
      "loss: 0.312853  [14160/15296]\n",
      "loss: 0.120852  [14240/15296]\n",
      "loss: 0.243430  [14320/15296]\n",
      "loss: 0.255144  [14400/15296]\n",
      "loss: 0.251733  [14480/15296]\n",
      "loss: 0.160756  [14560/15296]\n",
      "loss: 0.207622  [14640/15296]\n",
      "loss: 0.283745  [14720/15296]\n",
      "loss: 0.138983  [14800/15296]\n",
      "loss: 0.206140  [14880/15296]\n",
      "loss: 0.197337  [14960/15296]\n",
      "loss: 0.280635  [15040/15296]\n",
      "loss: 0.094966  [15120/15296]\n",
      "loss: 0.252163  [15200/15296]\n",
      "loss: 0.236473  [ 6112/15296]\n",
      "Test Loss: 0.191843%\n",
      "Epoch: 4 |  train_loss: 0.21074989611190542 |  test_loss: 0.19184270575982795 | \n",
      "loss: 0.289485  [    0/15296]\n",
      "loss: 0.161895  [   80/15296]\n",
      "loss: 0.147301  [  160/15296]\n",
      "loss: 0.292390  [  240/15296]\n",
      "loss: 0.235839  [  320/15296]\n",
      "loss: 0.252805  [  400/15296]\n",
      "loss: 0.201373  [  480/15296]\n",
      "loss: 0.213191  [  560/15296]\n",
      "loss: 0.225574  [  640/15296]\n",
      "loss: 0.199321  [  720/15296]\n",
      "loss: 0.269265  [  800/15296]\n",
      "loss: 0.254797  [  880/15296]\n",
      "loss: 0.202617  [  960/15296]\n",
      "loss: 0.217274  [ 1040/15296]\n",
      "loss: 0.177240  [ 1120/15296]\n",
      "loss: 0.183407  [ 1200/15296]\n",
      "loss: 0.312249  [ 1280/15296]\n",
      "loss: 0.247702  [ 1360/15296]\n",
      "loss: 0.128452  [ 1440/15296]\n",
      "loss: 0.254528  [ 1520/15296]\n",
      "loss: 0.135116  [ 1600/15296]\n",
      "loss: 0.158750  [ 1680/15296]\n",
      "loss: 0.101962  [ 1760/15296]\n",
      "loss: 0.281847  [ 1840/15296]\n",
      "loss: 0.245367  [ 1920/15296]\n",
      "loss: 0.291198  [ 2000/15296]\n",
      "loss: 0.225448  [ 2080/15296]\n",
      "loss: 0.111107  [ 2160/15296]\n",
      "loss: 0.163291  [ 2240/15296]\n",
      "loss: 0.192667  [ 2320/15296]\n",
      "loss: 0.254811  [ 2400/15296]\n",
      "loss: 0.209134  [ 2480/15296]\n",
      "loss: 0.203502  [ 2560/15296]\n",
      "loss: 0.197822  [ 2640/15296]\n",
      "loss: 0.161686  [ 2720/15296]\n",
      "loss: 0.167627  [ 2800/15296]\n",
      "loss: 0.296960  [ 2880/15296]\n",
      "loss: 0.239143  [ 2960/15296]\n",
      "loss: 0.267523  [ 3040/15296]\n",
      "loss: 0.223378  [ 3120/15296]\n",
      "loss: 0.278885  [ 3200/15296]\n",
      "loss: 0.172283  [ 3280/15296]\n",
      "loss: 0.219496  [ 3360/15296]\n",
      "loss: 0.194722  [ 3440/15296]\n",
      "loss: 0.160855  [ 3520/15296]\n",
      "loss: 0.185306  [ 3600/15296]\n",
      "loss: 0.123184  [ 3680/15296]\n",
      "loss: 0.272385  [ 3760/15296]\n",
      "loss: 0.182431  [ 3840/15296]\n",
      "loss: 0.225139  [ 3920/15296]\n",
      "loss: 0.176667  [ 4000/15296]\n",
      "loss: 0.330656  [ 4080/15296]\n",
      "loss: 0.201668  [ 4160/15296]\n",
      "loss: 0.251057  [ 4240/15296]\n",
      "loss: 0.244106  [ 4320/15296]\n",
      "loss: 0.239310  [ 4400/15296]\n",
      "loss: 0.169920  [ 4480/15296]\n",
      "loss: 0.121424  [ 4560/15296]\n",
      "loss: 0.254697  [ 4640/15296]\n",
      "loss: 0.230180  [ 4720/15296]\n",
      "loss: 0.270245  [ 4800/15296]\n",
      "loss: 0.223534  [ 4880/15296]\n",
      "loss: 0.186176  [ 4960/15296]\n",
      "loss: 0.244307  [ 5040/15296]\n",
      "loss: 0.265818  [ 5120/15296]\n",
      "loss: 0.246046  [ 5200/15296]\n",
      "loss: 0.125657  [ 5280/15296]\n",
      "loss: 0.157517  [ 5360/15296]\n",
      "loss: 0.320113  [ 5440/15296]\n",
      "loss: 0.311067  [ 5520/15296]\n",
      "loss: 0.196167  [ 5600/15296]\n",
      "loss: 0.287115  [ 5680/15296]\n",
      "loss: 0.135674  [ 5760/15296]\n",
      "loss: 0.222577  [ 5840/15296]\n",
      "loss: 0.185297  [ 5920/15296]\n",
      "loss: 0.237322  [ 6000/15296]\n",
      "loss: 0.170918  [ 6080/15296]\n",
      "loss: 0.179744  [ 6160/15296]\n",
      "loss: 0.209717  [ 6240/15296]\n",
      "loss: 0.222049  [ 6320/15296]\n",
      "loss: 0.211666  [ 6400/15296]\n",
      "loss: 0.138598  [ 6480/15296]\n",
      "loss: 0.112772  [ 6560/15296]\n",
      "loss: 0.254595  [ 6640/15296]\n",
      "loss: 0.230704  [ 6720/15296]\n",
      "loss: 0.217485  [ 6800/15296]\n",
      "loss: 0.283955  [ 6880/15296]\n",
      "loss: 0.162347  [ 6960/15296]\n",
      "loss: 0.209219  [ 7040/15296]\n",
      "loss: 0.231904  [ 7120/15296]\n",
      "loss: 0.243580  [ 7200/15296]\n",
      "loss: 0.215103  [ 7280/15296]\n",
      "loss: 0.148853  [ 7360/15296]\n",
      "loss: 0.212757  [ 7440/15296]\n",
      "loss: 0.243239  [ 7520/15296]\n",
      "loss: 0.126641  [ 7600/15296]\n",
      "loss: 0.309442  [ 7680/15296]\n",
      "loss: 0.201559  [ 7760/15296]\n",
      "loss: 0.236154  [ 7840/15296]\n",
      "loss: 0.170197  [ 7920/15296]\n",
      "loss: 0.117197  [ 8000/15296]\n",
      "loss: 0.146114  [ 8080/15296]\n",
      "loss: 0.173764  [ 8160/15296]\n",
      "loss: 0.103767  [ 8240/15296]\n",
      "loss: 0.311826  [ 8320/15296]\n",
      "loss: 0.222406  [ 8400/15296]\n",
      "loss: 0.203204  [ 8480/15296]\n",
      "loss: 0.180785  [ 8560/15296]\n",
      "loss: 0.086151  [ 8640/15296]\n",
      "loss: 0.234184  [ 8720/15296]\n",
      "loss: 0.324797  [ 8800/15296]\n",
      "loss: 0.252935  [ 8880/15296]\n",
      "loss: 0.194310  [ 8960/15296]\n",
      "loss: 0.214005  [ 9040/15296]\n",
      "loss: 0.098818  [ 9120/15296]\n",
      "loss: 0.162961  [ 9200/15296]\n",
      "loss: 0.187184  [ 9280/15296]\n",
      "loss: 0.293769  [ 9360/15296]\n",
      "loss: 0.251826  [ 9440/15296]\n",
      "loss: 0.223049  [ 9520/15296]\n",
      "loss: 0.192535  [ 9600/15296]\n",
      "loss: 0.323195  [ 9680/15296]\n",
      "loss: 0.192888  [ 9760/15296]\n",
      "loss: 0.184531  [ 9840/15296]\n",
      "loss: 0.293793  [ 9920/15296]\n",
      "loss: 0.261349  [10000/15296]\n",
      "loss: 0.195459  [10080/15296]\n",
      "loss: 0.216546  [10160/15296]\n",
      "loss: 0.188568  [10240/15296]\n",
      "loss: 0.212204  [10320/15296]\n",
      "loss: 0.255559  [10400/15296]\n",
      "loss: 0.266461  [10480/15296]\n",
      "loss: 0.256895  [10560/15296]\n",
      "loss: 0.196326  [10640/15296]\n",
      "loss: 0.313280  [10720/15296]\n",
      "loss: 0.238068  [10800/15296]\n",
      "loss: 0.135370  [10880/15296]\n",
      "loss: 0.266711  [10960/15296]\n",
      "loss: 0.135389  [11040/15296]\n",
      "loss: 0.195069  [11120/15296]\n",
      "loss: 0.183082  [11200/15296]\n",
      "loss: 0.272796  [11280/15296]\n",
      "loss: 0.193242  [11360/15296]\n",
      "loss: 0.264696  [11440/15296]\n",
      "loss: 0.118408  [11520/15296]\n",
      "loss: 0.135967  [11600/15296]\n",
      "loss: 0.128608  [11680/15296]\n",
      "loss: 0.195682  [11760/15296]\n",
      "loss: 0.148001  [11840/15296]\n",
      "loss: 0.206809  [11920/15296]\n",
      "loss: 0.218008  [12000/15296]\n",
      "loss: 0.293095  [12080/15296]\n",
      "loss: 0.201314  [12160/15296]\n",
      "loss: 0.227432  [12240/15296]\n",
      "loss: 0.317282  [12320/15296]\n",
      "loss: 0.206946  [12400/15296]\n",
      "loss: 0.144523  [12480/15296]\n",
      "loss: 0.206804  [12560/15296]\n",
      "loss: 0.161603  [12640/15296]\n",
      "loss: 0.290969  [12720/15296]\n",
      "loss: 0.263432  [12800/15296]\n",
      "loss: 0.312253  [12880/15296]\n",
      "loss: 0.117921  [12960/15296]\n",
      "loss: 0.192368  [13040/15296]\n",
      "loss: 0.094123  [13120/15296]\n",
      "loss: 0.163001  [13200/15296]\n",
      "loss: 0.140588  [13280/15296]\n",
      "loss: 0.244743  [13360/15296]\n",
      "loss: 0.194965  [13440/15296]\n",
      "loss: 0.255432  [13520/15296]\n",
      "loss: 0.102932  [13600/15296]\n",
      "loss: 0.134248  [13680/15296]\n",
      "loss: 0.180269  [13760/15296]\n",
      "loss: 0.179300  [13840/15296]\n",
      "loss: 0.172140  [13920/15296]\n",
      "loss: 0.221907  [14000/15296]\n",
      "loss: 0.212851  [14080/15296]\n",
      "loss: 0.182820  [14160/15296]\n",
      "loss: 0.236191  [14240/15296]\n",
      "loss: 0.133201  [14320/15296]\n",
      "loss: 0.137434  [14400/15296]\n",
      "loss: 0.310101  [14480/15296]\n",
      "loss: 0.245946  [14560/15296]\n",
      "loss: 0.259044  [14640/15296]\n",
      "loss: 0.171952  [14720/15296]\n",
      "loss: 0.262469  [14800/15296]\n",
      "loss: 0.216847  [14880/15296]\n",
      "loss: 0.196017  [14960/15296]\n",
      "loss: 0.195634  [15040/15296]\n",
      "loss: 0.200439  [15120/15296]\n",
      "loss: 0.115151  [15200/15296]\n",
      "loss: 0.097567  [ 6112/15296]\n",
      "Test Loss: 0.192100%\n",
      "Epoch: 5 |  train_loss: 0.20975886156913817 |  test_loss: 0.19209989132232044 | \n",
      "loss: 0.266826  [    0/15296]\n",
      "loss: 0.205364  [   80/15296]\n",
      "loss: 0.125990  [  160/15296]\n",
      "loss: 0.276611  [  240/15296]\n",
      "loss: 0.220615  [  320/15296]\n",
      "loss: 0.246915  [  400/15296]\n",
      "loss: 0.255967  [  480/15296]\n",
      "loss: 0.101780  [  560/15296]\n",
      "loss: 0.135727  [  640/15296]\n",
      "loss: 0.092730  [  720/15296]\n",
      "loss: 0.236638  [  800/15296]\n",
      "loss: 0.226467  [  880/15296]\n",
      "loss: 0.355678  [  960/15296]\n",
      "loss: 0.225666  [ 1040/15296]\n",
      "loss: 0.225160  [ 1120/15296]\n",
      "loss: 0.280528  [ 1200/15296]\n",
      "loss: 0.174757  [ 1280/15296]\n",
      "loss: 0.207932  [ 1360/15296]\n",
      "loss: 0.301079  [ 1440/15296]\n",
      "loss: 0.168360  [ 1520/15296]\n",
      "loss: 0.248573  [ 1600/15296]\n",
      "loss: 0.103039  [ 1680/15296]\n",
      "loss: 0.261868  [ 1760/15296]\n",
      "loss: 0.166840  [ 1840/15296]\n",
      "loss: 0.212295  [ 1920/15296]\n",
      "loss: 0.195995  [ 2000/15296]\n",
      "loss: 0.170416  [ 2080/15296]\n",
      "loss: 0.283885  [ 2160/15296]\n",
      "loss: 0.217873  [ 2240/15296]\n",
      "loss: 0.173174  [ 2320/15296]\n",
      "loss: 0.096734  [ 2400/15296]\n",
      "loss: 0.229444  [ 2480/15296]\n",
      "loss: 0.208192  [ 2560/15296]\n",
      "loss: 0.296787  [ 2640/15296]\n",
      "loss: 0.210288  [ 2720/15296]\n",
      "loss: 0.254527  [ 2800/15296]\n",
      "loss: 0.236475  [ 2880/15296]\n",
      "loss: 0.223461  [ 2960/15296]\n",
      "loss: 0.174186  [ 3040/15296]\n",
      "loss: 0.262750  [ 3120/15296]\n",
      "loss: 0.150682  [ 3200/15296]\n",
      "loss: 0.244442  [ 3280/15296]\n",
      "loss: 0.182817  [ 3360/15296]\n",
      "loss: 0.145420  [ 3440/15296]\n",
      "loss: 0.231984  [ 3520/15296]\n",
      "loss: 0.185567  [ 3600/15296]\n",
      "loss: 0.265786  [ 3680/15296]\n",
      "loss: 0.218632  [ 3760/15296]\n",
      "loss: 0.217784  [ 3840/15296]\n",
      "loss: 0.160299  [ 3920/15296]\n",
      "loss: 0.245257  [ 4000/15296]\n",
      "loss: 0.230125  [ 4080/15296]\n",
      "loss: 0.242115  [ 4160/15296]\n",
      "loss: 0.236072  [ 4240/15296]\n",
      "loss: 0.181448  [ 4320/15296]\n",
      "loss: 0.243879  [ 4400/15296]\n",
      "loss: 0.218755  [ 4480/15296]\n",
      "loss: 0.169291  [ 4560/15296]\n",
      "loss: 0.221825  [ 4640/15296]\n",
      "loss: 0.325438  [ 4720/15296]\n",
      "loss: 0.199375  [ 4800/15296]\n",
      "loss: 0.182133  [ 4880/15296]\n",
      "loss: 0.233745  [ 4960/15296]\n",
      "loss: 0.271176  [ 5040/15296]\n",
      "loss: 0.232410  [ 5120/15296]\n",
      "loss: 0.245379  [ 5200/15296]\n",
      "loss: 0.252955  [ 5280/15296]\n",
      "loss: 0.284780  [ 5360/15296]\n",
      "loss: 0.232920  [ 5440/15296]\n",
      "loss: 0.263335  [ 5520/15296]\n",
      "loss: 0.179827  [ 5600/15296]\n",
      "loss: 0.285940  [ 5680/15296]\n",
      "loss: 0.209224  [ 5760/15296]\n",
      "loss: 0.315311  [ 5840/15296]\n",
      "loss: 0.183839  [ 5920/15296]\n",
      "loss: 0.238373  [ 6000/15296]\n",
      "loss: 0.154924  [ 6080/15296]\n",
      "loss: 0.163343  [ 6160/15296]\n",
      "loss: 0.190644  [ 6240/15296]\n",
      "loss: 0.209876  [ 6320/15296]\n",
      "loss: 0.169200  [ 6400/15296]\n",
      "loss: 0.290220  [ 6480/15296]\n",
      "loss: 0.138490  [ 6560/15296]\n",
      "loss: 0.245297  [ 6640/15296]\n",
      "loss: 0.277208  [ 6720/15296]\n",
      "loss: 0.194639  [ 6800/15296]\n",
      "loss: 0.248975  [ 6880/15296]\n",
      "loss: 0.142997  [ 6960/15296]\n",
      "loss: 0.163183  [ 7040/15296]\n",
      "loss: 0.288802  [ 7120/15296]\n",
      "loss: 0.215065  [ 7200/15296]\n",
      "loss: 0.246086  [ 7280/15296]\n",
      "loss: 0.184301  [ 7360/15296]\n",
      "loss: 0.225602  [ 7440/15296]\n",
      "loss: 0.150012  [ 7520/15296]\n",
      "loss: 0.237487  [ 7600/15296]\n",
      "loss: 0.279088  [ 7680/15296]\n",
      "loss: 0.225207  [ 7760/15296]\n",
      "loss: 0.193590  [ 7840/15296]\n",
      "loss: 0.222127  [ 7920/15296]\n",
      "loss: 0.219057  [ 8000/15296]\n",
      "loss: 0.227289  [ 8080/15296]\n",
      "loss: 0.210381  [ 8160/15296]\n",
      "loss: 0.269665  [ 8240/15296]\n",
      "loss: 0.172696  [ 8320/15296]\n",
      "loss: 0.197925  [ 8400/15296]\n",
      "loss: 0.197499  [ 8480/15296]\n",
      "loss: 0.270836  [ 8560/15296]\n",
      "loss: 0.283819  [ 8640/15296]\n",
      "loss: 0.130749  [ 8720/15296]\n",
      "loss: 0.181119  [ 8800/15296]\n",
      "loss: 0.257588  [ 8880/15296]\n",
      "loss: 0.213147  [ 8960/15296]\n",
      "loss: 0.198249  [ 9040/15296]\n",
      "loss: 0.092523  [ 9120/15296]\n",
      "loss: 0.214325  [ 9200/15296]\n",
      "loss: 0.265572  [ 9280/15296]\n",
      "loss: 0.139680  [ 9360/15296]\n",
      "loss: 0.208557  [ 9440/15296]\n",
      "loss: 0.188835  [ 9520/15296]\n",
      "loss: 0.212460  [ 9600/15296]\n",
      "loss: 0.128391  [ 9680/15296]\n",
      "loss: 0.233116  [ 9760/15296]\n",
      "loss: 0.203744  [ 9840/15296]\n",
      "loss: 0.194648  [ 9920/15296]\n",
      "loss: 0.163242  [10000/15296]\n",
      "loss: 0.188354  [10080/15296]\n",
      "loss: 0.202619  [10160/15296]\n",
      "loss: 0.222571  [10240/15296]\n",
      "loss: 0.217226  [10320/15296]\n",
      "loss: 0.230136  [10400/15296]\n",
      "loss: 0.210615  [10480/15296]\n",
      "loss: 0.223669  [10560/15296]\n",
      "loss: 0.199972  [10640/15296]\n",
      "loss: 0.190599  [10720/15296]\n",
      "loss: 0.175946  [10800/15296]\n",
      "loss: 0.243269  [10880/15296]\n",
      "loss: 0.214003  [10960/15296]\n",
      "loss: 0.180774  [11040/15296]\n",
      "loss: 0.214759  [11120/15296]\n",
      "loss: 0.171245  [11200/15296]\n",
      "loss: 0.214388  [11280/15296]\n",
      "loss: 0.230402  [11360/15296]\n",
      "loss: 0.166921  [11440/15296]\n",
      "loss: 0.228750  [11520/15296]\n",
      "loss: 0.281905  [11600/15296]\n",
      "loss: 0.260127  [11680/15296]\n",
      "loss: 0.242271  [11760/15296]\n",
      "loss: 0.181305  [11840/15296]\n",
      "loss: 0.254620  [11920/15296]\n",
      "loss: 0.242286  [12000/15296]\n",
      "loss: 0.098554  [12080/15296]\n",
      "loss: 0.187957  [12160/15296]\n",
      "loss: 0.268774  [12240/15296]\n",
      "loss: 0.279504  [12320/15296]\n",
      "loss: 0.216425  [12400/15296]\n",
      "loss: 0.229221  [12480/15296]\n",
      "loss: 0.184824  [12560/15296]\n",
      "loss: 0.102903  [12640/15296]\n",
      "loss: 0.173375  [12720/15296]\n",
      "loss: 0.193050  [12800/15296]\n",
      "loss: 0.292094  [12880/15296]\n",
      "loss: 0.203256  [12960/15296]\n",
      "loss: 0.292676  [13040/15296]\n",
      "loss: 0.197873  [13120/15296]\n",
      "loss: 0.165678  [13200/15296]\n",
      "loss: 0.197331  [13280/15296]\n",
      "loss: 0.163668  [13360/15296]\n",
      "loss: 0.244293  [13440/15296]\n",
      "loss: 0.127197  [13520/15296]\n",
      "loss: 0.231406  [13600/15296]\n",
      "loss: 0.181564  [13680/15296]\n",
      "loss: 0.248447  [13760/15296]\n",
      "loss: 0.224450  [13840/15296]\n",
      "loss: 0.325502  [13920/15296]\n",
      "loss: 0.227811  [14000/15296]\n",
      "loss: 0.255547  [14080/15296]\n",
      "loss: 0.286292  [14160/15296]\n",
      "loss: 0.236449  [14240/15296]\n",
      "loss: 0.330410  [14320/15296]\n",
      "loss: 0.115762  [14400/15296]\n",
      "loss: 0.114355  [14480/15296]\n",
      "loss: 0.201673  [14560/15296]\n",
      "loss: 0.169866  [14640/15296]\n",
      "loss: 0.282532  [14720/15296]\n",
      "loss: 0.254703  [14800/15296]\n",
      "loss: 0.264371  [14880/15296]\n",
      "loss: 0.154239  [14960/15296]\n",
      "loss: 0.090461  [15040/15296]\n",
      "loss: 0.199790  [15120/15296]\n",
      "loss: 0.188575  [15200/15296]\n",
      "loss: 0.214443  [ 6112/15296]\n",
      "Test Loss: 0.191394%\n",
      "Epoch: 6 |  train_loss: 0.21139676079115108 |  test_loss: 0.1913944633345898 | \n",
      "loss: 0.250579  [    0/15296]\n",
      "loss: 0.219529  [   80/15296]\n",
      "loss: 0.220983  [  160/15296]\n",
      "loss: 0.281523  [  240/15296]\n",
      "loss: 0.244487  [  320/15296]\n",
      "loss: 0.184374  [  400/15296]\n",
      "loss: 0.170481  [  480/15296]\n",
      "loss: 0.146222  [  560/15296]\n",
      "loss: 0.184171  [  640/15296]\n",
      "loss: 0.225110  [  720/15296]\n",
      "loss: 0.248088  [  800/15296]\n",
      "loss: 0.254194  [  880/15296]\n",
      "loss: 0.214496  [  960/15296]\n",
      "loss: 0.201289  [ 1040/15296]\n",
      "loss: 0.228613  [ 1120/15296]\n",
      "loss: 0.239374  [ 1200/15296]\n",
      "loss: 0.198565  [ 1280/15296]\n",
      "loss: 0.219270  [ 1360/15296]\n",
      "loss: 0.246883  [ 1440/15296]\n",
      "loss: 0.267079  [ 1520/15296]\n",
      "loss: 0.225385  [ 1600/15296]\n",
      "loss: 0.269671  [ 1680/15296]\n",
      "loss: 0.304460  [ 1760/15296]\n",
      "loss: 0.238434  [ 1840/15296]\n",
      "loss: 0.179084  [ 1920/15296]\n",
      "loss: 0.257116  [ 2000/15296]\n",
      "loss: 0.219965  [ 2080/15296]\n",
      "loss: 0.212101  [ 2160/15296]\n",
      "loss: 0.260401  [ 2240/15296]\n",
      "loss: 0.307035  [ 2320/15296]\n",
      "loss: 0.220569  [ 2400/15296]\n",
      "loss: 0.328479  [ 2480/15296]\n",
      "loss: 0.121791  [ 2560/15296]\n",
      "loss: 0.108252  [ 2640/15296]\n",
      "loss: 0.180616  [ 2720/15296]\n",
      "loss: 0.122884  [ 2800/15296]\n",
      "loss: 0.176185  [ 2880/15296]\n",
      "loss: 0.232133  [ 2960/15296]\n",
      "loss: 0.246587  [ 3040/15296]\n",
      "loss: 0.125055  [ 3120/15296]\n",
      "loss: 0.189564  [ 3200/15296]\n",
      "loss: 0.280619  [ 3280/15296]\n",
      "loss: 0.216756  [ 3360/15296]\n",
      "loss: 0.160842  [ 3440/15296]\n",
      "loss: 0.171338  [ 3520/15296]\n",
      "loss: 0.217019  [ 3600/15296]\n",
      "loss: 0.280113  [ 3680/15296]\n",
      "loss: 0.223237  [ 3760/15296]\n",
      "loss: 0.163534  [ 3840/15296]\n",
      "loss: 0.236188  [ 3920/15296]\n",
      "loss: 0.252630  [ 4000/15296]\n",
      "loss: 0.261036  [ 4080/15296]\n",
      "loss: 0.143340  [ 4160/15296]\n",
      "loss: 0.206567  [ 4240/15296]\n",
      "loss: 0.287756  [ 4320/15296]\n",
      "loss: 0.166876  [ 4400/15296]\n",
      "loss: 0.169896  [ 4480/15296]\n",
      "loss: 0.248516  [ 4560/15296]\n",
      "loss: 0.361455  [ 4640/15296]\n",
      "loss: 0.265961  [ 4720/15296]\n",
      "loss: 0.176103  [ 4800/15296]\n",
      "loss: 0.274171  [ 4880/15296]\n",
      "loss: 0.277382  [ 4960/15296]\n",
      "loss: 0.194837  [ 5040/15296]\n",
      "loss: 0.168047  [ 5120/15296]\n",
      "loss: 0.323784  [ 5200/15296]\n",
      "loss: 0.178879  [ 5280/15296]\n",
      "loss: 0.194762  [ 5360/15296]\n",
      "loss: 0.221224  [ 5440/15296]\n",
      "loss: 0.219455  [ 5520/15296]\n",
      "loss: 0.107185  [ 5600/15296]\n",
      "loss: 0.239589  [ 5680/15296]\n",
      "loss: 0.219103  [ 5760/15296]\n",
      "loss: 0.188950  [ 5840/15296]\n",
      "loss: 0.279211  [ 5920/15296]\n",
      "loss: 0.297376  [ 6000/15296]\n",
      "loss: 0.324032  [ 6080/15296]\n",
      "loss: 0.160277  [ 6160/15296]\n",
      "loss: 0.147055  [ 6240/15296]\n",
      "loss: 0.312849  [ 6320/15296]\n",
      "loss: 0.227142  [ 6400/15296]\n",
      "loss: 0.278368  [ 6480/15296]\n",
      "loss: 0.209459  [ 6560/15296]\n",
      "loss: 0.137137  [ 6640/15296]\n",
      "loss: 0.260124  [ 6720/15296]\n",
      "loss: 0.256847  [ 6800/15296]\n",
      "loss: 0.139321  [ 6880/15296]\n",
      "loss: 0.231037  [ 6960/15296]\n",
      "loss: 0.201558  [ 7040/15296]\n",
      "loss: 0.167223  [ 7120/15296]\n",
      "loss: 0.178747  [ 7200/15296]\n",
      "loss: 0.267555  [ 7280/15296]\n",
      "loss: 0.223762  [ 7360/15296]\n",
      "loss: 0.190128  [ 7440/15296]\n",
      "loss: 0.283500  [ 7520/15296]\n",
      "loss: 0.231723  [ 7600/15296]\n",
      "loss: 0.168231  [ 7680/15296]\n",
      "loss: 0.257251  [ 7760/15296]\n",
      "loss: 0.192564  [ 7840/15296]\n",
      "loss: 0.256284  [ 7920/15296]\n",
      "loss: 0.223727  [ 8000/15296]\n",
      "loss: 0.182642  [ 8080/15296]\n",
      "loss: 0.300279  [ 8160/15296]\n",
      "loss: 0.270780  [ 8240/15296]\n",
      "loss: 0.200042  [ 8320/15296]\n",
      "loss: 0.163556  [ 8400/15296]\n",
      "loss: 0.171793  [ 8480/15296]\n",
      "loss: 0.194934  [ 8560/15296]\n",
      "loss: 0.237508  [ 8640/15296]\n",
      "loss: 0.166259  [ 8720/15296]\n",
      "loss: 0.284057  [ 8800/15296]\n",
      "loss: 0.211974  [ 8880/15296]\n",
      "loss: 0.251247  [ 8960/15296]\n",
      "loss: 0.259907  [ 9040/15296]\n",
      "loss: 0.135721  [ 9120/15296]\n",
      "loss: 0.168645  [ 9200/15296]\n",
      "loss: 0.161797  [ 9280/15296]\n",
      "loss: 0.259808  [ 9360/15296]\n",
      "loss: 0.256949  [ 9440/15296]\n",
      "loss: 0.108712  [ 9520/15296]\n",
      "loss: 0.284611  [ 9600/15296]\n",
      "loss: 0.145377  [ 9680/15296]\n",
      "loss: 0.137512  [ 9760/15296]\n",
      "loss: 0.247866  [ 9840/15296]\n",
      "loss: 0.223160  [ 9920/15296]\n",
      "loss: 0.261600  [10000/15296]\n",
      "loss: 0.292090  [10080/15296]\n",
      "loss: 0.229963  [10160/15296]\n",
      "loss: 0.190798  [10240/15296]\n",
      "loss: 0.223751  [10320/15296]\n",
      "loss: 0.204097  [10400/15296]\n",
      "loss: 0.209597  [10480/15296]\n",
      "loss: 0.229693  [10560/15296]\n",
      "loss: 0.237838  [10640/15296]\n",
      "loss: 0.263870  [10720/15296]\n",
      "loss: 0.247627  [10800/15296]\n",
      "loss: 0.168871  [10880/15296]\n",
      "loss: 0.218136  [10960/15296]\n",
      "loss: 0.209389  [11040/15296]\n",
      "loss: 0.243300  [11120/15296]\n",
      "loss: 0.192617  [11200/15296]\n",
      "loss: 0.193358  [11280/15296]\n",
      "loss: 0.097534  [11360/15296]\n",
      "loss: 0.164797  [11440/15296]\n",
      "loss: 0.316666  [11520/15296]\n",
      "loss: 0.200809  [11600/15296]\n",
      "loss: 0.110806  [11680/15296]\n",
      "loss: 0.170678  [11760/15296]\n",
      "loss: 0.233670  [11840/15296]\n",
      "loss: 0.288698  [11920/15296]\n",
      "loss: 0.246699  [12000/15296]\n",
      "loss: 0.184831  [12080/15296]\n",
      "loss: 0.210939  [12160/15296]\n",
      "loss: 0.174650  [12240/15296]\n",
      "loss: 0.245380  [12320/15296]\n",
      "loss: 0.292171  [12400/15296]\n",
      "loss: 0.229285  [12480/15296]\n",
      "loss: 0.224882  [12560/15296]\n",
      "loss: 0.086773  [12640/15296]\n",
      "loss: 0.118371  [12720/15296]\n",
      "loss: 0.199932  [12800/15296]\n",
      "loss: 0.237887  [12880/15296]\n",
      "loss: 0.214388  [12960/15296]\n",
      "loss: 0.214697  [13040/15296]\n",
      "loss: 0.231717  [13120/15296]\n",
      "loss: 0.151486  [13200/15296]\n",
      "loss: 0.236903  [13280/15296]\n",
      "loss: 0.296789  [13360/15296]\n",
      "loss: 0.255731  [13440/15296]\n",
      "loss: 0.088017  [13520/15296]\n",
      "loss: 0.258299  [13600/15296]\n",
      "loss: 0.152653  [13680/15296]\n",
      "loss: 0.171087  [13760/15296]\n",
      "loss: 0.230991  [13840/15296]\n",
      "loss: 0.180410  [13920/15296]\n",
      "loss: 0.164643  [14000/15296]\n",
      "loss: 0.257522  [14080/15296]\n",
      "loss: 0.182249  [14160/15296]\n",
      "loss: 0.206067  [14240/15296]\n",
      "loss: 0.126921  [14320/15296]\n",
      "loss: 0.252766  [14400/15296]\n",
      "loss: 0.236776  [14480/15296]\n",
      "loss: 0.219866  [14560/15296]\n",
      "loss: 0.231539  [14640/15296]\n",
      "loss: 0.099854  [14720/15296]\n",
      "loss: 0.183831  [14800/15296]\n",
      "loss: 0.147112  [14880/15296]\n",
      "loss: 0.234545  [14960/15296]\n",
      "loss: 0.262882  [15040/15296]\n",
      "loss: 0.259333  [15120/15296]\n",
      "loss: 0.162107  [15200/15296]\n",
      "loss: 0.093282  [ 6112/15296]\n",
      "Test Loss: 0.191629%\n",
      "Epoch: 7 |  train_loss: 0.2104779710852135 |  test_loss: 0.19162940099563228 | \n",
      "loss: 0.219236  [    0/15296]\n",
      "loss: 0.258747  [   80/15296]\n",
      "loss: 0.144427  [  160/15296]\n",
      "loss: 0.193025  [  240/15296]\n",
      "loss: 0.263077  [  320/15296]\n",
      "loss: 0.151887  [  400/15296]\n",
      "loss: 0.320873  [  480/15296]\n",
      "loss: 0.192102  [  560/15296]\n",
      "loss: 0.273192  [  640/15296]\n",
      "loss: 0.170742  [  720/15296]\n",
      "loss: 0.189611  [  800/15296]\n",
      "loss: 0.270529  [  880/15296]\n",
      "loss: 0.234353  [  960/15296]\n",
      "loss: 0.204247  [ 1040/15296]\n",
      "loss: 0.174248  [ 1120/15296]\n",
      "loss: 0.264657  [ 1200/15296]\n",
      "loss: 0.281294  [ 1280/15296]\n",
      "loss: 0.141861  [ 1360/15296]\n",
      "loss: 0.225071  [ 1440/15296]\n",
      "loss: 0.227230  [ 1520/15296]\n",
      "loss: 0.225727  [ 1600/15296]\n",
      "loss: 0.132066  [ 1680/15296]\n",
      "loss: 0.122468  [ 1760/15296]\n",
      "loss: 0.210679  [ 1840/15296]\n",
      "loss: 0.195998  [ 1920/15296]\n",
      "loss: 0.175883  [ 2000/15296]\n",
      "loss: 0.128670  [ 2080/15296]\n",
      "loss: 0.184627  [ 2160/15296]\n",
      "loss: 0.242403  [ 2240/15296]\n",
      "loss: 0.199351  [ 2320/15296]\n",
      "loss: 0.169242  [ 2400/15296]\n",
      "loss: 0.180596  [ 2480/15296]\n",
      "loss: 0.199535  [ 2560/15296]\n",
      "loss: 0.215432  [ 2640/15296]\n",
      "loss: 0.234140  [ 2720/15296]\n",
      "loss: 0.293227  [ 2800/15296]\n",
      "loss: 0.213151  [ 2880/15296]\n",
      "loss: 0.136730  [ 2960/15296]\n",
      "loss: 0.249528  [ 3040/15296]\n",
      "loss: 0.279972  [ 3120/15296]\n",
      "loss: 0.183791  [ 3200/15296]\n",
      "loss: 0.102555  [ 3280/15296]\n",
      "loss: 0.270666  [ 3360/15296]\n",
      "loss: 0.246440  [ 3440/15296]\n",
      "loss: 0.167577  [ 3520/15296]\n",
      "loss: 0.147960  [ 3600/15296]\n",
      "loss: 0.251154  [ 3680/15296]\n",
      "loss: 0.164575  [ 3760/15296]\n",
      "loss: 0.228360  [ 3840/15296]\n",
      "loss: 0.277614  [ 3920/15296]\n",
      "loss: 0.108247  [ 4000/15296]\n",
      "loss: 0.234249  [ 4080/15296]\n",
      "loss: 0.211988  [ 4160/15296]\n",
      "loss: 0.264059  [ 4240/15296]\n",
      "loss: 0.195978  [ 4320/15296]\n",
      "loss: 0.249762  [ 4400/15296]\n",
      "loss: 0.298068  [ 4480/15296]\n",
      "loss: 0.376914  [ 4560/15296]\n",
      "loss: 0.280052  [ 4640/15296]\n",
      "loss: 0.213246  [ 4720/15296]\n",
      "loss: 0.128482  [ 4800/15296]\n",
      "loss: 0.254298  [ 4880/15296]\n",
      "loss: 0.126072  [ 4960/15296]\n",
      "loss: 0.190588  [ 5040/15296]\n",
      "loss: 0.257460  [ 5120/15296]\n",
      "loss: 0.166765  [ 5200/15296]\n",
      "loss: 0.170699  [ 5280/15296]\n",
      "loss: 0.162559  [ 5360/15296]\n",
      "loss: 0.129487  [ 5440/15296]\n",
      "loss: 0.150487  [ 5520/15296]\n",
      "loss: 0.248281  [ 5600/15296]\n",
      "loss: 0.257238  [ 5680/15296]\n",
      "loss: 0.265527  [ 5760/15296]\n",
      "loss: 0.231701  [ 5840/15296]\n",
      "loss: 0.278491  [ 5920/15296]\n",
      "loss: 0.234525  [ 6000/15296]\n",
      "loss: 0.245620  [ 6080/15296]\n",
      "loss: 0.254786  [ 6160/15296]\n",
      "loss: 0.176872  [ 6240/15296]\n",
      "loss: 0.247417  [ 6320/15296]\n",
      "loss: 0.260852  [ 6400/15296]\n",
      "loss: 0.205021  [ 6480/15296]\n",
      "loss: 0.215736  [ 6560/15296]\n",
      "loss: 0.255814  [ 6640/15296]\n",
      "loss: 0.230480  [ 6720/15296]\n",
      "loss: 0.123775  [ 6800/15296]\n",
      "loss: 0.204490  [ 6880/15296]\n",
      "loss: 0.185851  [ 6960/15296]\n",
      "loss: 0.123250  [ 7040/15296]\n",
      "loss: 0.229534  [ 7120/15296]\n",
      "loss: 0.178044  [ 7200/15296]\n",
      "loss: 0.161718  [ 7280/15296]\n",
      "loss: 0.217322  [ 7360/15296]\n",
      "loss: 0.198641  [ 7440/15296]\n",
      "loss: 0.145645  [ 7520/15296]\n",
      "loss: 0.233628  [ 7600/15296]\n",
      "loss: 0.173036  [ 7680/15296]\n",
      "loss: 0.195105  [ 7760/15296]\n",
      "loss: 0.220039  [ 7840/15296]\n",
      "loss: 0.114450  [ 7920/15296]\n",
      "loss: 0.311002  [ 8000/15296]\n",
      "loss: 0.245214  [ 8080/15296]\n",
      "loss: 0.211731  [ 8160/15296]\n",
      "loss: 0.251814  [ 8240/15296]\n",
      "loss: 0.205770  [ 8320/15296]\n",
      "loss: 0.133410  [ 8400/15296]\n",
      "loss: 0.256705  [ 8480/15296]\n",
      "loss: 0.260241  [ 8560/15296]\n",
      "loss: 0.152677  [ 8640/15296]\n",
      "loss: 0.159549  [ 8720/15296]\n",
      "loss: 0.214333  [ 8800/15296]\n",
      "loss: 0.105425  [ 8880/15296]\n",
      "loss: 0.237114  [ 8960/15296]\n",
      "loss: 0.274635  [ 9040/15296]\n",
      "loss: 0.191149  [ 9120/15296]\n",
      "loss: 0.119387  [ 9200/15296]\n",
      "loss: 0.182065  [ 9280/15296]\n",
      "loss: 0.208897  [ 9360/15296]\n",
      "loss: 0.239232  [ 9440/15296]\n",
      "loss: 0.254483  [ 9520/15296]\n",
      "loss: 0.191433  [ 9600/15296]\n",
      "loss: 0.278699  [ 9680/15296]\n",
      "loss: 0.205144  [ 9760/15296]\n",
      "loss: 0.107242  [ 9840/15296]\n",
      "loss: 0.149839  [ 9920/15296]\n",
      "loss: 0.223398  [10000/15296]\n",
      "loss: 0.149658  [10080/15296]\n",
      "loss: 0.252272  [10160/15296]\n",
      "loss: 0.204623  [10240/15296]\n",
      "loss: 0.190350  [10320/15296]\n",
      "loss: 0.208130  [10400/15296]\n",
      "loss: 0.257352  [10480/15296]\n",
      "loss: 0.224308  [10560/15296]\n",
      "loss: 0.259571  [10640/15296]\n",
      "loss: 0.257118  [10720/15296]\n",
      "loss: 0.114991  [10800/15296]\n",
      "loss: 0.276381  [10880/15296]\n",
      "loss: 0.193845  [10960/15296]\n",
      "loss: 0.277836  [11040/15296]\n",
      "loss: 0.246658  [11120/15296]\n",
      "loss: 0.127224  [11200/15296]\n",
      "loss: 0.202819  [11280/15296]\n",
      "loss: 0.290376  [11360/15296]\n",
      "loss: 0.151795  [11440/15296]\n",
      "loss: 0.188694  [11520/15296]\n",
      "loss: 0.196746  [11600/15296]\n",
      "loss: 0.106379  [11680/15296]\n",
      "loss: 0.251117  [11760/15296]\n",
      "loss: 0.158455  [11840/15296]\n",
      "loss: 0.171049  [11920/15296]\n",
      "loss: 0.260589  [12000/15296]\n",
      "loss: 0.164308  [12080/15296]\n",
      "loss: 0.272102  [12160/15296]\n",
      "loss: 0.253971  [12240/15296]\n",
      "loss: 0.178287  [12320/15296]\n",
      "loss: 0.208413  [12400/15296]\n",
      "loss: 0.236019  [12480/15296]\n",
      "loss: 0.178655  [12560/15296]\n",
      "loss: 0.256239  [12640/15296]\n",
      "loss: 0.085771  [12720/15296]\n",
      "loss: 0.166051  [12800/15296]\n",
      "loss: 0.185606  [12880/15296]\n",
      "loss: 0.187227  [12960/15296]\n",
      "loss: 0.215461  [13040/15296]\n",
      "loss: 0.189268  [13120/15296]\n",
      "loss: 0.215302  [13200/15296]\n",
      "loss: 0.268568  [13280/15296]\n",
      "loss: 0.237394  [13360/15296]\n",
      "loss: 0.198733  [13440/15296]\n",
      "loss: 0.300105  [13520/15296]\n",
      "loss: 0.229401  [13600/15296]\n",
      "loss: 0.224841  [13680/15296]\n",
      "loss: 0.247687  [13760/15296]\n",
      "loss: 0.109346  [13840/15296]\n",
      "loss: 0.233137  [13920/15296]\n",
      "loss: 0.347060  [14000/15296]\n",
      "loss: 0.109370  [14080/15296]\n",
      "loss: 0.152491  [14160/15296]\n",
      "loss: 0.256450  [14240/15296]\n",
      "loss: 0.251818  [14320/15296]\n",
      "loss: 0.210818  [14400/15296]\n",
      "loss: 0.271829  [14480/15296]\n",
      "loss: 0.221425  [14560/15296]\n",
      "loss: 0.190657  [14640/15296]\n",
      "loss: 0.121830  [14720/15296]\n",
      "loss: 0.165727  [14800/15296]\n",
      "loss: 0.209348  [14880/15296]\n",
      "loss: 0.124022  [14960/15296]\n",
      "loss: 0.177489  [15040/15296]\n",
      "loss: 0.265167  [15120/15296]\n",
      "loss: 0.209389  [15200/15296]\n",
      "loss: 0.227469  [ 6112/15296]\n",
      "Test Loss: 0.194300%\n",
      "Epoch: 8 |  train_loss: 0.21084481796770121 |  test_loss: 0.19429968496503897 | \n",
      "loss: 0.273020  [    0/15296]\n",
      "loss: 0.169288  [   80/15296]\n",
      "loss: 0.206924  [  160/15296]\n",
      "loss: 0.224822  [  240/15296]\n",
      "loss: 0.279503  [  320/15296]\n",
      "loss: 0.215120  [  400/15296]\n",
      "loss: 0.245170  [  480/15296]\n",
      "loss: 0.279403  [  560/15296]\n",
      "loss: 0.129475  [  640/15296]\n",
      "loss: 0.244741  [  720/15296]\n",
      "loss: 0.179322  [  800/15296]\n",
      "loss: 0.264859  [  880/15296]\n",
      "loss: 0.248748  [  960/15296]\n",
      "loss: 0.192348  [ 1040/15296]\n",
      "loss: 0.243691  [ 1120/15296]\n",
      "loss: 0.222564  [ 1200/15296]\n",
      "loss: 0.210678  [ 1280/15296]\n",
      "loss: 0.199032  [ 1360/15296]\n",
      "loss: 0.228960  [ 1440/15296]\n",
      "loss: 0.215175  [ 1520/15296]\n",
      "loss: 0.098386  [ 1600/15296]\n",
      "loss: 0.264977  [ 1680/15296]\n",
      "loss: 0.258149  [ 1760/15296]\n",
      "loss: 0.166343  [ 1840/15296]\n",
      "loss: 0.238559  [ 1920/15296]\n",
      "loss: 0.162685  [ 2000/15296]\n",
      "loss: 0.188646  [ 2080/15296]\n",
      "loss: 0.287868  [ 2160/15296]\n",
      "loss: 0.191706  [ 2240/15296]\n",
      "loss: 0.277865  [ 2320/15296]\n",
      "loss: 0.228029  [ 2400/15296]\n",
      "loss: 0.301012  [ 2480/15296]\n",
      "loss: 0.251254  [ 2560/15296]\n",
      "loss: 0.239398  [ 2640/15296]\n",
      "loss: 0.151557  [ 2720/15296]\n",
      "loss: 0.220631  [ 2800/15296]\n",
      "loss: 0.164928  [ 2880/15296]\n",
      "loss: 0.183880  [ 2960/15296]\n",
      "loss: 0.236786  [ 3040/15296]\n",
      "loss: 0.175266  [ 3120/15296]\n",
      "loss: 0.257728  [ 3200/15296]\n",
      "loss: 0.196843  [ 3280/15296]\n",
      "loss: 0.167397  [ 3360/15296]\n",
      "loss: 0.218447  [ 3440/15296]\n",
      "loss: 0.181509  [ 3520/15296]\n",
      "loss: 0.239017  [ 3600/15296]\n",
      "loss: 0.189640  [ 3680/15296]\n",
      "loss: 0.112460  [ 3760/15296]\n",
      "loss: 0.186998  [ 3840/15296]\n",
      "loss: 0.232186  [ 3920/15296]\n",
      "loss: 0.214258  [ 4000/15296]\n",
      "loss: 0.221318  [ 4080/15296]\n",
      "loss: 0.113546  [ 4160/15296]\n",
      "loss: 0.104544  [ 4240/15296]\n",
      "loss: 0.130699  [ 4320/15296]\n",
      "loss: 0.183161  [ 4400/15296]\n",
      "loss: 0.260447  [ 4480/15296]\n",
      "loss: 0.181836  [ 4560/15296]\n",
      "loss: 0.198962  [ 4640/15296]\n",
      "loss: 0.103245  [ 4720/15296]\n",
      "loss: 0.125926  [ 4800/15296]\n",
      "loss: 0.245843  [ 4880/15296]\n",
      "loss: 0.260805  [ 4960/15296]\n",
      "loss: 0.194871  [ 5040/15296]\n",
      "loss: 0.294506  [ 5120/15296]\n",
      "loss: 0.193999  [ 5200/15296]\n",
      "loss: 0.091450  [ 5280/15296]\n",
      "loss: 0.189474  [ 5360/15296]\n",
      "loss: 0.183456  [ 5440/15296]\n",
      "loss: 0.133993  [ 5520/15296]\n",
      "loss: 0.285632  [ 5600/15296]\n",
      "loss: 0.147535  [ 5680/15296]\n",
      "loss: 0.270110  [ 5760/15296]\n",
      "loss: 0.215941  [ 5840/15296]\n",
      "loss: 0.220624  [ 5920/15296]\n",
      "loss: 0.191627  [ 6000/15296]\n",
      "loss: 0.252012  [ 6080/15296]\n",
      "loss: 0.272011  [ 6160/15296]\n",
      "loss: 0.236033  [ 6240/15296]\n",
      "loss: 0.280236  [ 6320/15296]\n",
      "loss: 0.262189  [ 6400/15296]\n",
      "loss: 0.226728  [ 6480/15296]\n",
      "loss: 0.153529  [ 6560/15296]\n",
      "loss: 0.211836  [ 6640/15296]\n",
      "loss: 0.231880  [ 6720/15296]\n",
      "loss: 0.254069  [ 6800/15296]\n",
      "loss: 0.135389  [ 6880/15296]\n",
      "loss: 0.237274  [ 6960/15296]\n",
      "loss: 0.209993  [ 7040/15296]\n",
      "loss: 0.254031  [ 7120/15296]\n",
      "loss: 0.159282  [ 7200/15296]\n",
      "loss: 0.259229  [ 7280/15296]\n",
      "loss: 0.140595  [ 7360/15296]\n",
      "loss: 0.204083  [ 7440/15296]\n",
      "loss: 0.118691  [ 7520/15296]\n",
      "loss: 0.188125  [ 7600/15296]\n",
      "loss: 0.182233  [ 7680/15296]\n",
      "loss: 0.109342  [ 7760/15296]\n",
      "loss: 0.298103  [ 7840/15296]\n",
      "loss: 0.251115  [ 7920/15296]\n",
      "loss: 0.226456  [ 8000/15296]\n",
      "loss: 0.146545  [ 8080/15296]\n",
      "loss: 0.194696  [ 8160/15296]\n",
      "loss: 0.246800  [ 8240/15296]\n",
      "loss: 0.201640  [ 8320/15296]\n",
      "loss: 0.172703  [ 8400/15296]\n",
      "loss: 0.290241  [ 8480/15296]\n",
      "loss: 0.176712  [ 8560/15296]\n",
      "loss: 0.314376  [ 8640/15296]\n",
      "loss: 0.311654  [ 8720/15296]\n",
      "loss: 0.195359  [ 8800/15296]\n",
      "loss: 0.245923  [ 8880/15296]\n",
      "loss: 0.166275  [ 8960/15296]\n",
      "loss: 0.229215  [ 9040/15296]\n",
      "loss: 0.227299  [ 9120/15296]\n",
      "loss: 0.270593  [ 9200/15296]\n",
      "loss: 0.223665  [ 9280/15296]\n",
      "loss: 0.232307  [ 9360/15296]\n",
      "loss: 0.242043  [ 9440/15296]\n",
      "loss: 0.117860  [ 9520/15296]\n",
      "loss: 0.187980  [ 9600/15296]\n",
      "loss: 0.161119  [ 9680/15296]\n",
      "loss: 0.222451  [ 9760/15296]\n",
      "loss: 0.169574  [ 9840/15296]\n",
      "loss: 0.192629  [ 9920/15296]\n",
      "loss: 0.108271  [10000/15296]\n",
      "loss: 0.198154  [10080/15296]\n",
      "loss: 0.157124  [10160/15296]\n",
      "loss: 0.137594  [10240/15296]\n",
      "loss: 0.220736  [10320/15296]\n",
      "loss: 0.227620  [10400/15296]\n",
      "loss: 0.177142  [10480/15296]\n",
      "loss: 0.334535  [10560/15296]\n",
      "loss: 0.264726  [10640/15296]\n",
      "loss: 0.162183  [10720/15296]\n",
      "loss: 0.183124  [10800/15296]\n",
      "loss: 0.126011  [10880/15296]\n",
      "loss: 0.116923  [10960/15296]\n",
      "loss: 0.172053  [11040/15296]\n",
      "loss: 0.122969  [11120/15296]\n",
      "loss: 0.174727  [11200/15296]\n",
      "loss: 0.245986  [11280/15296]\n",
      "loss: 0.313428  [11360/15296]\n",
      "loss: 0.172326  [11440/15296]\n",
      "loss: 0.150897  [11520/15296]\n",
      "loss: 0.170885  [11600/15296]\n",
      "loss: 0.199603  [11680/15296]\n",
      "loss: 0.179065  [11760/15296]\n",
      "loss: 0.100666  [11840/15296]\n",
      "loss: 0.111764  [11920/15296]\n",
      "loss: 0.268667  [12000/15296]\n",
      "loss: 0.129925  [12080/15296]\n",
      "loss: 0.247583  [12160/15296]\n",
      "loss: 0.104208  [12240/15296]\n",
      "loss: 0.209856  [12320/15296]\n",
      "loss: 0.182000  [12400/15296]\n",
      "loss: 0.234984  [12480/15296]\n",
      "loss: 0.348333  [12560/15296]\n",
      "loss: 0.085678  [12640/15296]\n",
      "loss: 0.140879  [12720/15296]\n",
      "loss: 0.194772  [12800/15296]\n",
      "loss: 0.246131  [12880/15296]\n",
      "loss: 0.216587  [12960/15296]\n",
      "loss: 0.243589  [13040/15296]\n",
      "loss: 0.206585  [13120/15296]\n",
      "loss: 0.224141  [13200/15296]\n",
      "loss: 0.142493  [13280/15296]\n",
      "loss: 0.211762  [13360/15296]\n",
      "loss: 0.264923  [13440/15296]\n",
      "loss: 0.158969  [13520/15296]\n",
      "loss: 0.200883  [13600/15296]\n",
      "loss: 0.262748  [13680/15296]\n",
      "loss: 0.249162  [13760/15296]\n",
      "loss: 0.188181  [13840/15296]\n",
      "loss: 0.186998  [13920/15296]\n",
      "loss: 0.213192  [14000/15296]\n",
      "loss: 0.218221  [14080/15296]\n",
      "loss: 0.272626  [14160/15296]\n",
      "loss: 0.235300  [14240/15296]\n",
      "loss: 0.304675  [14320/15296]\n",
      "loss: 0.105241  [14400/15296]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m train(model, train_dl, test_dl, torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m1e-6\u001b[39;49m), RMSELoss(), epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m) \u001b[39m# Make| sure set shuffle for train back to True after testing\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# 3. Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m---> 18\u001b[0m     train_loss \u001b[39m=\u001b[39m train_step(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     19\u001b[0m                                        data_loader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m     20\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m     21\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[0;32m     22\u001b[0m     test_loss \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     23\u001b[0m         data_loader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[0;32m     24\u001b[0m         loss_fn\u001b[39m=\u001b[39mloss_fn)\n\u001b[0;32m     26\u001b[0m     \u001b[39m# 4. Print out what's happening\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data_loader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Setup train loss and train accuracy values\u001b[39;00m\n\u001b[0;32m      6\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Send data to target device\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# 1. Forward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_got_empty_message \u001b[39mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[39m.\u001b[39mPeekNamedPipe(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle)[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(wait([\u001b[39mself\u001b[39;49m], timeout))\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[39m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[39m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[39m.\u001b[39;49mkeys(), timeout)\n\u001b[0;32m    880\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[39m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[39mfor\u001b[39;00m ov \u001b[39min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[39mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mWaitForMultipleObjects(L, \u001b[39mFalse\u001b[39;49;00m, timeout)\n\u001b[0;32m    812\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = train(model, train_dl, test_dl, torch.optim.Adam(model.parameters(), lr=1e-6), RMSELoss(), epochs=10) # Make| sure set shuffle for train back to True after testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(loss_dict):\n",
    "    train_losses = loss_dict[\"train_loss\"]\n",
    "    test_losses = loss_dict[\"test_loss\"]\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, test_losses, label=\"Test Loss\")\n",
    "    plt.title(\"Training and Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "somehow the test_loss is smaller than train_loss -> could be due to small smaple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c74e58d50e1361e9a22e65e4f4b85ac48544f43f48a101259d9c47c27371742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
