{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import SNDataset, myNormalize, myToTensor\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)], [(12), (-4,2963)], [(13), (0, 90)]], oc_min = 0, oc_max = 200)\n",
    "mynorm = myNormalize(img_bands_min_max =[[(0,7),(0,1)], [(7,12),(-1,1)]], oc_min = 0, oc_max = 200)\n",
    "my_to_tensor = myToTensor()\n",
    "transform = transforms.Compose([mynorm, my_to_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "# train_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\train\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "train_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-1\\\\dataset\\\\l8_images\\\\train',\\\n",
    "                     'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-1\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh \n",
    "\n",
    "# test_ds = SNDataset('D:\\python\\SoilNet\\dataset\\l8_images\\\\test\\\\','D:\\python\\SoilNet\\dataset\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform)\n",
    "test_ds = SNDataset('C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-1\\\\dataset\\\\l8_images\\\\test',\\\n",
    "                     'C:\\\\Users\\\\nkakhani\\\\_Multimodal\\\\SoilNet-1\\\\dataset\\\\LUCAS_2015_all.csv',l8_bands=bands, transform=transform) #Nafiseh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CONFIG\n",
    "# NUM_WORKERS = 2\n",
    "# TRAIN_BATCH_SIZE = 8\n",
    "# TEST_BATCH_SIZE = 2\n",
    "\n",
    "# CONFIG\n",
    "NUM_WORKERS = 6\n",
    "TRAIN_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dl = DataLoader(test_ds, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([40, 12, 64, 64]) -> [batch_size, color_channels, height, width]\n",
      "Label shape: torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "# Get image and label from custom DataLoader\n",
    "img_custom, label_custom = next(iter(train_dl))\n",
    "\n",
    "print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label_custom.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soilnet.soil_net import SoilNetFC, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 torch.Size([40, 12, 64, 64])\n",
      "torch.Size([40, 12, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0581],\n",
       "        [0.0487],\n",
       "        [0.0625],\n",
       "        [0.0740],\n",
       "        [0.0630],\n",
       "        [0.0331],\n",
       "        [0.0390],\n",
       "        [0.0668],\n",
       "        [0.0355],\n",
       "        [0.0803],\n",
       "        [0.0762],\n",
       "        [0.0633],\n",
       "        [0.0645],\n",
       "        [0.0693],\n",
       "        [0.0673],\n",
       "        [0.0681],\n",
       "        [0.0690],\n",
       "        [0.0159],\n",
       "        [0.0462],\n",
       "        [0.0558],\n",
       "        [0.0891],\n",
       "        [0.0701],\n",
       "        [0.0668],\n",
       "        [0.0595],\n",
       "        [0.0517],\n",
       "        [0.0548],\n",
       "        [0.0688],\n",
       "        [0.0756],\n",
       "        [0.0596],\n",
       "        [0.0599],\n",
       "        [0.0640],\n",
       "        [0.0835],\n",
       "        [0.0903],\n",
       "        [0.0892],\n",
       "        [0.0509],\n",
       "        [0.0803],\n",
       "        [0.0767],\n",
       "        [0.1005],\n",
       "        [0.1114],\n",
       "        [0.0712]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = SoilNetFC(cnn_in_channels=12, regresor_input_from_cnn=1024, hidden_size=128).to(device)\n",
    "model = ResNet(resnet_architecture=\"101\",cnn_in_channels=12, regresor_input_from_cnn=1024, hidden_size=128).to(device)\n",
    "img_gpu = img_custom.to(device)\n",
    "print(img_gpu.device, img_gpu.shape)\n",
    "print(img_gpu.shape)\n",
    "y = model(img_gpu)\n",
    "y.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0039, device='cuda:0'), tensor(0.0081, device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(img_gpu[0]) , torch.min(img_gpu[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model:nn.Module, data_loader:DataLoader, loss_fn:nn.Module, optimizer:torch.optim.Optimizer):\n",
    "    size = len(data_loader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 2 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    train_loss = train_loss / len(data_loader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= train_step(model, train_dl, RMSELoss(), torch.optim.Adam(model.parameters(), lr=1e-4))\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test step function\n",
    "def test_step(model:nn.Module, data_loader:DataLoader, loss_fn:nn.Module):\n",
    "    size = len(data_loader.dataset)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(data_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # if batch % 2 == 0:\n",
    "            #     loss, current = loss.item(), batch * len(X)\n",
    "            #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    test_loss /= len(data_loader)\n",
    "    print(f\"Test Loss: {test_loss:>8f}%\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= test_step(model, test_dl, RMSELoss())\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = RMSELoss(),\n",
    "          epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model=model,\n",
    "                                           data_loader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss = test_step(model=model,\n",
    "            data_loader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \",\n",
    "            f\"train_loss: {train_loss} | \",\n",
    "            f\"test_loss: {test_loss} | \"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Model is to Small it can't even overfit! | try RESNET~~\n",
    "## A lower learning rate (1e-5) solved out problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36683dd442ec47b1bb70932c63e759b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.262164  [    0/15296]\n",
      "loss: 0.128228  [   80/15296]\n",
      "loss: 0.272501  [  160/15296]\n",
      "loss: 0.110108  [  240/15296]\n",
      "loss: 0.170466  [  320/15296]\n",
      "loss: 0.313222  [  400/15296]\n",
      "loss: 0.247358  [  480/15296]\n",
      "loss: 0.311356  [  560/15296]\n",
      "loss: 0.311026  [  640/15296]\n",
      "loss: 0.175252  [  720/15296]\n",
      "loss: 0.170851  [  800/15296]\n",
      "loss: 0.306202  [  880/15296]\n",
      "loss: 0.277061  [  960/15296]\n",
      "loss: 0.281729  [ 1040/15296]\n",
      "loss: 0.180702  [ 1120/15296]\n",
      "loss: 0.182989  [ 1200/15296]\n",
      "loss: 0.239269  [ 1280/15296]\n",
      "loss: 0.211538  [ 1360/15296]\n",
      "loss: 0.196537  [ 1440/15296]\n",
      "loss: 0.244849  [ 1520/15296]\n",
      "loss: 0.288492  [ 1600/15296]\n",
      "loss: 0.204621  [ 1680/15296]\n",
      "loss: 0.314439  [ 1760/15296]\n",
      "loss: 0.106161  [ 1840/15296]\n",
      "loss: 0.328589  [ 1920/15296]\n",
      "loss: 0.281972  [ 2000/15296]\n",
      "loss: 0.231625  [ 2080/15296]\n",
      "loss: 0.253049  [ 2160/15296]\n",
      "loss: 0.249226  [ 2240/15296]\n",
      "loss: 0.273661  [ 2320/15296]\n",
      "loss: 0.253644  [ 2400/15296]\n",
      "loss: 0.188450  [ 2480/15296]\n",
      "loss: 0.179007  [ 2560/15296]\n",
      "loss: 0.181600  [ 2640/15296]\n",
      "loss: 0.313872  [ 2720/15296]\n",
      "loss: 0.214657  [ 2800/15296]\n",
      "loss: 0.182826  [ 2880/15296]\n",
      "loss: 0.190576  [ 2960/15296]\n",
      "loss: 0.182832  [ 3040/15296]\n",
      "loss: 0.161739  [ 3120/15296]\n",
      "loss: 0.208688  [ 3200/15296]\n",
      "loss: 0.165963  [ 3280/15296]\n",
      "loss: 0.097701  [ 3360/15296]\n",
      "loss: 0.179053  [ 3440/15296]\n",
      "loss: 0.172198  [ 3520/15296]\n",
      "loss: 0.351171  [ 3600/15296]\n",
      "loss: 0.211258  [ 3680/15296]\n",
      "loss: 0.206975  [ 3760/15296]\n",
      "loss: 0.179704  [ 3840/15296]\n",
      "loss: 0.121257  [ 3920/15296]\n",
      "loss: 0.114801  [ 4000/15296]\n",
      "loss: 0.069888  [ 4080/15296]\n",
      "loss: 0.336288  [ 4160/15296]\n",
      "loss: 0.256213  [ 4240/15296]\n",
      "loss: 0.157945  [ 4320/15296]\n",
      "loss: 0.122172  [ 4400/15296]\n",
      "loss: 0.250109  [ 4480/15296]\n",
      "loss: 0.185512  [ 4560/15296]\n",
      "loss: 0.212763  [ 4640/15296]\n",
      "loss: 0.249665  [ 4720/15296]\n",
      "loss: 0.185236  [ 4800/15296]\n",
      "loss: 0.167218  [ 4880/15296]\n",
      "loss: 0.230238  [ 4960/15296]\n",
      "loss: 0.168451  [ 5040/15296]\n",
      "loss: 0.197457  [ 5120/15296]\n",
      "loss: 0.238135  [ 5200/15296]\n",
      "loss: 0.219596  [ 5280/15296]\n",
      "loss: 0.179174  [ 5360/15296]\n",
      "loss: 0.179095  [ 5440/15296]\n",
      "loss: 0.107539  [ 5520/15296]\n",
      "loss: 0.322285  [ 5600/15296]\n",
      "loss: 0.176962  [ 5680/15296]\n",
      "loss: 0.281298  [ 5760/15296]\n",
      "loss: 0.167694  [ 5840/15296]\n",
      "loss: 0.256602  [ 5920/15296]\n",
      "loss: 0.216489  [ 6000/15296]\n",
      "loss: 0.206578  [ 6080/15296]\n",
      "loss: 0.145692  [ 6160/15296]\n",
      "loss: 0.189805  [ 6240/15296]\n",
      "loss: 0.280290  [ 6320/15296]\n",
      "loss: 0.168250  [ 6400/15296]\n",
      "loss: 0.162384  [ 6480/15296]\n",
      "loss: 0.203953  [ 6560/15296]\n",
      "loss: 0.212652  [ 6640/15296]\n",
      "loss: 0.213944  [ 6720/15296]\n",
      "loss: 0.189263  [ 6800/15296]\n",
      "loss: 0.249140  [ 6880/15296]\n",
      "loss: 0.167404  [ 6960/15296]\n",
      "loss: 0.205042  [ 7040/15296]\n",
      "loss: 0.192991  [ 7120/15296]\n",
      "loss: 0.183380  [ 7200/15296]\n",
      "loss: 0.180306  [ 7280/15296]\n",
      "loss: 0.249418  [ 7360/15296]\n",
      "loss: 0.265958  [ 7440/15296]\n",
      "loss: 0.159433  [ 7520/15296]\n",
      "loss: 0.273993  [ 7600/15296]\n",
      "loss: 0.186029  [ 7680/15296]\n",
      "loss: 0.177480  [ 7760/15296]\n",
      "loss: 0.209929  [ 7840/15296]\n",
      "loss: 0.135835  [ 7920/15296]\n",
      "loss: 0.267570  [ 8000/15296]\n",
      "loss: 0.229446  [ 8080/15296]\n",
      "loss: 0.306757  [ 8160/15296]\n",
      "loss: 0.217714  [ 8240/15296]\n",
      "loss: 0.166052  [ 8320/15296]\n",
      "loss: 0.198077  [ 8400/15296]\n",
      "loss: 0.120188  [ 8480/15296]\n",
      "loss: 0.345591  [ 8560/15296]\n",
      "loss: 0.208405  [ 8640/15296]\n",
      "loss: 0.290654  [ 8720/15296]\n",
      "loss: 0.190628  [ 8800/15296]\n",
      "loss: 0.249349  [ 8880/15296]\n",
      "loss: 0.307407  [ 8960/15296]\n",
      "loss: 0.187563  [ 9040/15296]\n",
      "loss: 0.211661  [ 9120/15296]\n",
      "loss: 0.256986  [ 9200/15296]\n",
      "loss: 0.260735  [ 9280/15296]\n",
      "loss: 0.169265  [ 9360/15296]\n",
      "loss: 0.264471  [ 9440/15296]\n",
      "loss: 0.190028  [ 9520/15296]\n",
      "loss: 0.174253  [ 9600/15296]\n",
      "loss: 0.163606  [ 9680/15296]\n",
      "loss: 0.203226  [ 9760/15296]\n",
      "loss: 0.199484  [ 9840/15296]\n",
      "loss: 0.166542  [ 9920/15296]\n",
      "loss: 0.279854  [10000/15296]\n",
      "loss: 0.250739  [10080/15296]\n",
      "loss: 0.201648  [10160/15296]\n",
      "loss: 0.188887  [10240/15296]\n",
      "loss: 0.206422  [10320/15296]\n",
      "loss: 0.206526  [10400/15296]\n",
      "loss: 0.215748  [10480/15296]\n",
      "loss: 0.234821  [10560/15296]\n",
      "loss: 0.165969  [10640/15296]\n",
      "loss: 0.213292  [10720/15296]\n",
      "loss: 0.332065  [10800/15296]\n",
      "loss: 0.222651  [10880/15296]\n",
      "loss: 0.225607  [10960/15296]\n",
      "loss: 0.272418  [11040/15296]\n",
      "loss: 0.167525  [11120/15296]\n",
      "loss: 0.243936  [11200/15296]\n",
      "loss: 0.268728  [11280/15296]\n",
      "loss: 0.211371  [11360/15296]\n",
      "loss: 0.183482  [11440/15296]\n",
      "loss: 0.216760  [11520/15296]\n",
      "loss: 0.246884  [11600/15296]\n",
      "loss: 0.229046  [11680/15296]\n",
      "loss: 0.254308  [11760/15296]\n",
      "loss: 0.290158  [11840/15296]\n",
      "loss: 0.202367  [11920/15296]\n",
      "loss: 0.135501  [12000/15296]\n",
      "loss: 0.183858  [12080/15296]\n",
      "loss: 0.224317  [12160/15296]\n",
      "loss: 0.204115  [12240/15296]\n",
      "loss: 0.202825  [12320/15296]\n",
      "loss: 0.225301  [12400/15296]\n",
      "loss: 0.129374  [12480/15296]\n",
      "loss: 0.231753  [12560/15296]\n",
      "loss: 0.183435  [12640/15296]\n",
      "loss: 0.221535  [12720/15296]\n",
      "loss: 0.100579  [12800/15296]\n",
      "loss: 0.284095  [12880/15296]\n",
      "loss: 0.199776  [12960/15296]\n",
      "loss: 0.277003  [13040/15296]\n",
      "loss: 0.195095  [13120/15296]\n",
      "loss: 0.165940  [13200/15296]\n",
      "loss: 0.216116  [13280/15296]\n",
      "loss: 0.225587  [13360/15296]\n",
      "loss: 0.212438  [13440/15296]\n",
      "loss: 0.245706  [13520/15296]\n",
      "loss: 0.128258  [13600/15296]\n",
      "loss: 0.224238  [13680/15296]\n",
      "loss: 0.162267  [13760/15296]\n",
      "loss: 0.197496  [13840/15296]\n",
      "loss: 0.262917  [13920/15296]\n",
      "loss: 0.249768  [14000/15296]\n",
      "loss: 0.284100  [14080/15296]\n",
      "loss: 0.220052  [14160/15296]\n",
      "loss: 0.243231  [14240/15296]\n",
      "loss: 0.290897  [14320/15296]\n",
      "loss: 0.138181  [14400/15296]\n",
      "loss: 0.181521  [14480/15296]\n",
      "loss: 0.186249  [14560/15296]\n",
      "loss: 0.176667  [14640/15296]\n",
      "loss: 0.237527  [14720/15296]\n",
      "loss: 0.143781  [14800/15296]\n",
      "loss: 0.221678  [14880/15296]\n",
      "loss: 0.270434  [14960/15296]\n",
      "loss: 0.265161  [15040/15296]\n",
      "loss: 0.200206  [15120/15296]\n",
      "loss: 0.323451  [15200/15296]\n",
      "loss: 0.271908  [ 6112/15296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.190745%\n",
      "Epoch: 1 |  train_loss: 0.2124485336924969 |  test_loss: 0.19074469124345475 | \n",
      "loss: 0.226012  [    0/15296]\n",
      "loss: 0.103859  [   80/15296]\n",
      "loss: 0.188940  [  160/15296]\n",
      "loss: 0.234730  [  240/15296]\n",
      "loss: 0.160507  [  320/15296]\n",
      "loss: 0.097801  [  400/15296]\n",
      "loss: 0.181355  [  480/15296]\n",
      "loss: 0.229649  [  560/15296]\n",
      "loss: 0.214906  [  640/15296]\n",
      "loss: 0.259706  [  720/15296]\n",
      "loss: 0.195027  [  800/15296]\n",
      "loss: 0.142075  [  880/15296]\n",
      "loss: 0.216346  [  960/15296]\n",
      "loss: 0.177041  [ 1040/15296]\n",
      "loss: 0.220427  [ 1120/15296]\n",
      "loss: 0.222111  [ 1200/15296]\n",
      "loss: 0.184099  [ 1280/15296]\n",
      "loss: 0.243941  [ 1360/15296]\n",
      "loss: 0.186323  [ 1440/15296]\n",
      "loss: 0.164113  [ 1520/15296]\n",
      "loss: 0.180122  [ 1600/15296]\n",
      "loss: 0.177560  [ 1680/15296]\n",
      "loss: 0.086970  [ 1760/15296]\n",
      "loss: 0.299391  [ 1840/15296]\n",
      "loss: 0.129401  [ 1920/15296]\n",
      "loss: 0.286828  [ 2000/15296]\n",
      "loss: 0.125230  [ 2080/15296]\n",
      "loss: 0.163177  [ 2160/15296]\n",
      "loss: 0.174043  [ 2240/15296]\n",
      "loss: 0.218924  [ 2320/15296]\n",
      "loss: 0.136674  [ 2400/15296]\n",
      "loss: 0.170222  [ 2480/15296]\n",
      "loss: 0.240653  [ 2560/15296]\n",
      "loss: 0.194000  [ 2640/15296]\n",
      "loss: 0.269172  [ 2720/15296]\n",
      "loss: 0.195844  [ 2800/15296]\n",
      "loss: 0.255618  [ 2880/15296]\n",
      "loss: 0.173531  [ 2960/15296]\n",
      "loss: 0.249717  [ 3040/15296]\n",
      "loss: 0.223629  [ 3120/15296]\n",
      "loss: 0.222963  [ 3200/15296]\n",
      "loss: 0.132365  [ 3280/15296]\n",
      "loss: 0.208029  [ 3360/15296]\n",
      "loss: 0.234218  [ 3440/15296]\n",
      "loss: 0.246735  [ 3520/15296]\n",
      "loss: 0.198044  [ 3600/15296]\n",
      "loss: 0.108911  [ 3680/15296]\n",
      "loss: 0.222008  [ 3760/15296]\n",
      "loss: 0.286726  [ 3840/15296]\n",
      "loss: 0.177236  [ 3920/15296]\n",
      "loss: 0.229416  [ 4000/15296]\n",
      "loss: 0.205819  [ 4080/15296]\n",
      "loss: 0.113970  [ 4160/15296]\n",
      "loss: 0.209089  [ 4240/15296]\n",
      "loss: 0.181283  [ 4320/15296]\n",
      "loss: 0.210884  [ 4400/15296]\n",
      "loss: 0.276965  [ 4480/15296]\n",
      "loss: 0.166602  [ 4560/15296]\n",
      "loss: 0.222522  [ 4640/15296]\n",
      "loss: 0.251135  [ 4720/15296]\n",
      "loss: 0.213816  [ 4800/15296]\n",
      "loss: 0.197675  [ 4880/15296]\n",
      "loss: 0.100244  [ 4960/15296]\n",
      "loss: 0.173149  [ 5040/15296]\n",
      "loss: 0.195469  [ 5120/15296]\n",
      "loss: 0.155698  [ 5200/15296]\n",
      "loss: 0.222414  [ 5280/15296]\n",
      "loss: 0.282789  [ 5360/15296]\n",
      "loss: 0.199448  [ 5440/15296]\n",
      "loss: 0.219808  [ 5520/15296]\n",
      "loss: 0.213704  [ 5600/15296]\n",
      "loss: 0.302410  [ 5680/15296]\n",
      "loss: 0.163412  [ 5760/15296]\n",
      "loss: 0.197555  [ 5840/15296]\n",
      "loss: 0.229369  [ 5920/15296]\n",
      "loss: 0.214964  [ 6000/15296]\n",
      "loss: 0.158722  [ 6080/15296]\n",
      "loss: 0.226241  [ 6160/15296]\n",
      "loss: 0.174368  [ 6240/15296]\n",
      "loss: 0.111404  [ 6320/15296]\n",
      "loss: 0.225015  [ 6400/15296]\n",
      "loss: 0.116167  [ 6480/15296]\n",
      "loss: 0.331901  [ 6560/15296]\n",
      "loss: 0.234344  [ 6640/15296]\n",
      "loss: 0.093045  [ 6720/15296]\n",
      "loss: 0.132918  [ 6800/15296]\n",
      "loss: 0.167485  [ 6880/15296]\n",
      "loss: 0.173207  [ 6960/15296]\n",
      "loss: 0.279728  [ 7040/15296]\n",
      "loss: 0.295750  [ 7120/15296]\n",
      "loss: 0.288035  [ 7200/15296]\n",
      "loss: 0.238591  [ 7280/15296]\n",
      "loss: 0.295545  [ 7360/15296]\n",
      "loss: 0.177197  [ 7440/15296]\n",
      "loss: 0.289274  [ 7520/15296]\n",
      "loss: 0.176826  [ 7600/15296]\n",
      "loss: 0.142349  [ 7680/15296]\n",
      "loss: 0.164591  [ 7760/15296]\n",
      "loss: 0.216336  [ 7840/15296]\n",
      "loss: 0.240939  [ 7920/15296]\n",
      "loss: 0.218137  [ 8000/15296]\n",
      "loss: 0.210099  [ 8080/15296]\n",
      "loss: 0.171517  [ 8160/15296]\n",
      "loss: 0.187712  [ 8240/15296]\n",
      "loss: 0.229724  [ 8320/15296]\n",
      "loss: 0.221134  [ 8400/15296]\n",
      "loss: 0.189658  [ 8480/15296]\n",
      "loss: 0.228572  [ 8560/15296]\n",
      "loss: 0.141444  [ 8640/15296]\n",
      "loss: 0.134653  [ 8720/15296]\n",
      "loss: 0.203863  [ 8800/15296]\n",
      "loss: 0.254696  [ 8880/15296]\n",
      "loss: 0.225423  [ 8960/15296]\n",
      "loss: 0.245008  [ 9040/15296]\n",
      "loss: 0.218017  [ 9120/15296]\n",
      "loss: 0.174899  [ 9200/15296]\n",
      "loss: 0.223542  [ 9280/15296]\n",
      "loss: 0.157751  [ 9360/15296]\n",
      "loss: 0.196083  [ 9440/15296]\n",
      "loss: 0.147228  [ 9520/15296]\n",
      "loss: 0.247768  [ 9600/15296]\n",
      "loss: 0.203252  [ 9680/15296]\n",
      "loss: 0.135542  [ 9760/15296]\n",
      "loss: 0.142407  [ 9840/15296]\n",
      "loss: 0.135506  [ 9920/15296]\n",
      "loss: 0.091620  [10000/15296]\n",
      "loss: 0.097639  [10080/15296]\n",
      "loss: 0.268791  [10160/15296]\n",
      "loss: 0.185459  [10240/15296]\n",
      "loss: 0.186781  [10320/15296]\n",
      "loss: 0.213490  [10400/15296]\n",
      "loss: 0.194329  [10480/15296]\n",
      "loss: 0.217246  [10560/15296]\n",
      "loss: 0.180637  [10640/15296]\n",
      "loss: 0.109596  [10720/15296]\n",
      "loss: 0.178536  [10800/15296]\n",
      "loss: 0.239464  [10880/15296]\n",
      "loss: 0.206598  [10960/15296]\n",
      "loss: 0.300778  [11040/15296]\n",
      "loss: 0.201895  [11120/15296]\n",
      "loss: 0.161640  [11200/15296]\n",
      "loss: 0.206026  [11280/15296]\n",
      "loss: 0.217982  [11360/15296]\n",
      "loss: 0.106665  [11440/15296]\n",
      "loss: 0.177234  [11520/15296]\n",
      "loss: 0.198352  [11600/15296]\n",
      "loss: 0.206114  [11680/15296]\n",
      "loss: 0.222559  [11760/15296]\n",
      "loss: 0.188378  [11840/15296]\n",
      "loss: 0.243761  [11920/15296]\n",
      "loss: 0.193501  [12000/15296]\n",
      "loss: 0.115548  [12080/15296]\n",
      "loss: 0.242752  [12160/15296]\n",
      "loss: 0.229513  [12240/15296]\n",
      "loss: 0.123514  [12320/15296]\n",
      "loss: 0.197928  [12400/15296]\n",
      "loss: 0.180597  [12480/15296]\n",
      "loss: 0.291541  [12560/15296]\n",
      "loss: 0.203989  [12640/15296]\n",
      "loss: 0.266526  [12720/15296]\n",
      "loss: 0.102050  [12800/15296]\n",
      "loss: 0.225652  [12880/15296]\n",
      "loss: 0.191531  [12960/15296]\n",
      "loss: 0.240084  [13040/15296]\n",
      "loss: 0.203934  [13120/15296]\n",
      "loss: 0.277469  [13200/15296]\n",
      "loss: 0.247974  [13280/15296]\n",
      "loss: 0.237699  [13360/15296]\n",
      "loss: 0.196573  [13440/15296]\n",
      "loss: 0.215844  [13520/15296]\n",
      "loss: 0.165579  [13600/15296]\n",
      "loss: 0.214998  [13680/15296]\n",
      "loss: 0.228026  [13760/15296]\n",
      "loss: 0.110816  [13840/15296]\n",
      "loss: 0.249324  [13920/15296]\n",
      "loss: 0.275846  [14000/15296]\n",
      "loss: 0.223878  [14080/15296]\n",
      "loss: 0.244786  [14160/15296]\n",
      "loss: 0.341507  [14240/15296]\n",
      "loss: 0.229983  [14320/15296]\n",
      "loss: 0.192122  [14400/15296]\n",
      "loss: 0.340604  [14480/15296]\n",
      "loss: 0.307295  [14560/15296]\n",
      "loss: 0.229913  [14640/15296]\n",
      "loss: 0.159254  [14720/15296]\n",
      "loss: 0.205720  [14800/15296]\n",
      "loss: 0.159459  [14880/15296]\n",
      "loss: 0.188443  [14960/15296]\n",
      "loss: 0.182199  [15040/15296]\n",
      "loss: 0.248659  [15120/15296]\n",
      "loss: 0.185000  [15200/15296]\n",
      "loss: 0.255545  [ 6112/15296]\n",
      "Test Loss: 0.192084%\n",
      "Epoch: 2 |  train_loss: 0.2107699076710731 |  test_loss: 0.1920838513364803 | \n",
      "loss: 0.159162  [    0/15296]\n",
      "loss: 0.243155  [   80/15296]\n",
      "loss: 0.290580  [  160/15296]\n",
      "loss: 0.274873  [  240/15296]\n",
      "loss: 0.182075  [  320/15296]\n",
      "loss: 0.261015  [  400/15296]\n",
      "loss: 0.229726  [  480/15296]\n",
      "loss: 0.219122  [  560/15296]\n",
      "loss: 0.241935  [  640/15296]\n",
      "loss: 0.275060  [  720/15296]\n",
      "loss: 0.275366  [  800/15296]\n",
      "loss: 0.238750  [  880/15296]\n",
      "loss: 0.176890  [  960/15296]\n",
      "loss: 0.235808  [ 1040/15296]\n",
      "loss: 0.075363  [ 1120/15296]\n",
      "loss: 0.206816  [ 1200/15296]\n",
      "loss: 0.228678  [ 1280/15296]\n",
      "loss: 0.216188  [ 1360/15296]\n",
      "loss: 0.172356  [ 1440/15296]\n",
      "loss: 0.162776  [ 1520/15296]\n",
      "loss: 0.207116  [ 1600/15296]\n",
      "loss: 0.210818  [ 1680/15296]\n",
      "loss: 0.208452  [ 1760/15296]\n",
      "loss: 0.267276  [ 1840/15296]\n",
      "loss: 0.250285  [ 1920/15296]\n",
      "loss: 0.219151  [ 2000/15296]\n",
      "loss: 0.146698  [ 2080/15296]\n",
      "loss: 0.296110  [ 2160/15296]\n",
      "loss: 0.195549  [ 2240/15296]\n",
      "loss: 0.191706  [ 2320/15296]\n",
      "loss: 0.213892  [ 2400/15296]\n",
      "loss: 0.089966  [ 2480/15296]\n",
      "loss: 0.191504  [ 2560/15296]\n",
      "loss: 0.267876  [ 2640/15296]\n",
      "loss: 0.177731  [ 2720/15296]\n",
      "loss: 0.182483  [ 2800/15296]\n",
      "loss: 0.267518  [ 2880/15296]\n",
      "loss: 0.303008  [ 2960/15296]\n",
      "loss: 0.153564  [ 3040/15296]\n",
      "loss: 0.122538  [ 3120/15296]\n",
      "loss: 0.225927  [ 3200/15296]\n",
      "loss: 0.240256  [ 3280/15296]\n",
      "loss: 0.253646  [ 3360/15296]\n",
      "loss: 0.205475  [ 3440/15296]\n",
      "loss: 0.218065  [ 3520/15296]\n",
      "loss: 0.258673  [ 3600/15296]\n",
      "loss: 0.110269  [ 3680/15296]\n",
      "loss: 0.247690  [ 3760/15296]\n",
      "loss: 0.168999  [ 3840/15296]\n",
      "loss: 0.276975  [ 3920/15296]\n",
      "loss: 0.214381  [ 4000/15296]\n",
      "loss: 0.205669  [ 4080/15296]\n",
      "loss: 0.171472  [ 4160/15296]\n",
      "loss: 0.275576  [ 4240/15296]\n",
      "loss: 0.210540  [ 4320/15296]\n",
      "loss: 0.207976  [ 4400/15296]\n",
      "loss: 0.221108  [ 4480/15296]\n",
      "loss: 0.193881  [ 4560/15296]\n",
      "loss: 0.187932  [ 4640/15296]\n",
      "loss: 0.307596  [ 4720/15296]\n",
      "loss: 0.256646  [ 4800/15296]\n",
      "loss: 0.220716  [ 4880/15296]\n",
      "loss: 0.270110  [ 4960/15296]\n",
      "loss: 0.227151  [ 5040/15296]\n",
      "loss: 0.263941  [ 5120/15296]\n",
      "loss: 0.198251  [ 5200/15296]\n",
      "loss: 0.235041  [ 5280/15296]\n",
      "loss: 0.129768  [ 5360/15296]\n",
      "loss: 0.237758  [ 5440/15296]\n",
      "loss: 0.189726  [ 5520/15296]\n",
      "loss: 0.127387  [ 5600/15296]\n",
      "loss: 0.141028  [ 5680/15296]\n",
      "loss: 0.213997  [ 5760/15296]\n",
      "loss: 0.186137  [ 5840/15296]\n",
      "loss: 0.243270  [ 5920/15296]\n",
      "loss: 0.241428  [ 6000/15296]\n",
      "loss: 0.224103  [ 6080/15296]\n",
      "loss: 0.151611  [ 6160/15296]\n",
      "loss: 0.194557  [ 6240/15296]\n",
      "loss: 0.176094  [ 6320/15296]\n",
      "loss: 0.258875  [ 6400/15296]\n",
      "loss: 0.155452  [ 6480/15296]\n",
      "loss: 0.114434  [ 6560/15296]\n",
      "loss: 0.280574  [ 6640/15296]\n",
      "loss: 0.255981  [ 6720/15296]\n",
      "loss: 0.196738  [ 6800/15296]\n",
      "loss: 0.202149  [ 6880/15296]\n",
      "loss: 0.159312  [ 6960/15296]\n",
      "loss: 0.165070  [ 7040/15296]\n",
      "loss: 0.281509  [ 7120/15296]\n",
      "loss: 0.217274  [ 7200/15296]\n",
      "loss: 0.188404  [ 7280/15296]\n",
      "loss: 0.164413  [ 7360/15296]\n",
      "loss: 0.231131  [ 7440/15296]\n",
      "loss: 0.269928  [ 7520/15296]\n",
      "loss: 0.262419  [ 7600/15296]\n",
      "loss: 0.267088  [ 7680/15296]\n",
      "loss: 0.184955  [ 7760/15296]\n",
      "loss: 0.163967  [ 7840/15296]\n",
      "loss: 0.186126  [ 7920/15296]\n",
      "loss: 0.225269  [ 8000/15296]\n",
      "loss: 0.222445  [ 8080/15296]\n",
      "loss: 0.165155  [ 8160/15296]\n",
      "loss: 0.282905  [ 8240/15296]\n",
      "loss: 0.228091  [ 8320/15296]\n",
      "loss: 0.278234  [ 8400/15296]\n",
      "loss: 0.289785  [ 8480/15296]\n",
      "loss: 0.187373  [ 8560/15296]\n",
      "loss: 0.244286  [ 8640/15296]\n",
      "loss: 0.255004  [ 8720/15296]\n",
      "loss: 0.180105  [ 8800/15296]\n",
      "loss: 0.099926  [ 8880/15296]\n",
      "loss: 0.292280  [ 8960/15296]\n",
      "loss: 0.229900  [ 9040/15296]\n",
      "loss: 0.218836  [ 9120/15296]\n",
      "loss: 0.263379  [ 9200/15296]\n",
      "loss: 0.158430  [ 9280/15296]\n",
      "loss: 0.171397  [ 9360/15296]\n",
      "loss: 0.254197  [ 9440/15296]\n",
      "loss: 0.260023  [ 9520/15296]\n",
      "loss: 0.218590  [ 9600/15296]\n",
      "loss: 0.255881  [ 9680/15296]\n",
      "loss: 0.232283  [ 9760/15296]\n",
      "loss: 0.238162  [ 9840/15296]\n",
      "loss: 0.183531  [ 9920/15296]\n",
      "loss: 0.178794  [10000/15296]\n",
      "loss: 0.263450  [10080/15296]\n",
      "loss: 0.242559  [10160/15296]\n",
      "loss: 0.254310  [10240/15296]\n",
      "loss: 0.232646  [10320/15296]\n",
      "loss: 0.214552  [10400/15296]\n",
      "loss: 0.251271  [10480/15296]\n",
      "loss: 0.247534  [10560/15296]\n",
      "loss: 0.226538  [10640/15296]\n",
      "loss: 0.169779  [10720/15296]\n",
      "loss: 0.217982  [10800/15296]\n",
      "loss: 0.234188  [10880/15296]\n",
      "loss: 0.249044  [10960/15296]\n",
      "loss: 0.231523  [11040/15296]\n",
      "loss: 0.194588  [11120/15296]\n",
      "loss: 0.224116  [11200/15296]\n",
      "loss: 0.160413  [11280/15296]\n",
      "loss: 0.236675  [11360/15296]\n",
      "loss: 0.225824  [11440/15296]\n",
      "loss: 0.134049  [11520/15296]\n",
      "loss: 0.185413  [11600/15296]\n",
      "loss: 0.120488  [11680/15296]\n",
      "loss: 0.207042  [11760/15296]\n",
      "loss: 0.199661  [11840/15296]\n",
      "loss: 0.239197  [11920/15296]\n",
      "loss: 0.213521  [12000/15296]\n",
      "loss: 0.232364  [12080/15296]\n",
      "loss: 0.114623  [12160/15296]\n",
      "loss: 0.197446  [12240/15296]\n",
      "loss: 0.157909  [12320/15296]\n",
      "loss: 0.353729  [12400/15296]\n",
      "loss: 0.145317  [12480/15296]\n",
      "loss: 0.191955  [12560/15296]\n",
      "loss: 0.188797  [12640/15296]\n",
      "loss: 0.299880  [12720/15296]\n",
      "loss: 0.192694  [12800/15296]\n",
      "loss: 0.246612  [12880/15296]\n",
      "loss: 0.197989  [12960/15296]\n",
      "loss: 0.248999  [13040/15296]\n",
      "loss: 0.272720  [13120/15296]\n",
      "loss: 0.112318  [13200/15296]\n",
      "loss: 0.186137  [13280/15296]\n",
      "loss: 0.241989  [13360/15296]\n",
      "loss: 0.206728  [13440/15296]\n",
      "loss: 0.260297  [13520/15296]\n",
      "loss: 0.295928  [13600/15296]\n",
      "loss: 0.170170  [13680/15296]\n",
      "loss: 0.111805  [13760/15296]\n",
      "loss: 0.171406  [13840/15296]\n",
      "loss: 0.177399  [13920/15296]\n",
      "loss: 0.174522  [14000/15296]\n",
      "loss: 0.141690  [14080/15296]\n",
      "loss: 0.190420  [14160/15296]\n",
      "loss: 0.178683  [14240/15296]\n",
      "loss: 0.213600  [14320/15296]\n",
      "loss: 0.245240  [14400/15296]\n",
      "loss: 0.262511  [14480/15296]\n",
      "loss: 0.166175  [14560/15296]\n",
      "loss: 0.297856  [14640/15296]\n",
      "loss: 0.202685  [14720/15296]\n",
      "loss: 0.240531  [14800/15296]\n",
      "loss: 0.211547  [14880/15296]\n",
      "loss: 0.170021  [14960/15296]\n",
      "loss: 0.254890  [15040/15296]\n",
      "loss: 0.182331  [15120/15296]\n",
      "loss: 0.221575  [15200/15296]\n",
      "loss: 0.231270  [ 6112/15296]\n",
      "Test Loss: 0.197990%\n",
      "Epoch: 3 |  train_loss: 0.21193355847120907 |  test_loss: 0.1979904960007428 | \n",
      "loss: 0.301900  [    0/15296]\n",
      "loss: 0.334516  [   80/15296]\n",
      "loss: 0.166658  [  160/15296]\n",
      "loss: 0.200531  [  240/15296]\n",
      "loss: 0.271365  [  320/15296]\n",
      "loss: 0.111041  [  400/15296]\n",
      "loss: 0.295309  [  480/15296]\n",
      "loss: 0.154976  [  560/15296]\n",
      "loss: 0.270353  [  640/15296]\n",
      "loss: 0.248524  [  720/15296]\n",
      "loss: 0.348234  [  800/15296]\n",
      "loss: 0.156666  [  880/15296]\n",
      "loss: 0.206722  [  960/15296]\n",
      "loss: 0.126460  [ 1040/15296]\n",
      "loss: 0.225220  [ 1120/15296]\n",
      "loss: 0.239247  [ 1200/15296]\n",
      "loss: 0.393810  [ 1280/15296]\n",
      "loss: 0.264483  [ 1360/15296]\n",
      "loss: 0.173352  [ 1440/15296]\n",
      "loss: 0.208982  [ 1520/15296]\n",
      "loss: 0.358195  [ 1600/15296]\n",
      "loss: 0.227935  [ 1680/15296]\n",
      "loss: 0.262066  [ 1760/15296]\n",
      "loss: 0.166052  [ 1840/15296]\n",
      "loss: 0.291029  [ 1920/15296]\n",
      "loss: 0.270294  [ 2000/15296]\n",
      "loss: 0.181001  [ 2080/15296]\n",
      "loss: 0.215781  [ 2160/15296]\n",
      "loss: 0.231896  [ 2240/15296]\n",
      "loss: 0.122607  [ 2320/15296]\n",
      "loss: 0.261578  [ 2400/15296]\n",
      "loss: 0.249315  [ 2480/15296]\n",
      "loss: 0.209813  [ 2560/15296]\n",
      "loss: 0.259864  [ 2640/15296]\n",
      "loss: 0.173044  [ 2720/15296]\n",
      "loss: 0.230992  [ 2800/15296]\n",
      "loss: 0.279591  [ 2880/15296]\n",
      "loss: 0.250680  [ 2960/15296]\n",
      "loss: 0.224173  [ 3040/15296]\n",
      "loss: 0.165808  [ 3120/15296]\n",
      "loss: 0.163806  [ 3200/15296]\n",
      "loss: 0.109342  [ 3280/15296]\n",
      "loss: 0.189283  [ 3360/15296]\n",
      "loss: 0.290467  [ 3440/15296]\n",
      "loss: 0.300523  [ 3520/15296]\n",
      "loss: 0.212208  [ 3600/15296]\n",
      "loss: 0.252963  [ 3680/15296]\n",
      "loss: 0.151380  [ 3760/15296]\n",
      "loss: 0.215228  [ 3840/15296]\n",
      "loss: 0.243648  [ 3920/15296]\n",
      "loss: 0.212988  [ 4000/15296]\n",
      "loss: 0.169502  [ 4080/15296]\n",
      "loss: 0.215617  [ 4160/15296]\n",
      "loss: 0.205199  [ 4240/15296]\n",
      "loss: 0.201057  [ 4320/15296]\n",
      "loss: 0.237691  [ 4400/15296]\n",
      "loss: 0.294007  [ 4480/15296]\n",
      "loss: 0.145964  [ 4560/15296]\n",
      "loss: 0.230033  [ 4640/15296]\n",
      "loss: 0.175880  [ 4720/15296]\n",
      "loss: 0.264240  [ 4800/15296]\n",
      "loss: 0.204811  [ 4880/15296]\n",
      "loss: 0.271922  [ 4960/15296]\n",
      "loss: 0.190857  [ 5040/15296]\n",
      "loss: 0.111405  [ 5120/15296]\n",
      "loss: 0.187093  [ 5200/15296]\n",
      "loss: 0.270636  [ 5280/15296]\n",
      "loss: 0.171525  [ 5360/15296]\n",
      "loss: 0.187772  [ 5440/15296]\n",
      "loss: 0.181587  [ 5520/15296]\n",
      "loss: 0.171015  [ 5600/15296]\n",
      "loss: 0.262841  [ 5680/15296]\n",
      "loss: 0.229603  [ 5760/15296]\n",
      "loss: 0.154942  [ 5840/15296]\n",
      "loss: 0.236433  [ 5920/15296]\n",
      "loss: 0.094617  [ 6000/15296]\n",
      "loss: 0.172511  [ 6080/15296]\n",
      "loss: 0.209764  [ 6160/15296]\n",
      "loss: 0.199588  [ 6240/15296]\n",
      "loss: 0.279829  [ 6320/15296]\n",
      "loss: 0.168739  [ 6400/15296]\n",
      "loss: 0.221833  [ 6480/15296]\n",
      "loss: 0.216122  [ 6560/15296]\n",
      "loss: 0.309550  [ 6640/15296]\n",
      "loss: 0.252343  [ 6720/15296]\n",
      "loss: 0.277051  [ 6800/15296]\n",
      "loss: 0.224096  [ 6880/15296]\n",
      "loss: 0.187785  [ 6960/15296]\n",
      "loss: 0.216215  [ 7040/15296]\n",
      "loss: 0.257500  [ 7120/15296]\n",
      "loss: 0.178716  [ 7200/15296]\n",
      "loss: 0.193542  [ 7280/15296]\n",
      "loss: 0.282226  [ 7360/15296]\n",
      "loss: 0.195741  [ 7440/15296]\n",
      "loss: 0.162426  [ 7520/15296]\n",
      "loss: 0.254649  [ 7600/15296]\n",
      "loss: 0.207125  [ 7680/15296]\n",
      "loss: 0.166580  [ 7760/15296]\n",
      "loss: 0.251559  [ 7840/15296]\n",
      "loss: 0.193587  [ 7920/15296]\n",
      "loss: 0.247351  [ 8000/15296]\n",
      "loss: 0.202625  [ 8080/15296]\n",
      "loss: 0.224307  [ 8160/15296]\n",
      "loss: 0.250771  [ 8240/15296]\n",
      "loss: 0.198402  [ 8320/15296]\n",
      "loss: 0.206489  [ 8400/15296]\n",
      "loss: 0.247797  [ 8480/15296]\n",
      "loss: 0.246187  [ 8560/15296]\n",
      "loss: 0.294733  [ 8640/15296]\n",
      "loss: 0.199491  [ 8720/15296]\n",
      "loss: 0.218844  [ 8800/15296]\n",
      "loss: 0.153195  [ 8880/15296]\n",
      "loss: 0.217703  [ 8960/15296]\n",
      "loss: 0.117074  [ 9040/15296]\n",
      "loss: 0.228484  [ 9120/15296]\n",
      "loss: 0.236658  [ 9200/15296]\n",
      "loss: 0.117517  [ 9280/15296]\n",
      "loss: 0.148865  [ 9360/15296]\n",
      "loss: 0.253404  [ 9440/15296]\n",
      "loss: 0.230700  [ 9520/15296]\n",
      "loss: 0.219256  [ 9600/15296]\n",
      "loss: 0.237099  [ 9680/15296]\n",
      "loss: 0.224610  [ 9760/15296]\n",
      "loss: 0.141415  [ 9840/15296]\n",
      "loss: 0.214588  [ 9920/15296]\n",
      "loss: 0.159867  [10000/15296]\n",
      "loss: 0.322309  [10080/15296]\n",
      "loss: 0.225930  [10160/15296]\n",
      "loss: 0.103215  [10240/15296]\n",
      "loss: 0.299654  [10320/15296]\n",
      "loss: 0.224785  [10400/15296]\n",
      "loss: 0.259751  [10480/15296]\n",
      "loss: 0.326238  [10560/15296]\n",
      "loss: 0.167898  [10640/15296]\n",
      "loss: 0.242444  [10720/15296]\n",
      "loss: 0.242291  [10800/15296]\n",
      "loss: 0.302725  [10880/15296]\n",
      "loss: 0.282809  [10960/15296]\n",
      "loss: 0.138323  [11040/15296]\n",
      "loss: 0.255308  [11120/15296]\n",
      "loss: 0.188571  [11200/15296]\n",
      "loss: 0.194405  [11280/15296]\n",
      "loss: 0.098584  [11360/15296]\n",
      "loss: 0.222121  [11440/15296]\n",
      "loss: 0.100302  [11520/15296]\n",
      "loss: 0.101384  [11600/15296]\n",
      "loss: 0.299931  [11680/15296]\n",
      "loss: 0.195493  [11760/15296]\n",
      "loss: 0.250914  [11840/15296]\n",
      "loss: 0.248712  [11920/15296]\n",
      "loss: 0.221269  [12000/15296]\n",
      "loss: 0.091966  [12080/15296]\n",
      "loss: 0.346094  [12160/15296]\n",
      "loss: 0.223479  [12240/15296]\n",
      "loss: 0.295541  [12320/15296]\n",
      "loss: 0.191668  [12400/15296]\n",
      "loss: 0.256599  [12480/15296]\n",
      "loss: 0.165076  [12560/15296]\n",
      "loss: 0.194129  [12640/15296]\n",
      "loss: 0.195097  [12720/15296]\n",
      "loss: 0.157410  [12800/15296]\n",
      "loss: 0.215961  [12880/15296]\n",
      "loss: 0.175150  [12960/15296]\n",
      "loss: 0.128161  [13040/15296]\n",
      "loss: 0.162125  [13120/15296]\n",
      "loss: 0.121488  [13200/15296]\n",
      "loss: 0.283597  [13280/15296]\n",
      "loss: 0.273404  [13360/15296]\n",
      "loss: 0.202030  [13440/15296]\n",
      "loss: 0.161181  [13520/15296]\n",
      "loss: 0.142125  [13600/15296]\n",
      "loss: 0.194398  [13680/15296]\n",
      "loss: 0.182643  [13760/15296]\n",
      "loss: 0.205183  [13840/15296]\n",
      "loss: 0.157118  [13920/15296]\n",
      "loss: 0.211031  [14000/15296]\n",
      "loss: 0.234980  [14080/15296]\n",
      "loss: 0.312853  [14160/15296]\n",
      "loss: 0.120852  [14240/15296]\n",
      "loss: 0.243430  [14320/15296]\n",
      "loss: 0.255144  [14400/15296]\n",
      "loss: 0.251733  [14480/15296]\n",
      "loss: 0.160756  [14560/15296]\n",
      "loss: 0.207622  [14640/15296]\n",
      "loss: 0.283745  [14720/15296]\n",
      "loss: 0.138983  [14800/15296]\n",
      "loss: 0.206140  [14880/15296]\n",
      "loss: 0.197337  [14960/15296]\n",
      "loss: 0.280635  [15040/15296]\n",
      "loss: 0.094966  [15120/15296]\n",
      "loss: 0.252163  [15200/15296]\n",
      "loss: 0.236473  [ 6112/15296]\n",
      "Test Loss: 0.191843%\n",
      "Epoch: 4 |  train_loss: 0.21074989611190542 |  test_loss: 0.19184270575982795 | \n",
      "loss: 0.289485  [    0/15296]\n",
      "loss: 0.161895  [   80/15296]\n",
      "loss: 0.147301  [  160/15296]\n",
      "loss: 0.292390  [  240/15296]\n",
      "loss: 0.235839  [  320/15296]\n",
      "loss: 0.252805  [  400/15296]\n",
      "loss: 0.201373  [  480/15296]\n",
      "loss: 0.213191  [  560/15296]\n",
      "loss: 0.225574  [  640/15296]\n",
      "loss: 0.199321  [  720/15296]\n",
      "loss: 0.269265  [  800/15296]\n",
      "loss: 0.254797  [  880/15296]\n",
      "loss: 0.202617  [  960/15296]\n",
      "loss: 0.217274  [ 1040/15296]\n",
      "loss: 0.177240  [ 1120/15296]\n",
      "loss: 0.183407  [ 1200/15296]\n",
      "loss: 0.312249  [ 1280/15296]\n",
      "loss: 0.247702  [ 1360/15296]\n",
      "loss: 0.128452  [ 1440/15296]\n",
      "loss: 0.254528  [ 1520/15296]\n",
      "loss: 0.135116  [ 1600/15296]\n",
      "loss: 0.158750  [ 1680/15296]\n",
      "loss: 0.101962  [ 1760/15296]\n",
      "loss: 0.281847  [ 1840/15296]\n",
      "loss: 0.245367  [ 1920/15296]\n",
      "loss: 0.291198  [ 2000/15296]\n",
      "loss: 0.225448  [ 2080/15296]\n",
      "loss: 0.111107  [ 2160/15296]\n",
      "loss: 0.163291  [ 2240/15296]\n",
      "loss: 0.192667  [ 2320/15296]\n",
      "loss: 0.254811  [ 2400/15296]\n",
      "loss: 0.209134  [ 2480/15296]\n",
      "loss: 0.203502  [ 2560/15296]\n",
      "loss: 0.197822  [ 2640/15296]\n",
      "loss: 0.161686  [ 2720/15296]\n",
      "loss: 0.167627  [ 2800/15296]\n",
      "loss: 0.296960  [ 2880/15296]\n",
      "loss: 0.239143  [ 2960/15296]\n",
      "loss: 0.267523  [ 3040/15296]\n",
      "loss: 0.223378  [ 3120/15296]\n",
      "loss: 0.278885  [ 3200/15296]\n",
      "loss: 0.172283  [ 3280/15296]\n",
      "loss: 0.219496  [ 3360/15296]\n",
      "loss: 0.194722  [ 3440/15296]\n",
      "loss: 0.160855  [ 3520/15296]\n",
      "loss: 0.185306  [ 3600/15296]\n",
      "loss: 0.123184  [ 3680/15296]\n",
      "loss: 0.272385  [ 3760/15296]\n",
      "loss: 0.182431  [ 3840/15296]\n",
      "loss: 0.225139  [ 3920/15296]\n",
      "loss: 0.176667  [ 4000/15296]\n",
      "loss: 0.330656  [ 4080/15296]\n",
      "loss: 0.201668  [ 4160/15296]\n",
      "loss: 0.251057  [ 4240/15296]\n",
      "loss: 0.244106  [ 4320/15296]\n",
      "loss: 0.239310  [ 4400/15296]\n",
      "loss: 0.169920  [ 4480/15296]\n",
      "loss: 0.121424  [ 4560/15296]\n",
      "loss: 0.254697  [ 4640/15296]\n",
      "loss: 0.230180  [ 4720/15296]\n",
      "loss: 0.270245  [ 4800/15296]\n",
      "loss: 0.223534  [ 4880/15296]\n",
      "loss: 0.186176  [ 4960/15296]\n",
      "loss: 0.244307  [ 5040/15296]\n",
      "loss: 0.265818  [ 5120/15296]\n",
      "loss: 0.246046  [ 5200/15296]\n",
      "loss: 0.125657  [ 5280/15296]\n",
      "loss: 0.157517  [ 5360/15296]\n",
      "loss: 0.320113  [ 5440/15296]\n",
      "loss: 0.311067  [ 5520/15296]\n",
      "loss: 0.196167  [ 5600/15296]\n",
      "loss: 0.287115  [ 5680/15296]\n",
      "loss: 0.135674  [ 5760/15296]\n",
      "loss: 0.222577  [ 5840/15296]\n",
      "loss: 0.185297  [ 5920/15296]\n",
      "loss: 0.237322  [ 6000/15296]\n",
      "loss: 0.170918  [ 6080/15296]\n",
      "loss: 0.179744  [ 6160/15296]\n",
      "loss: 0.209717  [ 6240/15296]\n",
      "loss: 0.222049  [ 6320/15296]\n",
      "loss: 0.211666  [ 6400/15296]\n",
      "loss: 0.138598  [ 6480/15296]\n",
      "loss: 0.112772  [ 6560/15296]\n",
      "loss: 0.254595  [ 6640/15296]\n",
      "loss: 0.230704  [ 6720/15296]\n",
      "loss: 0.217485  [ 6800/15296]\n",
      "loss: 0.283955  [ 6880/15296]\n",
      "loss: 0.162347  [ 6960/15296]\n",
      "loss: 0.209219  [ 7040/15296]\n",
      "loss: 0.231904  [ 7120/15296]\n",
      "loss: 0.243580  [ 7200/15296]\n",
      "loss: 0.215103  [ 7280/15296]\n",
      "loss: 0.148853  [ 7360/15296]\n",
      "loss: 0.212757  [ 7440/15296]\n",
      "loss: 0.243239  [ 7520/15296]\n",
      "loss: 0.126641  [ 7600/15296]\n",
      "loss: 0.309442  [ 7680/15296]\n",
      "loss: 0.201559  [ 7760/15296]\n",
      "loss: 0.236154  [ 7840/15296]\n",
      "loss: 0.170197  [ 7920/15296]\n",
      "loss: 0.117197  [ 8000/15296]\n",
      "loss: 0.146114  [ 8080/15296]\n",
      "loss: 0.173764  [ 8160/15296]\n",
      "loss: 0.103767  [ 8240/15296]\n",
      "loss: 0.311826  [ 8320/15296]\n",
      "loss: 0.222406  [ 8400/15296]\n",
      "loss: 0.203204  [ 8480/15296]\n",
      "loss: 0.180785  [ 8560/15296]\n",
      "loss: 0.086151  [ 8640/15296]\n",
      "loss: 0.234184  [ 8720/15296]\n",
      "loss: 0.324797  [ 8800/15296]\n",
      "loss: 0.252935  [ 8880/15296]\n",
      "loss: 0.194310  [ 8960/15296]\n",
      "loss: 0.214005  [ 9040/15296]\n",
      "loss: 0.098818  [ 9120/15296]\n",
      "loss: 0.162961  [ 9200/15296]\n",
      "loss: 0.187184  [ 9280/15296]\n",
      "loss: 0.293769  [ 9360/15296]\n",
      "loss: 0.251826  [ 9440/15296]\n",
      "loss: 0.223049  [ 9520/15296]\n",
      "loss: 0.192535  [ 9600/15296]\n",
      "loss: 0.323195  [ 9680/15296]\n",
      "loss: 0.192888  [ 9760/15296]\n",
      "loss: 0.184531  [ 9840/15296]\n",
      "loss: 0.293793  [ 9920/15296]\n",
      "loss: 0.261349  [10000/15296]\n",
      "loss: 0.195459  [10080/15296]\n",
      "loss: 0.216546  [10160/15296]\n",
      "loss: 0.188568  [10240/15296]\n",
      "loss: 0.212204  [10320/15296]\n",
      "loss: 0.255559  [10400/15296]\n",
      "loss: 0.266461  [10480/15296]\n",
      "loss: 0.256895  [10560/15296]\n",
      "loss: 0.196326  [10640/15296]\n",
      "loss: 0.313280  [10720/15296]\n",
      "loss: 0.238068  [10800/15296]\n",
      "loss: 0.135370  [10880/15296]\n",
      "loss: 0.266711  [10960/15296]\n",
      "loss: 0.135389  [11040/15296]\n",
      "loss: 0.195069  [11120/15296]\n",
      "loss: 0.183082  [11200/15296]\n",
      "loss: 0.272796  [11280/15296]\n",
      "loss: 0.193242  [11360/15296]\n",
      "loss: 0.264696  [11440/15296]\n",
      "loss: 0.118408  [11520/15296]\n",
      "loss: 0.135967  [11600/15296]\n",
      "loss: 0.128608  [11680/15296]\n",
      "loss: 0.195682  [11760/15296]\n",
      "loss: 0.148001  [11840/15296]\n",
      "loss: 0.206809  [11920/15296]\n",
      "loss: 0.218008  [12000/15296]\n",
      "loss: 0.293095  [12080/15296]\n",
      "loss: 0.201314  [12160/15296]\n",
      "loss: 0.227432  [12240/15296]\n",
      "loss: 0.317282  [12320/15296]\n",
      "loss: 0.206946  [12400/15296]\n",
      "loss: 0.144523  [12480/15296]\n",
      "loss: 0.206804  [12560/15296]\n",
      "loss: 0.161603  [12640/15296]\n",
      "loss: 0.290969  [12720/15296]\n",
      "loss: 0.263432  [12800/15296]\n",
      "loss: 0.312253  [12880/15296]\n",
      "loss: 0.117921  [12960/15296]\n",
      "loss: 0.192368  [13040/15296]\n",
      "loss: 0.094123  [13120/15296]\n",
      "loss: 0.163001  [13200/15296]\n",
      "loss: 0.140588  [13280/15296]\n",
      "loss: 0.244743  [13360/15296]\n",
      "loss: 0.194965  [13440/15296]\n",
      "loss: 0.255432  [13520/15296]\n",
      "loss: 0.102932  [13600/15296]\n",
      "loss: 0.134248  [13680/15296]\n",
      "loss: 0.180269  [13760/15296]\n",
      "loss: 0.179300  [13840/15296]\n",
      "loss: 0.172140  [13920/15296]\n",
      "loss: 0.221907  [14000/15296]\n",
      "loss: 0.212851  [14080/15296]\n",
      "loss: 0.182820  [14160/15296]\n",
      "loss: 0.236191  [14240/15296]\n",
      "loss: 0.133201  [14320/15296]\n",
      "loss: 0.137434  [14400/15296]\n",
      "loss: 0.310101  [14480/15296]\n",
      "loss: 0.245946  [14560/15296]\n",
      "loss: 0.259044  [14640/15296]\n",
      "loss: 0.171952  [14720/15296]\n",
      "loss: 0.262469  [14800/15296]\n",
      "loss: 0.216847  [14880/15296]\n",
      "loss: 0.196017  [14960/15296]\n",
      "loss: 0.195634  [15040/15296]\n",
      "loss: 0.200439  [15120/15296]\n",
      "loss: 0.115151  [15200/15296]\n",
      "loss: 0.097567  [ 6112/15296]\n",
      "Test Loss: 0.192100%\n",
      "Epoch: 5 |  train_loss: 0.20975886156913817 |  test_loss: 0.19209989132232044 | \n",
      "loss: 0.266826  [    0/15296]\n",
      "loss: 0.205364  [   80/15296]\n",
      "loss: 0.125990  [  160/15296]\n",
      "loss: 0.276611  [  240/15296]\n",
      "loss: 0.220615  [  320/15296]\n",
      "loss: 0.246915  [  400/15296]\n",
      "loss: 0.255967  [  480/15296]\n",
      "loss: 0.101780  [  560/15296]\n",
      "loss: 0.135727  [  640/15296]\n",
      "loss: 0.092730  [  720/15296]\n",
      "loss: 0.236638  [  800/15296]\n",
      "loss: 0.226467  [  880/15296]\n",
      "loss: 0.355678  [  960/15296]\n",
      "loss: 0.225666  [ 1040/15296]\n",
      "loss: 0.225160  [ 1120/15296]\n",
      "loss: 0.280528  [ 1200/15296]\n",
      "loss: 0.174757  [ 1280/15296]\n",
      "loss: 0.207932  [ 1360/15296]\n",
      "loss: 0.301079  [ 1440/15296]\n",
      "loss: 0.168360  [ 1520/15296]\n",
      "loss: 0.248573  [ 1600/15296]\n",
      "loss: 0.103039  [ 1680/15296]\n",
      "loss: 0.261868  [ 1760/15296]\n",
      "loss: 0.166840  [ 1840/15296]\n",
      "loss: 0.212295  [ 1920/15296]\n",
      "loss: 0.195995  [ 2000/15296]\n",
      "loss: 0.170416  [ 2080/15296]\n",
      "loss: 0.283885  [ 2160/15296]\n",
      "loss: 0.217873  [ 2240/15296]\n",
      "loss: 0.173174  [ 2320/15296]\n",
      "loss: 0.096734  [ 2400/15296]\n",
      "loss: 0.229444  [ 2480/15296]\n",
      "loss: 0.208192  [ 2560/15296]\n",
      "loss: 0.296787  [ 2640/15296]\n",
      "loss: 0.210288  [ 2720/15296]\n",
      "loss: 0.254527  [ 2800/15296]\n",
      "loss: 0.236475  [ 2880/15296]\n",
      "loss: 0.223461  [ 2960/15296]\n",
      "loss: 0.174186  [ 3040/15296]\n",
      "loss: 0.262750  [ 3120/15296]\n",
      "loss: 0.150682  [ 3200/15296]\n",
      "loss: 0.244442  [ 3280/15296]\n",
      "loss: 0.182817  [ 3360/15296]\n",
      "loss: 0.145420  [ 3440/15296]\n",
      "loss: 0.231984  [ 3520/15296]\n",
      "loss: 0.185567  [ 3600/15296]\n",
      "loss: 0.265786  [ 3680/15296]\n",
      "loss: 0.218632  [ 3760/15296]\n",
      "loss: 0.217784  [ 3840/15296]\n",
      "loss: 0.160299  [ 3920/15296]\n",
      "loss: 0.245257  [ 4000/15296]\n",
      "loss: 0.230125  [ 4080/15296]\n",
      "loss: 0.242115  [ 4160/15296]\n",
      "loss: 0.236072  [ 4240/15296]\n",
      "loss: 0.181448  [ 4320/15296]\n",
      "loss: 0.243879  [ 4400/15296]\n",
      "loss: 0.218755  [ 4480/15296]\n",
      "loss: 0.169291  [ 4560/15296]\n",
      "loss: 0.221825  [ 4640/15296]\n",
      "loss: 0.325438  [ 4720/15296]\n",
      "loss: 0.199375  [ 4800/15296]\n",
      "loss: 0.182133  [ 4880/15296]\n",
      "loss: 0.233745  [ 4960/15296]\n",
      "loss: 0.271176  [ 5040/15296]\n",
      "loss: 0.232410  [ 5120/15296]\n",
      "loss: 0.245379  [ 5200/15296]\n",
      "loss: 0.252955  [ 5280/15296]\n",
      "loss: 0.284780  [ 5360/15296]\n",
      "loss: 0.232920  [ 5440/15296]\n",
      "loss: 0.263335  [ 5520/15296]\n",
      "loss: 0.179827  [ 5600/15296]\n",
      "loss: 0.285940  [ 5680/15296]\n",
      "loss: 0.209224  [ 5760/15296]\n",
      "loss: 0.315311  [ 5840/15296]\n",
      "loss: 0.183839  [ 5920/15296]\n",
      "loss: 0.238373  [ 6000/15296]\n",
      "loss: 0.154924  [ 6080/15296]\n",
      "loss: 0.163343  [ 6160/15296]\n",
      "loss: 0.190644  [ 6240/15296]\n",
      "loss: 0.209876  [ 6320/15296]\n",
      "loss: 0.169200  [ 6400/15296]\n",
      "loss: 0.290220  [ 6480/15296]\n",
      "loss: 0.138490  [ 6560/15296]\n",
      "loss: 0.245297  [ 6640/15296]\n",
      "loss: 0.277208  [ 6720/15296]\n",
      "loss: 0.194639  [ 6800/15296]\n",
      "loss: 0.248975  [ 6880/15296]\n",
      "loss: 0.142997  [ 6960/15296]\n",
      "loss: 0.163183  [ 7040/15296]\n",
      "loss: 0.288802  [ 7120/15296]\n",
      "loss: 0.215065  [ 7200/15296]\n",
      "loss: 0.246086  [ 7280/15296]\n",
      "loss: 0.184301  [ 7360/15296]\n",
      "loss: 0.225602  [ 7440/15296]\n",
      "loss: 0.150012  [ 7520/15296]\n",
      "loss: 0.237487  [ 7600/15296]\n",
      "loss: 0.279088  [ 7680/15296]\n",
      "loss: 0.225207  [ 7760/15296]\n",
      "loss: 0.193590  [ 7840/15296]\n",
      "loss: 0.222127  [ 7920/15296]\n",
      "loss: 0.219057  [ 8000/15296]\n",
      "loss: 0.227289  [ 8080/15296]\n",
      "loss: 0.210381  [ 8160/15296]\n",
      "loss: 0.269665  [ 8240/15296]\n",
      "loss: 0.172696  [ 8320/15296]\n",
      "loss: 0.197925  [ 8400/15296]\n",
      "loss: 0.197499  [ 8480/15296]\n",
      "loss: 0.270836  [ 8560/15296]\n",
      "loss: 0.283819  [ 8640/15296]\n",
      "loss: 0.130749  [ 8720/15296]\n",
      "loss: 0.181119  [ 8800/15296]\n",
      "loss: 0.257588  [ 8880/15296]\n",
      "loss: 0.213147  [ 8960/15296]\n",
      "loss: 0.198249  [ 9040/15296]\n",
      "loss: 0.092523  [ 9120/15296]\n",
      "loss: 0.214325  [ 9200/15296]\n",
      "loss: 0.265572  [ 9280/15296]\n",
      "loss: 0.139680  [ 9360/15296]\n",
      "loss: 0.208557  [ 9440/15296]\n",
      "loss: 0.188835  [ 9520/15296]\n",
      "loss: 0.212460  [ 9600/15296]\n",
      "loss: 0.128391  [ 9680/15296]\n",
      "loss: 0.233116  [ 9760/15296]\n",
      "loss: 0.203744  [ 9840/15296]\n",
      "loss: 0.194648  [ 9920/15296]\n",
      "loss: 0.163242  [10000/15296]\n",
      "loss: 0.188354  [10080/15296]\n",
      "loss: 0.202619  [10160/15296]\n",
      "loss: 0.222571  [10240/15296]\n",
      "loss: 0.217226  [10320/15296]\n",
      "loss: 0.230136  [10400/15296]\n",
      "loss: 0.210615  [10480/15296]\n",
      "loss: 0.223669  [10560/15296]\n",
      "loss: 0.199972  [10640/15296]\n",
      "loss: 0.190599  [10720/15296]\n",
      "loss: 0.175946  [10800/15296]\n",
      "loss: 0.243269  [10880/15296]\n",
      "loss: 0.214003  [10960/15296]\n",
      "loss: 0.180774  [11040/15296]\n",
      "loss: 0.214759  [11120/15296]\n",
      "loss: 0.171245  [11200/15296]\n",
      "loss: 0.214388  [11280/15296]\n",
      "loss: 0.230402  [11360/15296]\n",
      "loss: 0.166921  [11440/15296]\n",
      "loss: 0.228750  [11520/15296]\n",
      "loss: 0.281905  [11600/15296]\n",
      "loss: 0.260127  [11680/15296]\n",
      "loss: 0.242271  [11760/15296]\n",
      "loss: 0.181305  [11840/15296]\n",
      "loss: 0.254620  [11920/15296]\n",
      "loss: 0.242286  [12000/15296]\n",
      "loss: 0.098554  [12080/15296]\n",
      "loss: 0.187957  [12160/15296]\n",
      "loss: 0.268774  [12240/15296]\n",
      "loss: 0.279504  [12320/15296]\n",
      "loss: 0.216425  [12400/15296]\n",
      "loss: 0.229221  [12480/15296]\n",
      "loss: 0.184824  [12560/15296]\n",
      "loss: 0.102903  [12640/15296]\n",
      "loss: 0.173375  [12720/15296]\n",
      "loss: 0.193050  [12800/15296]\n",
      "loss: 0.292094  [12880/15296]\n",
      "loss: 0.203256  [12960/15296]\n",
      "loss: 0.292676  [13040/15296]\n",
      "loss: 0.197873  [13120/15296]\n",
      "loss: 0.165678  [13200/15296]\n",
      "loss: 0.197331  [13280/15296]\n",
      "loss: 0.163668  [13360/15296]\n",
      "loss: 0.244293  [13440/15296]\n",
      "loss: 0.127197  [13520/15296]\n",
      "loss: 0.231406  [13600/15296]\n",
      "loss: 0.181564  [13680/15296]\n",
      "loss: 0.248447  [13760/15296]\n",
      "loss: 0.224450  [13840/15296]\n",
      "loss: 0.325502  [13920/15296]\n",
      "loss: 0.227811  [14000/15296]\n",
      "loss: 0.255547  [14080/15296]\n",
      "loss: 0.286292  [14160/15296]\n",
      "loss: 0.236449  [14240/15296]\n",
      "loss: 0.330410  [14320/15296]\n",
      "loss: 0.115762  [14400/15296]\n",
      "loss: 0.114355  [14480/15296]\n",
      "loss: 0.201673  [14560/15296]\n",
      "loss: 0.169866  [14640/15296]\n",
      "loss: 0.282532  [14720/15296]\n",
      "loss: 0.254703  [14800/15296]\n",
      "loss: 0.264371  [14880/15296]\n",
      "loss: 0.154239  [14960/15296]\n",
      "loss: 0.090461  [15040/15296]\n",
      "loss: 0.199790  [15120/15296]\n",
      "loss: 0.188575  [15200/15296]\n",
      "loss: 0.214443  [ 6112/15296]\n",
      "Test Loss: 0.191394%\n",
      "Epoch: 6 |  train_loss: 0.21139676079115108 |  test_loss: 0.1913944633345898 | \n",
      "loss: 0.250579  [    0/15296]\n",
      "loss: 0.219529  [   80/15296]\n",
      "loss: 0.220983  [  160/15296]\n",
      "loss: 0.281523  [  240/15296]\n",
      "loss: 0.244487  [  320/15296]\n",
      "loss: 0.184374  [  400/15296]\n",
      "loss: 0.170481  [  480/15296]\n",
      "loss: 0.146222  [  560/15296]\n",
      "loss: 0.184171  [  640/15296]\n",
      "loss: 0.225110  [  720/15296]\n",
      "loss: 0.248088  [  800/15296]\n",
      "loss: 0.254194  [  880/15296]\n",
      "loss: 0.214496  [  960/15296]\n",
      "loss: 0.201289  [ 1040/15296]\n",
      "loss: 0.228613  [ 1120/15296]\n",
      "loss: 0.239374  [ 1200/15296]\n",
      "loss: 0.198565  [ 1280/15296]\n",
      "loss: 0.219270  [ 1360/15296]\n",
      "loss: 0.246883  [ 1440/15296]\n",
      "loss: 0.267079  [ 1520/15296]\n",
      "loss: 0.225385  [ 1600/15296]\n",
      "loss: 0.269671  [ 1680/15296]\n",
      "loss: 0.304460  [ 1760/15296]\n",
      "loss: 0.238434  [ 1840/15296]\n",
      "loss: 0.179084  [ 1920/15296]\n",
      "loss: 0.257116  [ 2000/15296]\n",
      "loss: 0.219965  [ 2080/15296]\n",
      "loss: 0.212101  [ 2160/15296]\n",
      "loss: 0.260401  [ 2240/15296]\n",
      "loss: 0.307035  [ 2320/15296]\n",
      "loss: 0.220569  [ 2400/15296]\n",
      "loss: 0.328479  [ 2480/15296]\n",
      "loss: 0.121791  [ 2560/15296]\n",
      "loss: 0.108252  [ 2640/15296]\n",
      "loss: 0.180616  [ 2720/15296]\n",
      "loss: 0.122884  [ 2800/15296]\n",
      "loss: 0.176185  [ 2880/15296]\n",
      "loss: 0.232133  [ 2960/15296]\n",
      "loss: 0.246587  [ 3040/15296]\n",
      "loss: 0.125055  [ 3120/15296]\n",
      "loss: 0.189564  [ 3200/15296]\n",
      "loss: 0.280619  [ 3280/15296]\n",
      "loss: 0.216756  [ 3360/15296]\n",
      "loss: 0.160842  [ 3440/15296]\n",
      "loss: 0.171338  [ 3520/15296]\n",
      "loss: 0.217019  [ 3600/15296]\n",
      "loss: 0.280113  [ 3680/15296]\n",
      "loss: 0.223237  [ 3760/15296]\n",
      "loss: 0.163534  [ 3840/15296]\n",
      "loss: 0.236188  [ 3920/15296]\n",
      "loss: 0.252630  [ 4000/15296]\n",
      "loss: 0.261036  [ 4080/15296]\n",
      "loss: 0.143340  [ 4160/15296]\n",
      "loss: 0.206567  [ 4240/15296]\n",
      "loss: 0.287756  [ 4320/15296]\n",
      "loss: 0.166876  [ 4400/15296]\n",
      "loss: 0.169896  [ 4480/15296]\n",
      "loss: 0.248516  [ 4560/15296]\n",
      "loss: 0.361455  [ 4640/15296]\n",
      "loss: 0.265961  [ 4720/15296]\n",
      "loss: 0.176103  [ 4800/15296]\n",
      "loss: 0.274171  [ 4880/15296]\n",
      "loss: 0.277382  [ 4960/15296]\n",
      "loss: 0.194837  [ 5040/15296]\n",
      "loss: 0.168047  [ 5120/15296]\n",
      "loss: 0.323784  [ 5200/15296]\n",
      "loss: 0.178879  [ 5280/15296]\n",
      "loss: 0.194762  [ 5360/15296]\n",
      "loss: 0.221224  [ 5440/15296]\n",
      "loss: 0.219455  [ 5520/15296]\n",
      "loss: 0.107185  [ 5600/15296]\n",
      "loss: 0.239589  [ 5680/15296]\n",
      "loss: 0.219103  [ 5760/15296]\n",
      "loss: 0.188950  [ 5840/15296]\n",
      "loss: 0.279211  [ 5920/15296]\n",
      "loss: 0.297376  [ 6000/15296]\n",
      "loss: 0.324032  [ 6080/15296]\n",
      "loss: 0.160277  [ 6160/15296]\n",
      "loss: 0.147055  [ 6240/15296]\n",
      "loss: 0.312849  [ 6320/15296]\n",
      "loss: 0.227142  [ 6400/15296]\n",
      "loss: 0.278368  [ 6480/15296]\n",
      "loss: 0.209459  [ 6560/15296]\n",
      "loss: 0.137137  [ 6640/15296]\n",
      "loss: 0.260124  [ 6720/15296]\n",
      "loss: 0.256847  [ 6800/15296]\n",
      "loss: 0.139321  [ 6880/15296]\n",
      "loss: 0.231037  [ 6960/15296]\n",
      "loss: 0.201558  [ 7040/15296]\n",
      "loss: 0.167223  [ 7120/15296]\n",
      "loss: 0.178747  [ 7200/15296]\n",
      "loss: 0.267555  [ 7280/15296]\n",
      "loss: 0.223762  [ 7360/15296]\n",
      "loss: 0.190128  [ 7440/15296]\n",
      "loss: 0.283500  [ 7520/15296]\n",
      "loss: 0.231723  [ 7600/15296]\n",
      "loss: 0.168231  [ 7680/15296]\n",
      "loss: 0.257251  [ 7760/15296]\n",
      "loss: 0.192564  [ 7840/15296]\n",
      "loss: 0.256284  [ 7920/15296]\n",
      "loss: 0.223727  [ 8000/15296]\n",
      "loss: 0.182642  [ 8080/15296]\n",
      "loss: 0.300279  [ 8160/15296]\n",
      "loss: 0.270780  [ 8240/15296]\n",
      "loss: 0.200042  [ 8320/15296]\n",
      "loss: 0.163556  [ 8400/15296]\n",
      "loss: 0.171793  [ 8480/15296]\n",
      "loss: 0.194934  [ 8560/15296]\n",
      "loss: 0.237508  [ 8640/15296]\n",
      "loss: 0.166259  [ 8720/15296]\n",
      "loss: 0.284057  [ 8800/15296]\n",
      "loss: 0.211974  [ 8880/15296]\n",
      "loss: 0.251247  [ 8960/15296]\n",
      "loss: 0.259907  [ 9040/15296]\n",
      "loss: 0.135721  [ 9120/15296]\n",
      "loss: 0.168645  [ 9200/15296]\n",
      "loss: 0.161797  [ 9280/15296]\n",
      "loss: 0.259808  [ 9360/15296]\n",
      "loss: 0.256949  [ 9440/15296]\n",
      "loss: 0.108712  [ 9520/15296]\n",
      "loss: 0.284611  [ 9600/15296]\n",
      "loss: 0.145377  [ 9680/15296]\n",
      "loss: 0.137512  [ 9760/15296]\n",
      "loss: 0.247866  [ 9840/15296]\n",
      "loss: 0.223160  [ 9920/15296]\n",
      "loss: 0.261600  [10000/15296]\n",
      "loss: 0.292090  [10080/15296]\n",
      "loss: 0.229963  [10160/15296]\n",
      "loss: 0.190798  [10240/15296]\n",
      "loss: 0.223751  [10320/15296]\n",
      "loss: 0.204097  [10400/15296]\n",
      "loss: 0.209597  [10480/15296]\n",
      "loss: 0.229693  [10560/15296]\n",
      "loss: 0.237838  [10640/15296]\n",
      "loss: 0.263870  [10720/15296]\n",
      "loss: 0.247627  [10800/15296]\n",
      "loss: 0.168871  [10880/15296]\n",
      "loss: 0.218136  [10960/15296]\n",
      "loss: 0.209389  [11040/15296]\n",
      "loss: 0.243300  [11120/15296]\n",
      "loss: 0.192617  [11200/15296]\n",
      "loss: 0.193358  [11280/15296]\n",
      "loss: 0.097534  [11360/15296]\n",
      "loss: 0.164797  [11440/15296]\n",
      "loss: 0.316666  [11520/15296]\n",
      "loss: 0.200809  [11600/15296]\n",
      "loss: 0.110806  [11680/15296]\n",
      "loss: 0.170678  [11760/15296]\n",
      "loss: 0.233670  [11840/15296]\n",
      "loss: 0.288698  [11920/15296]\n",
      "loss: 0.246699  [12000/15296]\n",
      "loss: 0.184831  [12080/15296]\n",
      "loss: 0.210939  [12160/15296]\n",
      "loss: 0.174650  [12240/15296]\n",
      "loss: 0.245380  [12320/15296]\n",
      "loss: 0.292171  [12400/15296]\n",
      "loss: 0.229285  [12480/15296]\n",
      "loss: 0.224882  [12560/15296]\n",
      "loss: 0.086773  [12640/15296]\n",
      "loss: 0.118371  [12720/15296]\n",
      "loss: 0.199932  [12800/15296]\n",
      "loss: 0.237887  [12880/15296]\n",
      "loss: 0.214388  [12960/15296]\n",
      "loss: 0.214697  [13040/15296]\n",
      "loss: 0.231717  [13120/15296]\n",
      "loss: 0.151486  [13200/15296]\n",
      "loss: 0.236903  [13280/15296]\n",
      "loss: 0.296789  [13360/15296]\n",
      "loss: 0.255731  [13440/15296]\n",
      "loss: 0.088017  [13520/15296]\n",
      "loss: 0.258299  [13600/15296]\n",
      "loss: 0.152653  [13680/15296]\n",
      "loss: 0.171087  [13760/15296]\n",
      "loss: 0.230991  [13840/15296]\n",
      "loss: 0.180410  [13920/15296]\n",
      "loss: 0.164643  [14000/15296]\n",
      "loss: 0.257522  [14080/15296]\n",
      "loss: 0.182249  [14160/15296]\n",
      "loss: 0.206067  [14240/15296]\n",
      "loss: 0.126921  [14320/15296]\n",
      "loss: 0.252766  [14400/15296]\n",
      "loss: 0.236776  [14480/15296]\n",
      "loss: 0.219866  [14560/15296]\n",
      "loss: 0.231539  [14640/15296]\n",
      "loss: 0.099854  [14720/15296]\n",
      "loss: 0.183831  [14800/15296]\n",
      "loss: 0.147112  [14880/15296]\n",
      "loss: 0.234545  [14960/15296]\n",
      "loss: 0.262882  [15040/15296]\n",
      "loss: 0.259333  [15120/15296]\n",
      "loss: 0.162107  [15200/15296]\n",
      "loss: 0.093282  [ 6112/15296]\n",
      "Test Loss: 0.191629%\n",
      "Epoch: 7 |  train_loss: 0.2104779710852135 |  test_loss: 0.19162940099563228 | \n",
      "loss: 0.219236  [    0/15296]\n",
      "loss: 0.258747  [   80/15296]\n",
      "loss: 0.144427  [  160/15296]\n",
      "loss: 0.193025  [  240/15296]\n",
      "loss: 0.263077  [  320/15296]\n",
      "loss: 0.151887  [  400/15296]\n",
      "loss: 0.320873  [  480/15296]\n",
      "loss: 0.192102  [  560/15296]\n",
      "loss: 0.273192  [  640/15296]\n",
      "loss: 0.170742  [  720/15296]\n",
      "loss: 0.189611  [  800/15296]\n",
      "loss: 0.270529  [  880/15296]\n",
      "loss: 0.234353  [  960/15296]\n",
      "loss: 0.204247  [ 1040/15296]\n",
      "loss: 0.174248  [ 1120/15296]\n",
      "loss: 0.264657  [ 1200/15296]\n",
      "loss: 0.281294  [ 1280/15296]\n",
      "loss: 0.141861  [ 1360/15296]\n",
      "loss: 0.225071  [ 1440/15296]\n",
      "loss: 0.227230  [ 1520/15296]\n",
      "loss: 0.225727  [ 1600/15296]\n",
      "loss: 0.132066  [ 1680/15296]\n",
      "loss: 0.122468  [ 1760/15296]\n",
      "loss: 0.210679  [ 1840/15296]\n",
      "loss: 0.195998  [ 1920/15296]\n",
      "loss: 0.175883  [ 2000/15296]\n",
      "loss: 0.128670  [ 2080/15296]\n",
      "loss: 0.184627  [ 2160/15296]\n",
      "loss: 0.242403  [ 2240/15296]\n",
      "loss: 0.199351  [ 2320/15296]\n",
      "loss: 0.169242  [ 2400/15296]\n",
      "loss: 0.180596  [ 2480/15296]\n",
      "loss: 0.199535  [ 2560/15296]\n",
      "loss: 0.215432  [ 2640/15296]\n",
      "loss: 0.234140  [ 2720/15296]\n",
      "loss: 0.293227  [ 2800/15296]\n",
      "loss: 0.213151  [ 2880/15296]\n",
      "loss: 0.136730  [ 2960/15296]\n",
      "loss: 0.249528  [ 3040/15296]\n",
      "loss: 0.279972  [ 3120/15296]\n",
      "loss: 0.183791  [ 3200/15296]\n",
      "loss: 0.102555  [ 3280/15296]\n",
      "loss: 0.270666  [ 3360/15296]\n",
      "loss: 0.246440  [ 3440/15296]\n",
      "loss: 0.167577  [ 3520/15296]\n",
      "loss: 0.147960  [ 3600/15296]\n",
      "loss: 0.251154  [ 3680/15296]\n",
      "loss: 0.164575  [ 3760/15296]\n",
      "loss: 0.228360  [ 3840/15296]\n",
      "loss: 0.277614  [ 3920/15296]\n",
      "loss: 0.108247  [ 4000/15296]\n",
      "loss: 0.234249  [ 4080/15296]\n",
      "loss: 0.211988  [ 4160/15296]\n",
      "loss: 0.264059  [ 4240/15296]\n",
      "loss: 0.195978  [ 4320/15296]\n",
      "loss: 0.249762  [ 4400/15296]\n",
      "loss: 0.298068  [ 4480/15296]\n",
      "loss: 0.376914  [ 4560/15296]\n",
      "loss: 0.280052  [ 4640/15296]\n",
      "loss: 0.213246  [ 4720/15296]\n",
      "loss: 0.128482  [ 4800/15296]\n",
      "loss: 0.254298  [ 4880/15296]\n",
      "loss: 0.126072  [ 4960/15296]\n",
      "loss: 0.190588  [ 5040/15296]\n",
      "loss: 0.257460  [ 5120/15296]\n",
      "loss: 0.166765  [ 5200/15296]\n",
      "loss: 0.170699  [ 5280/15296]\n",
      "loss: 0.162559  [ 5360/15296]\n",
      "loss: 0.129487  [ 5440/15296]\n",
      "loss: 0.150487  [ 5520/15296]\n",
      "loss: 0.248281  [ 5600/15296]\n",
      "loss: 0.257238  [ 5680/15296]\n",
      "loss: 0.265527  [ 5760/15296]\n",
      "loss: 0.231701  [ 5840/15296]\n",
      "loss: 0.278491  [ 5920/15296]\n",
      "loss: 0.234525  [ 6000/15296]\n",
      "loss: 0.245620  [ 6080/15296]\n",
      "loss: 0.254786  [ 6160/15296]\n",
      "loss: 0.176872  [ 6240/15296]\n",
      "loss: 0.247417  [ 6320/15296]\n",
      "loss: 0.260852  [ 6400/15296]\n",
      "loss: 0.205021  [ 6480/15296]\n",
      "loss: 0.215736  [ 6560/15296]\n",
      "loss: 0.255814  [ 6640/15296]\n",
      "loss: 0.230480  [ 6720/15296]\n",
      "loss: 0.123775  [ 6800/15296]\n",
      "loss: 0.204490  [ 6880/15296]\n",
      "loss: 0.185851  [ 6960/15296]\n",
      "loss: 0.123250  [ 7040/15296]\n",
      "loss: 0.229534  [ 7120/15296]\n",
      "loss: 0.178044  [ 7200/15296]\n",
      "loss: 0.161718  [ 7280/15296]\n",
      "loss: 0.217322  [ 7360/15296]\n",
      "loss: 0.198641  [ 7440/15296]\n",
      "loss: 0.145645  [ 7520/15296]\n",
      "loss: 0.233628  [ 7600/15296]\n",
      "loss: 0.173036  [ 7680/15296]\n",
      "loss: 0.195105  [ 7760/15296]\n",
      "loss: 0.220039  [ 7840/15296]\n",
      "loss: 0.114450  [ 7920/15296]\n",
      "loss: 0.311002  [ 8000/15296]\n",
      "loss: 0.245214  [ 8080/15296]\n",
      "loss: 0.211731  [ 8160/15296]\n",
      "loss: 0.251814  [ 8240/15296]\n",
      "loss: 0.205770  [ 8320/15296]\n",
      "loss: 0.133410  [ 8400/15296]\n",
      "loss: 0.256705  [ 8480/15296]\n",
      "loss: 0.260241  [ 8560/15296]\n",
      "loss: 0.152677  [ 8640/15296]\n",
      "loss: 0.159549  [ 8720/15296]\n",
      "loss: 0.214333  [ 8800/15296]\n",
      "loss: 0.105425  [ 8880/15296]\n",
      "loss: 0.237114  [ 8960/15296]\n",
      "loss: 0.274635  [ 9040/15296]\n",
      "loss: 0.191149  [ 9120/15296]\n",
      "loss: 0.119387  [ 9200/15296]\n",
      "loss: 0.182065  [ 9280/15296]\n",
      "loss: 0.208897  [ 9360/15296]\n",
      "loss: 0.239232  [ 9440/15296]\n",
      "loss: 0.254483  [ 9520/15296]\n",
      "loss: 0.191433  [ 9600/15296]\n",
      "loss: 0.278699  [ 9680/15296]\n",
      "loss: 0.205144  [ 9760/15296]\n",
      "loss: 0.107242  [ 9840/15296]\n",
      "loss: 0.149839  [ 9920/15296]\n",
      "loss: 0.223398  [10000/15296]\n",
      "loss: 0.149658  [10080/15296]\n",
      "loss: 0.252272  [10160/15296]\n",
      "loss: 0.204623  [10240/15296]\n",
      "loss: 0.190350  [10320/15296]\n",
      "loss: 0.208130  [10400/15296]\n",
      "loss: 0.257352  [10480/15296]\n",
      "loss: 0.224308  [10560/15296]\n",
      "loss: 0.259571  [10640/15296]\n",
      "loss: 0.257118  [10720/15296]\n",
      "loss: 0.114991  [10800/15296]\n",
      "loss: 0.276381  [10880/15296]\n",
      "loss: 0.193845  [10960/15296]\n",
      "loss: 0.277836  [11040/15296]\n",
      "loss: 0.246658  [11120/15296]\n",
      "loss: 0.127224  [11200/15296]\n",
      "loss: 0.202819  [11280/15296]\n",
      "loss: 0.290376  [11360/15296]\n",
      "loss: 0.151795  [11440/15296]\n",
      "loss: 0.188694  [11520/15296]\n",
      "loss: 0.196746  [11600/15296]\n",
      "loss: 0.106379  [11680/15296]\n",
      "loss: 0.251117  [11760/15296]\n",
      "loss: 0.158455  [11840/15296]\n",
      "loss: 0.171049  [11920/15296]\n",
      "loss: 0.260589  [12000/15296]\n",
      "loss: 0.164308  [12080/15296]\n",
      "loss: 0.272102  [12160/15296]\n",
      "loss: 0.253971  [12240/15296]\n",
      "loss: 0.178287  [12320/15296]\n",
      "loss: 0.208413  [12400/15296]\n",
      "loss: 0.236019  [12480/15296]\n",
      "loss: 0.178655  [12560/15296]\n",
      "loss: 0.256239  [12640/15296]\n",
      "loss: 0.085771  [12720/15296]\n",
      "loss: 0.166051  [12800/15296]\n",
      "loss: 0.185606  [12880/15296]\n",
      "loss: 0.187227  [12960/15296]\n",
      "loss: 0.215461  [13040/15296]\n",
      "loss: 0.189268  [13120/15296]\n",
      "loss: 0.215302  [13200/15296]\n",
      "loss: 0.268568  [13280/15296]\n",
      "loss: 0.237394  [13360/15296]\n",
      "loss: 0.198733  [13440/15296]\n",
      "loss: 0.300105  [13520/15296]\n",
      "loss: 0.229401  [13600/15296]\n",
      "loss: 0.224841  [13680/15296]\n",
      "loss: 0.247687  [13760/15296]\n",
      "loss: 0.109346  [13840/15296]\n",
      "loss: 0.233137  [13920/15296]\n",
      "loss: 0.347060  [14000/15296]\n",
      "loss: 0.109370  [14080/15296]\n",
      "loss: 0.152491  [14160/15296]\n",
      "loss: 0.256450  [14240/15296]\n",
      "loss: 0.251818  [14320/15296]\n",
      "loss: 0.210818  [14400/15296]\n",
      "loss: 0.271829  [14480/15296]\n",
      "loss: 0.221425  [14560/15296]\n",
      "loss: 0.190657  [14640/15296]\n",
      "loss: 0.121830  [14720/15296]\n",
      "loss: 0.165727  [14800/15296]\n",
      "loss: 0.209348  [14880/15296]\n",
      "loss: 0.124022  [14960/15296]\n",
      "loss: 0.177489  [15040/15296]\n",
      "loss: 0.265167  [15120/15296]\n",
      "loss: 0.209389  [15200/15296]\n",
      "loss: 0.227469  [ 6112/15296]\n",
      "Test Loss: 0.194300%\n",
      "Epoch: 8 |  train_loss: 0.21084481796770121 |  test_loss: 0.19429968496503897 | \n",
      "loss: 0.273020  [    0/15296]\n",
      "loss: 0.169288  [   80/15296]\n",
      "loss: 0.206924  [  160/15296]\n",
      "loss: 0.224822  [  240/15296]\n",
      "loss: 0.279503  [  320/15296]\n",
      "loss: 0.215120  [  400/15296]\n",
      "loss: 0.245170  [  480/15296]\n",
      "loss: 0.279403  [  560/15296]\n",
      "loss: 0.129475  [  640/15296]\n",
      "loss: 0.244741  [  720/15296]\n",
      "loss: 0.179322  [  800/15296]\n",
      "loss: 0.264859  [  880/15296]\n",
      "loss: 0.248748  [  960/15296]\n",
      "loss: 0.192348  [ 1040/15296]\n",
      "loss: 0.243691  [ 1120/15296]\n",
      "loss: 0.222564  [ 1200/15296]\n",
      "loss: 0.210678  [ 1280/15296]\n",
      "loss: 0.199032  [ 1360/15296]\n",
      "loss: 0.228960  [ 1440/15296]\n",
      "loss: 0.215175  [ 1520/15296]\n",
      "loss: 0.098386  [ 1600/15296]\n",
      "loss: 0.264977  [ 1680/15296]\n",
      "loss: 0.258149  [ 1760/15296]\n",
      "loss: 0.166343  [ 1840/15296]\n",
      "loss: 0.238559  [ 1920/15296]\n",
      "loss: 0.162685  [ 2000/15296]\n",
      "loss: 0.188646  [ 2080/15296]\n",
      "loss: 0.287868  [ 2160/15296]\n",
      "loss: 0.191706  [ 2240/15296]\n",
      "loss: 0.277865  [ 2320/15296]\n",
      "loss: 0.228029  [ 2400/15296]\n",
      "loss: 0.301012  [ 2480/15296]\n",
      "loss: 0.251254  [ 2560/15296]\n",
      "loss: 0.239398  [ 2640/15296]\n",
      "loss: 0.151557  [ 2720/15296]\n",
      "loss: 0.220631  [ 2800/15296]\n",
      "loss: 0.164928  [ 2880/15296]\n",
      "loss: 0.183880  [ 2960/15296]\n",
      "loss: 0.236786  [ 3040/15296]\n",
      "loss: 0.175266  [ 3120/15296]\n",
      "loss: 0.257728  [ 3200/15296]\n",
      "loss: 0.196843  [ 3280/15296]\n",
      "loss: 0.167397  [ 3360/15296]\n",
      "loss: 0.218447  [ 3440/15296]\n",
      "loss: 0.181509  [ 3520/15296]\n",
      "loss: 0.239017  [ 3600/15296]\n",
      "loss: 0.189640  [ 3680/15296]\n",
      "loss: 0.112460  [ 3760/15296]\n",
      "loss: 0.186998  [ 3840/15296]\n",
      "loss: 0.232186  [ 3920/15296]\n",
      "loss: 0.214258  [ 4000/15296]\n",
      "loss: 0.221318  [ 4080/15296]\n",
      "loss: 0.113546  [ 4160/15296]\n",
      "loss: 0.104544  [ 4240/15296]\n",
      "loss: 0.130699  [ 4320/15296]\n",
      "loss: 0.183161  [ 4400/15296]\n",
      "loss: 0.260447  [ 4480/15296]\n",
      "loss: 0.181836  [ 4560/15296]\n",
      "loss: 0.198962  [ 4640/15296]\n",
      "loss: 0.103245  [ 4720/15296]\n",
      "loss: 0.125926  [ 4800/15296]\n",
      "loss: 0.245843  [ 4880/15296]\n",
      "loss: 0.260805  [ 4960/15296]\n",
      "loss: 0.194871  [ 5040/15296]\n",
      "loss: 0.294506  [ 5120/15296]\n",
      "loss: 0.193999  [ 5200/15296]\n",
      "loss: 0.091450  [ 5280/15296]\n",
      "loss: 0.189474  [ 5360/15296]\n",
      "loss: 0.183456  [ 5440/15296]\n",
      "loss: 0.133993  [ 5520/15296]\n",
      "loss: 0.285632  [ 5600/15296]\n",
      "loss: 0.147535  [ 5680/15296]\n",
      "loss: 0.270110  [ 5760/15296]\n",
      "loss: 0.215941  [ 5840/15296]\n",
      "loss: 0.220624  [ 5920/15296]\n",
      "loss: 0.191627  [ 6000/15296]\n",
      "loss: 0.252012  [ 6080/15296]\n",
      "loss: 0.272011  [ 6160/15296]\n",
      "loss: 0.236033  [ 6240/15296]\n",
      "loss: 0.280236  [ 6320/15296]\n",
      "loss: 0.262189  [ 6400/15296]\n",
      "loss: 0.226728  [ 6480/15296]\n",
      "loss: 0.153529  [ 6560/15296]\n",
      "loss: 0.211836  [ 6640/15296]\n",
      "loss: 0.231880  [ 6720/15296]\n",
      "loss: 0.254069  [ 6800/15296]\n",
      "loss: 0.135389  [ 6880/15296]\n",
      "loss: 0.237274  [ 6960/15296]\n",
      "loss: 0.209993  [ 7040/15296]\n",
      "loss: 0.254031  [ 7120/15296]\n",
      "loss: 0.159282  [ 7200/15296]\n",
      "loss: 0.259229  [ 7280/15296]\n",
      "loss: 0.140595  [ 7360/15296]\n",
      "loss: 0.204083  [ 7440/15296]\n",
      "loss: 0.118691  [ 7520/15296]\n",
      "loss: 0.188125  [ 7600/15296]\n",
      "loss: 0.182233  [ 7680/15296]\n",
      "loss: 0.109342  [ 7760/15296]\n",
      "loss: 0.298103  [ 7840/15296]\n",
      "loss: 0.251115  [ 7920/15296]\n",
      "loss: 0.226456  [ 8000/15296]\n",
      "loss: 0.146545  [ 8080/15296]\n",
      "loss: 0.194696  [ 8160/15296]\n",
      "loss: 0.246800  [ 8240/15296]\n",
      "loss: 0.201640  [ 8320/15296]\n",
      "loss: 0.172703  [ 8400/15296]\n",
      "loss: 0.290241  [ 8480/15296]\n",
      "loss: 0.176712  [ 8560/15296]\n",
      "loss: 0.314376  [ 8640/15296]\n",
      "loss: 0.311654  [ 8720/15296]\n",
      "loss: 0.195359  [ 8800/15296]\n",
      "loss: 0.245923  [ 8880/15296]\n",
      "loss: 0.166275  [ 8960/15296]\n",
      "loss: 0.229215  [ 9040/15296]\n",
      "loss: 0.227299  [ 9120/15296]\n",
      "loss: 0.270593  [ 9200/15296]\n",
      "loss: 0.223665  [ 9280/15296]\n",
      "loss: 0.232307  [ 9360/15296]\n",
      "loss: 0.242043  [ 9440/15296]\n",
      "loss: 0.117860  [ 9520/15296]\n",
      "loss: 0.187980  [ 9600/15296]\n",
      "loss: 0.161119  [ 9680/15296]\n",
      "loss: 0.222451  [ 9760/15296]\n",
      "loss: 0.169574  [ 9840/15296]\n",
      "loss: 0.192629  [ 9920/15296]\n",
      "loss: 0.108271  [10000/15296]\n",
      "loss: 0.198154  [10080/15296]\n",
      "loss: 0.157124  [10160/15296]\n",
      "loss: 0.137594  [10240/15296]\n",
      "loss: 0.220736  [10320/15296]\n",
      "loss: 0.227620  [10400/15296]\n",
      "loss: 0.177142  [10480/15296]\n",
      "loss: 0.334535  [10560/15296]\n",
      "loss: 0.264726  [10640/15296]\n",
      "loss: 0.162183  [10720/15296]\n",
      "loss: 0.183124  [10800/15296]\n",
      "loss: 0.126011  [10880/15296]\n",
      "loss: 0.116923  [10960/15296]\n",
      "loss: 0.172053  [11040/15296]\n",
      "loss: 0.122969  [11120/15296]\n",
      "loss: 0.174727  [11200/15296]\n",
      "loss: 0.245986  [11280/15296]\n",
      "loss: 0.313428  [11360/15296]\n",
      "loss: 0.172326  [11440/15296]\n",
      "loss: 0.150897  [11520/15296]\n",
      "loss: 0.170885  [11600/15296]\n",
      "loss: 0.199603  [11680/15296]\n",
      "loss: 0.179065  [11760/15296]\n",
      "loss: 0.100666  [11840/15296]\n",
      "loss: 0.111764  [11920/15296]\n",
      "loss: 0.268667  [12000/15296]\n",
      "loss: 0.129925  [12080/15296]\n",
      "loss: 0.247583  [12160/15296]\n",
      "loss: 0.104208  [12240/15296]\n",
      "loss: 0.209856  [12320/15296]\n",
      "loss: 0.182000  [12400/15296]\n",
      "loss: 0.234984  [12480/15296]\n",
      "loss: 0.348333  [12560/15296]\n",
      "loss: 0.085678  [12640/15296]\n",
      "loss: 0.140879  [12720/15296]\n",
      "loss: 0.194772  [12800/15296]\n",
      "loss: 0.246131  [12880/15296]\n",
      "loss: 0.216587  [12960/15296]\n",
      "loss: 0.243589  [13040/15296]\n",
      "loss: 0.206585  [13120/15296]\n",
      "loss: 0.224141  [13200/15296]\n",
      "loss: 0.142493  [13280/15296]\n",
      "loss: 0.211762  [13360/15296]\n",
      "loss: 0.264923  [13440/15296]\n",
      "loss: 0.158969  [13520/15296]\n",
      "loss: 0.200883  [13600/15296]\n",
      "loss: 0.262748  [13680/15296]\n",
      "loss: 0.249162  [13760/15296]\n",
      "loss: 0.188181  [13840/15296]\n",
      "loss: 0.186998  [13920/15296]\n",
      "loss: 0.213192  [14000/15296]\n",
      "loss: 0.218221  [14080/15296]\n",
      "loss: 0.272626  [14160/15296]\n",
      "loss: 0.235300  [14240/15296]\n",
      "loss: 0.304675  [14320/15296]\n",
      "loss: 0.105241  [14400/15296]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m train(model, train_dl, test_dl, torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m1e-6\u001b[39;49m), RMSELoss(), epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m) \u001b[39m# Make| sure set shuffle for train back to True after testing\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# 3. Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m---> 18\u001b[0m     train_loss \u001b[39m=\u001b[39m train_step(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     19\u001b[0m                                        data_loader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m     20\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m     21\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[0;32m     22\u001b[0m     test_loss \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     23\u001b[0m         data_loader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[0;32m     24\u001b[0m         loss_fn\u001b[39m=\u001b[39mloss_fn)\n\u001b[0;32m     26\u001b[0m     \u001b[39m# 4. Print out what's happening\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data_loader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Setup train loss and train accuracy values\u001b[39;00m\n\u001b[0;32m      6\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Send data to target device\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# 1. Forward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_got_empty_message \u001b[39mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[39m.\u001b[39mPeekNamedPipe(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle)[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(wait([\u001b[39mself\u001b[39;49m], timeout))\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[39m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[39m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[39m.\u001b[39;49mkeys(), timeout)\n\u001b[0;32m    880\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[39m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[39mfor\u001b[39;00m ov \u001b[39min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\nkakhani\\anaconda3\\envs\\pytorchGPU\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[39mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mWaitForMultipleObjects(L, \u001b[39mFalse\u001b[39;49;00m, timeout)\n\u001b[0;32m    812\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = train(model, train_dl, test_dl, torch.optim.Adam(model.parameters(), lr=1e-6), RMSELoss(), epochs=10) # Make| sure set shuffle for train back to True after testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(loss_dict):\n",
    "    train_losses = loss_dict[\"train_loss\"]\n",
    "    test_losses = loss_dict[\"test_loss\"]\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, test_losses, label=\"Test Loss\")\n",
    "    plt.title(\"Training and Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "somehow the test_loss is smaller than train_loss -> could be due to small smaple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c74e58d50e1361e9a22e65e4f4b85ac48544f43f48a101259d9c47c27371742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
